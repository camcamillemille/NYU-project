{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.ticker import MultipleLocator, FuncFormatter\n",
    "import seaborn as sns\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import pairwise_distances, silhouette_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler,normalize\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score, pairwise_distances\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "import igraph as ig\n",
    "import leidenalg as la\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from mpl_toolkits.mplot3d import Axes3D \n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)  # None = no limit for when i do df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "#### Opening the text document that contains the columns description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/gc_peak2_all_colnames.txt\", \"r\") as f:\n",
    "    description_colnames = f.read()\n",
    "\n",
    "print(description_colnames)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "#### Hard coding variables useful :\n",
    "- treatment_group list\n",
    "- hex_colors list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##LIST CONTAINITNG THE GROUPS NAMES \n",
    "treatment_groups = [\n",
    "        'BT474_mV_72hNHWD',\n",
    "        'BT474_mV_72hSTC15',\n",
    "        'BT474_mV_high_Untreated',\n",
    "        'BT474_mV_low_Untreated',\n",
    "        'BT474_mV_Untreated_Unsorted']\n",
    "\n",
    "## ARBITRARY CHOSEN COLORS FOR THE CLUSTERS (THEY NEED TO HAVE A HIGH VALUE AND CHROMA TO STAND OUT ON A GREY BACKGROUND)\n",
    "hex_colors = [\n",
    "    \"#0e67a7\", \"#ff7f0e\", \"#a0e468\", \"#d62728\", \"#9467bd\",\n",
    "    \"#672417\", \"#e377c2\", \"#f5f523\", \"#28e0f5\", \"#3214a8\",\n",
    "    \"#ca9d16\", \"#04a887\", \"#8c564b\", \"#17becf\", \"#bcbd22\",\n",
    "    \"#2ca02c\", \"#1f77b4\", \"#ff9896\", \"#c5b0d5\", \"#98df8a\",\n",
    "    \"#ffbb78\", \"#aec7e8\", \"#7f7f7f\", \"#c49c94\", \"#dbdb8d\",\n",
    "    \"#9edae5\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "##### Opening the datafile:\n",
    "Calling the dataframe df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/gc_peak2_all.txt\", delim_whitespace=True)\n",
    "\n",
    "print(f'The shape of the dataframe is of {df.shape}')\n",
    "#print(df.dtypes)\n",
    "#print(df.columns)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bulk = pd.read_csv(\"../data/camille_average_profile_pol2_by_cluster.txt\", delim_whitespace=True)\n",
    "\n",
    "print(f'The shape of the dataframe is of {df.shape}')\n",
    "#print(df.dtypes)\n",
    "#print(df.columns)\n",
    "is_constant = df_bulk.groupby('cluster')['cov'].nunique().eq(1)\n",
    "\n",
    "print(\"cov constant per cluster:\", is_constant.all())\n",
    "\n",
    "df_bulk.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Making a short function to manually calculate pol2 position, in case the column 'motif_center' has wrong values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MANUALLY RECALCULATING POL2 POSITION AS THE MIDPOINT BETWEEN MOTIF_START AND MOTIF_END\n",
    "def calculate_pol2_position(df):\n",
    "    \"\"\"Calculate the Pol II position as the midpoint between motif_start and motif_end.\"\"\"\n",
    "    if 'motif_start' in df.columns and 'motif_end' in df.columns:\n",
    "        df['pol2_pos']= (df['motif_start'] + df['motif_end']) / 2\n",
    "    else:\n",
    "        raise ValueError(\"DataFrame must contain 'motif_start' and 'motif_end' columns.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "##### Preprocessing\n",
    "Here are the following functions defined:\n",
    "1. 'preprocess_dataframe'\n",
    "2. 'filter_reads_per_gene_middle_bin_name'\n",
    "3. 'bin_then_matrix'\n",
    "4. 'process_into_matrix'\n",
    "5. 'handling_NaN'\n",
    "6. 'preprocess_long_for_plot'\n",
    "7. 'plot_reads_long'\n",
    "8. 'get_genes_list'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. PREPROCESS OF THE DATAFRAME FUNCTION\n",
    "\n",
    "def preprocess_dataframe(\n",
    "    df: pd.DataFrame,\n",
    "    #nan_threshold: float = 0.9,\n",
    "    drop_columns: list = None,\n",
    ") -> pd.DataFrame:\n",
    "    \n",
    "    \"\"\"\n",
    "    Preprocess a DataFrame by:\n",
    "    # Dropping columns with too many NaN values\n",
    "    0. Creating the 'pol2_pos' column as the midpoint between 'motif_start' and 'motif_end' (ignorinng 'motif_center' column, in case error in dataset)\n",
    "    1. Removing user-defined non-useful columns\n",
    "    2. Converting 'chr' column to numeric\n",
    "    3. Converting the variable 'pol2' into a binary marker: 1 for pol2 and 0 for no pol2\n",
    "    4. Creating a unique 'gene_tss' identifier\n",
    "    5. Dropping duplicate rows\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame to preprocess.\n",
    "    #nan_threshold : float, optional (default=0.9) (each column that has more than 10% of NaN is dropped)\n",
    "        Minimum fraction of non-NaN values required to keep a column.\n",
    "    drop_columns : list, optional\n",
    "        Additional columns to drop.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Cleaned and preprocessed DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    # Work on a copy to avoid modifying original\n",
    "    df_proc = df.copy().drop_duplicates()\n",
    "\n",
    "    # # Drop columns with too many NaN values\n",
    "    # coverage_mask = df_proc.notna().mean(axis=0) > nan_threshold    #dropping columns that have too many NaN\n",
    "    # dropped_columns = df_proc.columns[~coverage_mask].tolist()      #lst of dropped columns\n",
    "    # df_proc = df_proc.loc[:, coverage_mask]                         #new df only keeping useful columns\n",
    "    # print(f\"Filtered from {df.shape[1]} to {df_proc.shape[1]} regions\") #tells how many columns dropped\n",
    "    # print(\"Dropped (NaN coverage):\", dropped_columns)                   #lists the dropped columns\n",
    "\n",
    "    # Create 'pol2_pos' column as midpoint between 'motif_start' and 'motif_end'\n",
    "    df_proc = calculate_pol2_position(df_proc)\n",
    "\n",
    "    # Drop user-deemed non-useful columns\n",
    "    if drop_columns:\n",
    "        df_proc = df_proc.drop(columns=[col for col in drop_columns if col in df_proc.columns], errors=\"ignore\")\n",
    "\n",
    "    # Convert chr column to numeric (strip \"chr\")\n",
    "    if \"chr\" in df_proc.columns:\n",
    "        df_proc[\"chr\"] = pd.to_numeric(df_proc[\"chr\"].str.replace(\"chr\", \"\", regex=False), errors=\"coerce\")\n",
    "\n",
    "    #Convert 'pol2' label into a binary marker (change pol2 into 1)\n",
    "    if 'pol2' in df_proc.columns:\n",
    "        mapping_pol2 = {\"pol2\":1, \"nonpol2\":0}\n",
    "        df_proc[\"pol2\"] = df_proc[\"pol2\"].map(mapping_pol2)\n",
    "\n",
    "    # Create gene_tss column if columns exist\n",
    "    if \"gene\" in df_proc.columns and \"tss_pos\" in df_proc.columns:\n",
    "        df_proc[\"gene_tss\"] = df_proc[\"gene\"].astype(str) + \"_\" + df_proc[\"tss_pos\"].astype(str)\n",
    "\n",
    "    # Drop duplicates again after transformations\n",
    "    df_proc = df_proc.drop_duplicates()\n",
    "\n",
    "    print(f\"Final shape: {df_proc.shape}\")\n",
    "    print(f\"NUmber of gene_tss: {df_proc['gene_tss'].nunique() if 'gene_tss' in df_proc.columns else 'N/A'}\")\n",
    "\n",
    "    return df_proc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. FUNCTION TO FILTER ONLY THE GENES THAT HAVE SUFFICIENT READS THAT OVERLAP ON THE SAME GENOMIC REGION, CENTERED ON THE MIDDLE\n",
    "## THE INPUT DF MUST BE BINNED\n",
    "\n",
    "def filter_reads_per_gene_middle_bin_name(\n",
    "    df: pd.DataFrame, #usually need to put the preprocessed dataframe BUT BINNED, with all the genes in there\n",
    "    middle_bin_name: str =None,\n",
    "    min_reads: int =50, # the minimal number of reads that we want\n",
    "    min_bins: int =40,  # the miniml lenght per bin that we want the read to be\n",
    "    require_middle_bin: bool =True\n",
    "    \n",
    "):\n",
    "    \"\"\"\n",
    "    Filters reads per gene based on coverage in consecutive bins and keeps intervals\n",
    "    containing a specified middle bin column.\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame with reads as rows, bins as columns, and a gene identifier column or index.\n",
    "    middle_bin_name : str\n",
    "        Name of the middle bin column to require in the kept intervals.\n",
    "    min_reads : int\n",
    "        Minimum number of overlapping reads per bin.\n",
    "    min_bins : int\n",
    "        Minimum number of consecutive bins required.\n",
    "    require_middle_bin : bool\n",
    "        If True, only keeps regions that contain the middle bin.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_filtered : pd.DataFrame\n",
    "        Reads from genes that pass the filter, with original indices preserved.\n",
    "        THE DF IS BINNED AND IN MATRIX FORM WITH THE NAN VALUES\n",
    "        IT IS GROUPED BY GENE, had had as index; 'gene-tss', 'readid', 'group' 'cluster\n",
    "    filtered_regions : dict\n",
    "        Per-gene list of intervals (start_bin, end_bin) that pass the filter.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Detect bin columns\n",
    "    if 'readid' in df.columns:   \n",
    "        bin_cols = df.columns.difference(['readid']) #it creates a dataframe that keeps every other column of df except the columns 'readid'\n",
    "        #it's just to take th bin columns, not the 'readid' column\n",
    "    \n",
    "    else:\n",
    "        bin_cols = df.columns\n",
    "    \n",
    "    # Get index of middle bin column if specified\n",
    "    if middle_bin_name is not None:\n",
    "        if middle_bin_name not in bin_cols:\n",
    "            raise ValueError(f\"Middle bin '{middle_bin_name}' not found in bin columns\")\n",
    "        middle_bin_idx = bin_cols.get_loc(middle_bin_name)\n",
    "    \n",
    "    def find_high_coverage_regions(\n",
    "            coverage, \n",
    "            min_reads, \n",
    "            min_bins\n",
    "            ): \n",
    "        \"\"\"\n",
    "        Identifies continuous genomic regions (bins) where the coverage (number of overlapping reads) \n",
    "        is consistently above a minimum threshold, for at least a minimum region length.\n",
    "\n",
    "        Parameters:\n",
    "        -----\n",
    "        coverage: list or np.ndarray\n",
    "            A sequence of coverage values (e.g., per genomic bin). Each value indicates how many reads cover that bin.\n",
    "        min_reads: int)\n",
    "            The minimum number of reads required for a bin to be considered \"covered.\"\n",
    "        min_bins: int\n",
    "            The minimum number of consecutive bins that must satisfy the coverage threshold in order to define a valid region.\n",
    "\n",
    "        Returns:\n",
    "        -----\n",
    "\n",
    "        regions: list of tuples (start, end)\n",
    "            Each tuple (start, end) represents the indices of bins forming a valid high-coverage region.\n",
    "        \"\"\"\n",
    "        regions = []\n",
    "        start = None\n",
    "        for i, cov in enumerate(coverage):\n",
    "            if cov >= min_reads:\n",
    "                if start is None:\n",
    "                    start = i\n",
    "            else:\n",
    "                if start is not None:\n",
    "                    end = i - 1\n",
    "                    if (end - start + 1) >= min_bins:\n",
    "                        regions.append((start, end))\n",
    "                    start = None\n",
    "        if start is not None:\n",
    "            end = len(coverage) - 1\n",
    "            if (end - start + 1) >= min_bins:\n",
    "                regions.append((start, end))\n",
    "        return regions\n",
    "    \n",
    "    filtered_rows = []\n",
    "    filtered_regions = {}\n",
    "\n",
    "    # Group by gene\n",
    "    groups = df.groupby(level='gene_tss', observed=True)\n",
    "    \n",
    "    for gene, sub in groups:\n",
    "        coverage = sub[bin_cols].notna().sum(axis=0)\n",
    "        regions = find_high_coverage_regions(coverage, min_reads, min_bins)\n",
    "        \n",
    "        # Keep only intervals covering the middle bin if required\n",
    "        if require_middle_bin and middle_bin_name is not None:\n",
    "            regions = [(s,e) for s,e in regions if s <= middle_bin_idx <= e]\n",
    "        \n",
    "        if not regions:\n",
    "            continue  # skip gene if no valid region\n",
    "        \n",
    "        filtered_regions[gene] = regions\n",
    "        \n",
    "        # Keep reads overlapping at least one valid region\n",
    "        mask = np.zeros(sub.shape[0], dtype=bool)\n",
    "        for start, end in regions:\n",
    "            mask |= sub[bin_cols].iloc[:, start:end+1].notna().any(axis=1)\n",
    "        \n",
    "        filtered_rows.append(sub[mask])\n",
    "    \n",
    "    if not filtered_rows:\n",
    "        return pd.DataFrame(), {}\n",
    "    \n",
    "    df_filtered = pd.concat(filtered_rows, ignore_index=False)\n",
    "\n",
    "    \n",
    "    return df_filtered, filtered_regions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. BINNING THE DATAFRAME AND TURNING IT INTO MATRIX FORM\n",
    "# TO BE CALLED IN A FUNCITON, NO USE ALONE\n",
    "\n",
    "def bin_then_matrix(\n",
    "        df : pd.DataFrame ,\n",
    "        indexes : list = ['gene_tss','readid','group','cluster'], \n",
    "        bin_size : int =50,\n",
    "):\n",
    "        \"\"\"\n",
    "        Bins the reads, and then transforms the dataframe into a dataframe matrix with methylation values per bin\n",
    "\n",
    "        Parameters:\n",
    "        df : pd.dataframe\n",
    "                the dataframe must have 'readid','gene_tss','group','cluster','meth', 'C_pos' as minimal columns\n",
    "        \n",
    "        indexes: list\n",
    "                by default the following columns turn into indexes ['gene_tss','readid','group','cluster']\n",
    "        \n",
    "        bin_size : int\n",
    "                by default 50 bp\n",
    "        \n",
    "        Returns:\n",
    "        df_matrix: dataframe\n",
    "                a matric that still is a dataframe, having as indexes gene_tss, readid, group, cluster, \n",
    "                and the bins as columns, and as value the mean methylation value per bin\n",
    "        \"\"\"\n",
    "\n",
    "        #Creating a new 'bin' column:\n",
    "        if 'C_pos' in df.columns:\n",
    "                df['bin'] = (df['C_pos']//bin_size)*bin_size\n",
    "\n",
    "\n",
    "        #Pivot the dataframe into a matrix (but it's still a dataframe)\n",
    "        if 'readid' and 'group' in df.columns:\n",
    "                df = df.pivot_table(index=indexes, columns='bin', values='meth', aggfunc='mean')\n",
    "        \n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. PROCESSING -- MARKING THE METHYLATION OF READS BY BINNED REGIONS -- GIVING OUT A DATAFRAME\n",
    "\n",
    "def process_into_matrix(\n",
    "        df: pd.DataFrame,\n",
    "        gene: object=None,          #the id taken from the column gene_tss, only if we want to process per gene\n",
    "        bin_size: int = 50,         #bin size, by default 50bp\n",
    "):\n",
    "\n",
    "    \"\"\"\n",
    "    Process the DataFrame to turn it into a binned methylation value matrix:\n",
    "    CAN EITHER BE USED ON THE WHOLE DATAFRAME WITH ALL THE GENES, OR ON A SINGLE GENE\n",
    "\n",
    "    1. Binning on the GpC site position, relative to the Pol2 summit, by default bin size = 50bp\n",
    "    2. Making a dataframe 'matrix' having as indexes: 'readid','group' and 'cluster' if processed per gene, adding 'gene_tss' if processed entirely\n",
    "       as colums: 'bin', and as values: 'meth' (mean methylation value per bin)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input preprocessed dataframe \n",
    "    gene : object \n",
    "        Name of the gene, that we want to isolate and work on, BY DEFAULT SSUMING THAT WE WORK ON THE WHOLE DATAFRAME\n",
    "    bin_size : integer\n",
    "        By default set as 50 bp\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "     \n",
    "    pd.DataFrame\n",
    "        with missing values, not yet a standardized matrix, ready to be processed in a PCA, and for treatments.\n",
    "    \"\"\"\n",
    "\n",
    "    if gene != None:\n",
    "        #filtering out to get the new dataframe to work on\n",
    "        df=df[df['gene_tss']== gene]\n",
    "\n",
    "        #Binning and then turning into a methylation matrix\n",
    "        df= bin_then_matrix(df, indexes= ['readid','group','cluster'], bin_size= bin_size)\n",
    "\n",
    "    else:\n",
    "        df = bin_then_matrix(df, indexes= ['gene_tss','readid','group','cluster'], bin_size= bin_size)\n",
    "\n",
    "     \n",
    "    print(f\"Shape of the dataframe : {df.shape}\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. TO HANDLE THE NAN VALUES ONCE IN MATRIX FORM\n",
    "\n",
    "def handling_NaN(\n",
    "        df: pd.DataFrame, # must be binned and in matrix form\n",
    "        nan_threshold: float = 0.7, #limit to drop bins that have too many missing values\n",
    "        nan_method: object = None \n",
    "\n",
    "):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "\n",
    "        nan_threshold: float\n",
    "        threshold for dropping bins that have too many missing values\n",
    "        nan_method : string\n",
    "        Choosing the method on how to deal with the missing methylation values, either by 'drop' or by 'impute', or eavint the NaN there. By default leaving them there\n",
    "\n",
    "        Returns:\n",
    "        df: the dataframe matrix with or without missing values depending on the nan method\n",
    "\n",
    "        \"\"\"\n",
    "        #Dropping the reads that contain NaN values\n",
    "        if nan_method == 'drop':\n",
    "                \n",
    "                # Drop bins with too many NaN values\n",
    "                coverage_mask = df.notna().mean(axis=0) > nan_threshold   #dropping columns that have too many NaN\n",
    "                dropped_columns = df.columns[~coverage_mask].tolist()      #lst of dropped columns\n",
    "                df = df.loc[:, coverage_mask]                         #new df only keeping useful columns\n",
    "\n",
    "                print(\"Dropped (NaN coverage):\", dropped_columns)                   #lists the dropped columns\n",
    "\n",
    "                i,j = df.shape\n",
    "                #filter by dropping the reads that have too many NaN\n",
    "                df = df.dropna(thresh=j)       # Keep rows with at least j non-NaN values (= dropping all rows that had a NaN)\n",
    "        \n",
    "        #Imputing the values with the kNN method\n",
    "        elif nan_method == 'impute':\n",
    "                \n",
    "                # Drop bins with too many NaN values\n",
    "                coverage_mask = df.notna().mean(axis=0) > nan_threshold   #dropping columns that have too many NaN\n",
    "                dropped_columns = df.columns[~coverage_mask].tolist()      #lst of dropped columns\n",
    "                df = df.loc[:, coverage_mask]                         #new df only keeping useful columns\n",
    "\n",
    "                print(\"Dropped (NaN coverage):\", dropped_columns)                   #lists the dropped columns\n",
    "\n",
    "                imputer = KNNImputer(n_neighbors=5)\n",
    "                X = imputer.fit_transform(df) #X_impute is a numpy array\n",
    "                df.loc[:, :] = X        # Put results back into the same DataFrame\n",
    "\n",
    "        #leaving the missing values otherwise:\n",
    "        \n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6. FUNCTION TO PREPARE THE DATAFRAME IN LONG FORMAT FOR PLOTTING READS\n",
    "def preprocess_long_for_plot(df, include_locus_cluster: bool = False):\n",
    "    \"\"\"\n",
    "    Prepare dataframe in long format for plotting reads.\n",
    "    Each row = one CpG per read.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe with columns at least ['gene_tss', 'group', 'cluster', 'pol2_pos', \"readid\", \"C_start\", \"meth\", 'locus_cluster'].\n",
    "    include_locus_cluster : bool, optional\n",
    "        If True, keep 'locus_cluster' column (for plotting after clustering).\n",
    "        If False, ignore it.\n",
    "    \"\"\"\n",
    "    df = df.reset_index(drop=True)  # ensure features are columns\n",
    "    \n",
    "    # Define base columns\n",
    "    base_cols = [\"gene_tss\", \"group\", \"readid\", \"cluster\", \"C_start\", \"meth\",'pol2_pos']\n",
    "    keep_cols = [c for c in df.columns if c in base_cols]\n",
    "\n",
    "    # Optionally add locus_cluster if it exists\n",
    "    if include_locus_cluster and \"locus_cluster\" in df.columns:\n",
    "        keep_cols.append(\"locus_cluster\")\n",
    "\n",
    "    df = df[keep_cols]\n",
    "\n",
    "    # Compute per-read start/end (no lists!)\n",
    "    span = (\n",
    "        df.groupby(\"readid\")[\"C_start\"]\n",
    "        .agg([\"min\", \"max\"])\n",
    "        .rename(columns={\"min\": \"read_start\", \"max\": \"read_end\"})\n",
    "    )\n",
    "    df = df.merge(span, on=\"readid\", how=\"left\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 7. FUNCTION TO PLOT THE READS IN LONG FORMAT    \n",
    "def plot_reads_long(\n",
    "    df,\n",
    "    filters=None,\n",
    "    facet_by=None,\n",
    "    color_by=None,\n",
    "    hex_colors=None,\n",
    "    max_facets=14\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot per-read methylation (long format, one row per CpG per read), centered on Pol2 position.\n",
    "    Facet heights scale with number of reads per facet.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Long-format dataframe (one row per CpG per read).\n",
    "        Must have 'read_start', 'read_end', and 'pol2_pos' columns.\n",
    "    filters : dict, optional\n",
    "        Column:value filters, e.g. {\"gene_tss\": \"BRCA1\", \"group\": \"control\"}.\n",
    "    facet_by : str or list, optional\n",
    "        Column(s) to facet subplots by (e.g. \"group\", [\"gene_tss\",\"group\"]).\n",
    "    color_by : str, optional\n",
    "        Column to color read spans by (e.g. \"cluster\").\n",
    "    hex_colors : list, optional\n",
    "        List of hex colors to use for categories in `color_by`.\n",
    "    max_facets : int\n",
    "        Prevents generating too many subplots at once.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # --- Filtering ---\n",
    "    if filters:\n",
    "        for col, val in filters.items():\n",
    "            if col not in df.columns:\n",
    "                raise ValueError(f\"Column '{col}' not found in dataframe.\")\n",
    "            if isinstance(val, list):\n",
    "                df = df[df[col].isin(val)]\n",
    "            else:\n",
    "                df = df[df[col] == val]\n",
    "\n",
    "    if df.empty:\n",
    "        raise ValueError(\"No reads left after filtering!\")\n",
    "\n",
    "    # --- Faceting ---\n",
    "    if facet_by is None:\n",
    "        facet_values = [(\"All\", df)]\n",
    "    else:\n",
    "        if isinstance(facet_by, str):\n",
    "            facet_by = [facet_by]\n",
    "        facet_values = list(df.groupby(facet_by))\n",
    "        if len(facet_values) > max_facets:\n",
    "            raise ValueError(f\"Too many facets ({len(facet_values)}). Max allowed: {max_facets}\")\n",
    "\n",
    "    # --- Color mapping ---\n",
    "    color_map = None\n",
    "    if color_by:\n",
    "        if color_by not in df.columns:\n",
    "            raise ValueError(f\"Column '{color_by}' not found in dataframe.\")\n",
    "        categories = sorted(df[color_by].dropna().unique())\n",
    "        if hex_colors is None:\n",
    "            import matplotlib.cm as cm\n",
    "            cmap = cm.get_cmap(\"tab20\", len(categories))\n",
    "            hex_colors = [cmap(i) for i in range(len(categories))]\n",
    "        if len(hex_colors) < len(categories):\n",
    "            raise ValueError(f\"Not enough colors for {len(categories)} categories.\")\n",
    "        color_map = dict(zip(categories, hex_colors))\n",
    "\n",
    "    # --- Figure setup with gridspec ---\n",
    "    n_facets = len(facet_values)\n",
    "    heights = [max(1, subdf[\"readid\"].nunique() * 0.15) for _, subdf in facet_values]  # scale heights\n",
    "    total_height = sum(heights) + 2  # add some padding\n",
    "    fig = plt.figure(figsize=(30, total_height))\n",
    "    gs = gridspec.GridSpec(n_facets, 1, height_ratios=heights)\n",
    "\n",
    "    # --- Plot each facet ---\n",
    "    for i, (facet_key, subdf) in enumerate(facet_values):\n",
    "        ax = fig.add_subplot(gs[i, 0])\n",
    "        ax.set_facecolor(\"#919191\")  # light gray background\n",
    "\n",
    "        if \"pol2_pos\" not in subdf.columns:\n",
    "            raise ValueError(\"Column 'pol2_pos' not found for centering!\")\n",
    "\n",
    "        # Shift positions relative to pol2_pos\n",
    "        subdf = subdf.copy()\n",
    "        subdf[\"C_start_shifted\"] = subdf[\"C_start\"] - subdf[\"pol2_pos\"]\n",
    "        subdf[\"read_start_shifted\"] = subdf[\"read_start\"] - subdf[\"pol2_pos\"]\n",
    "        subdf[\"read_end_shifted\"] = subdf[\"read_end\"] - subdf[\"pol2_pos\"]\n",
    "\n",
    "        # Order reads by color_by if requested\n",
    "        if color_by and color_by in subdf.columns:\n",
    "            grouped_reads = subdf.groupby(color_by)[\"readid\"].unique().to_dict()\n",
    "            read_order = [(cat, rid) for cat, rids in grouped_reads.items() for rid in rids]\n",
    "        else:\n",
    "            read_order = [(None, rid) for rid in subdf[\"readid\"].unique()]\n",
    "\n",
    "        # Plot each read\n",
    "        for idx, (cat_value, readid) in enumerate(read_order):\n",
    "            sub = subdf[subdf[\"readid\"] == readid]\n",
    "            span_color = color_map.get(cat_value, \"black\") if color_by else \"black\"\n",
    "\n",
    "            ax.hlines(\n",
    "                idx,\n",
    "                xmin=sub[\"read_start_shifted\"].iloc[0],\n",
    "                xmax=sub[\"read_end_shifted\"].iloc[0],\n",
    "                color=span_color,\n",
    "                linewidth=1.6\n",
    "            )\n",
    "\n",
    "            # CpG dots\n",
    "            for _, row in sub.iterrows():\n",
    "                dot_color = \"white\" if row[\"meth\"] == 1 else \"black\"\n",
    "                ax.plot(row[\"C_start_shifted\"], idx, \"o\", color=dot_color, markersize=4)\n",
    "\n",
    "        # Vertical line at Pol2\n",
    "        ax.axvline(x=0, color=\"#C80028\", linestyle=\"-\", linewidth=2, label=\"Pol2 position\")\n",
    "\n",
    "        # Labels, limits, formatting\n",
    "        ax.set_xlabel(\"Position relative to Pol2 (bp)\", fontsize=12)\n",
    "        ax.set_ylabel(\"Read IDs\", fontsize=12)\n",
    "        x_min = subdf[\"read_start_shifted\"].min() - 100\n",
    "        x_max = subdf[\"read_end_shifted\"].max() + 100\n",
    "        ax.set_xlim(x_min, x_max)\n",
    "        ax.set_ylim(-2, len(read_order) + 2)\n",
    "\n",
    "        # Facet title\n",
    "        if facet_by:\n",
    "            if isinstance(facet_key, tuple):\n",
    "                title = \", \".join([f\"{col}={val}\" for col, val in zip(facet_by, facet_key)])\n",
    "            else:\n",
    "                title = f\"{facet_by[0]}={facet_key}\"\n",
    "        else:\n",
    "            title = \"All Reads\"\n",
    "        ax.set_title(f\"Read-level methylation centered on Pol2 ({title})\", fontsize=16)\n",
    "        ax.grid(True)\n",
    "        ax.invert_yaxis()\n",
    "\n",
    "    # --- Legend ---\n",
    "    handles = [\n",
    "        mlines.Line2D([], [], color=\"black\", marker=\"o\", linestyle=\"None\", markersize=6, label=\"Unmethylated (0)\"),\n",
    "        mlines.Line2D([], [], color=\"black\", marker=\"o\", markerfacecolor=\"white\", linestyle=\"None\", markersize=6, label=\"Methylated (1)\"),\n",
    "        mlines.Line2D([], [], color=\"#C80028\", linestyle=\"-\", linewidth=2, label=\"Pol2 position\")\n",
    "    ]\n",
    "    if color_map:\n",
    "        for cat, col in color_map.items():\n",
    "            handles.append(mlines.Line2D([], [], color=col, linewidth=2, label=f\"{color_by}={cat}\"))\n",
    "\n",
    "    fig.legend(handles=handles, loc=\"upper right\", fontsize=9, frameon=True)\n",
    "    fig.tight_layout()\n",
    "    plt.subplots_adjust(top=0.93, bottom=0.05)\n",
    "\n",
    "    # return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 8. SMALL FUNCTION TO GET THE LIST OF ALL THE GENES NAMES OF A DATAFRAME AND THE LIST OF ALL THE SUBDATAFRAME OF THESE GENES\n",
    "\n",
    "def get_genes_list(df):\n",
    "    gene_list_names= [] #the list that will take in the genes names\n",
    "    gene_list_df=[] #the list that will take in the dataframes associated to the genes\n",
    "\n",
    "    for gene, sub_df in df.groupby(df.index.get_level_values(\"gene_tss\")):\n",
    "        gene_list_names.append(gene)\n",
    "        gene_list_df.append(sub_df)\n",
    "\n",
    "    return gene_list_names,gene_list_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "##### Functions coded for the clustering:\n",
    "- 'clustering_final'\n",
    "- (unused) (trying to sweep parameters to get exact number of clusters)\n",
    "- 'run_pipelines_on_genes' (to violin scatter plot the metrics)\n",
    "- 'plot_violin_scatter'\n",
    "- 'plot_compare_pipelines_grid'\n",
    "- 'silhouettes_multi'\n",
    "- 'alternate_clustering' ( and wit it 'filter_by_missingness', 'masked_pearson_correlationmatrix')\n",
    "- 'plot_umap'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. IMPROVED CLUSTERING ALGORITHM, FOLLOWING THE STEPS: PCA --> ADAPTIVE KNN GRAPH (WEIGHTED) --> LEIDEN\n",
    "def clustering_final(\n",
    "    df,\n",
    "    n_neighbors=15,\n",
    "    nan_threshold : float = 0.7, #to drop the bins that have too many nan\n",
    "    nan_method : str = 'drop', #by default dropping the rows that contain nan, but can use 'impute' too\n",
    "    scaling : bool = False, #whether to scale the data or not\n",
    "    pca_or_not = True, #whether to do a pca or not\n",
    "    n_pcs= None ,# if None: by default 0.95 variance, otherwise int number of pcs to keep if pca is done\n",
    "    metric='cosine', #cosine or euclidean\n",
    "    transform='none', # 'none', 'logit', or 'arcsine'\n",
    "    kernel_type='laplacian', # 'laplacian' or 'gaussian'\n",
    "    leiden_resolution=1.0,\n",
    "    seed=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform Leiden clustering on PCA-reduced data with adaptive similarity weighting.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Input data (numeric).\n",
    "    n_neighbors : int, default=15\n",
    "        Number of neighbors for kNN graph construction.\n",
    "    n_pcs : int, default=30\n",
    "        Number of principal components to retain.\n",
    "    metric : str, default='cosine'\n",
    "        Distance metric for nearest neighbors.\n",
    "    transform : {'none', 'logit', 'sqrt'}, default='none'\n",
    "        Optional transformation applied to data before PCA.\n",
    "    leiden_resolution : float, default=1.0\n",
    "        Resolution parameter for the Leiden algorithm.\n",
    "    seed : int, default=42\n",
    "        Random seed for reproducibility.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    clusters : np.ndarray\n",
    "        Cluster assignments for each observation.\n",
    "    part : leidenalg.Partition\n",
    "        Leiden partition object.\n",
    "    X_pca : np.ndarray\n",
    "        PCA-transformed coordinates.\n",
    "    \"\"\"\n",
    "    #Handling the NaN:\n",
    "\n",
    "    df= handling_NaN(df, nan_threshold, nan_method)\n",
    "    \n",
    "    # --- 1) Convert to NumPy and handle NaNs ---\n",
    "    X = df.to_numpy(dtype=float)\n",
    "\n",
    "    # assert no NaNs remain if nan_method='drop'\n",
    "    if np.isnan(X).any():\n",
    "        raise ValueError(\"NaNs remain after handling_NaN; impute explicitly before PCA.\")\n",
    "    \n",
    "    # --- 2) Optional transformation ---\n",
    "    if transform == 'logit':\n",
    "        Xc = np.clip(X, 1e-3, 1 - 1e-3)\n",
    "        Xc = np.log(Xc / (1 - Xc))\n",
    "    elif transform == 'arcsine':\n",
    "        Xc = np.arcsin(np.sqrt(np.clip(X, 0.0, 1.0)))\n",
    "    elif transform == 'none':\n",
    "        Xc = X\n",
    "    else:\n",
    "        raise ValueError(\"transform must be 'none', 'logit', or 'arcsine'\")\n",
    "\n",
    "    # # # 3) Standardize features\n",
    "    if scaling:\n",
    "        scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "        Xc = scaler.fit_transform(Xc)\n",
    "\n",
    "    # --- 3) PCA reduction ---\n",
    "    if pca_or_not == True:\n",
    "        if n_pcs is None:\n",
    "            pca = PCA(n_components=0.95) # if i want to choose the number of components that keep 95% of the variance\n",
    "        elif isinstance(n_pcs, int) and n_pcs > 0:\n",
    "            n_pcs = min(n_pcs, min(Xc.shape) - 1)\n",
    "        \n",
    "        pca = PCA(n_components=n_pcs, svd_solver='auto', random_state=seed) #if i want to manually define pca components\n",
    "\n",
    "        X_pca = pca.fit_transform(np.nan_to_num(Xc, nan=0.0))\n",
    "        print(\"Number of components chosen:\", pca.n_components_)\n",
    "    else:\n",
    "        X_pca = Xc  # Skip PCA \n",
    "        \n",
    "    if metric == 'cosine' and not pca_or_not:\n",
    "        X_pca = normalize(X_pca, norm='l2', axis=1) \n",
    "    N = X_pca.shape[0]\n",
    "\n",
    "    # --- kNN graph ---\n",
    "    nn = NearestNeighbors(n_neighbors=n_neighbors, metric=metric)\n",
    "    nn.fit(X_pca)\n",
    "    dist, idx = nn.kneighbors(X_pca, return_distance=True)  # dist, idx: (N, k)\n",
    "\n",
    "    # --- Detect if self is included ---\n",
    "    self_included = np.all(idx[:, 0] == np.arange(N))\n",
    "    eps = 1e-12\n",
    "\n",
    "    # --- Adaptive kernel construction ---\n",
    "    if kernel_type == 'laplacian':\n",
    "        # Per-node scale τ_i: median neighbor distance (exclude self if present)\n",
    "        dist_for_scale = dist[:, 1:] if self_included else dist #if self is included, drop the first column\n",
    "        tau = np.median(dist_for_scale, axis=1) + 1e-6  # (N,)\n",
    "\n",
    "        \n",
    "        # Similarity per neighbor (N, k), asymmetric (depends on i only)\n",
    "        sim = np.exp(-dist / tau[:, None]) \n",
    "\n",
    "    elif kernel_type == 'gaussian':\n",
    "        # Local scales σ_i = distance to k-th neighbor (last col)\n",
    "        sigma = dist[:, -1] + eps  # (N,)\n",
    "\n",
    "        # σ_j per neighbor via indexing\n",
    "        sigma_j = sigma[idx]       # (N, k)\n",
    "        sigma_i = sigma[:, None]   # (N, 1)\n",
    "\n",
    "        # Symmetric adaptive Gaussian weights per i→j neighbor\n",
    "        sim = np.exp(- (dist ** 2) / (sigma_i * sigma_j + eps))\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"kernel_type must be 'laplacian' or 'gaussian'\")\n",
    "\n",
    "    # --- Build undirected weighted graph (union kNN with max weight) ---\n",
    "    edges = {}\n",
    "\n",
    "    for i in range(N):\n",
    "        if self_included:\n",
    "            neigh_idx = idx[i, 1:]\n",
    "            neigh_w = sim[i, 1:]\n",
    "        else:\n",
    "            neigh_idx = idx[i, :]\n",
    "            neigh_w = sim[i, :]\n",
    "\n",
    "        for j, w in zip(neigh_idx, neigh_w):\n",
    "            if i == j or w <= 0:\n",
    "                continue\n",
    "\n",
    "            a, b = (i, j) if i < j else (j, i)\n",
    "            edges[(a, b)] = max(edges.get((a, b), 0.0), float(w))\n",
    "\n",
    "    # Build graph\n",
    "    e_list = list(edges.keys())\n",
    "    w_list = [edges[e] for e in e_list]\n",
    "\n",
    "    g = ig.Graph(n=N, edges=e_list, directed=False)\n",
    "    g.es[\"weight\"] = w_list\n",
    "\n",
    "    # Leiden\n",
    "    part = la.find_partition(\n",
    "        g,\n",
    "        la.RBConfigurationVertexPartition,\n",
    "        weights=g.es['weight'],\n",
    "        resolution_parameter=leiden_resolution,\n",
    "        seed=seed\n",
    "    )\n",
    "\n",
    "    clusters = np.array(part.membership)\n",
    "\n",
    "    # 9) Quality metrics\n",
    "    metrics = {}\n",
    "\n",
    "    # Silhouette\n",
    "    try:\n",
    "        sil = silhouette_score(X_pca, clusters, metric=metric)\n",
    "    except Exception:\n",
    "        D = pairwise_distances(X_pca, metric=metric)\n",
    "        sil = silhouette_score(D, clusters, metric='precomputed')\n",
    "\n",
    "    metrics['silhouette'] = sil if len(np.unique(clusters)) > 1 else None\n",
    "\n",
    "    # Calinski–Harabasz and Davies–Bouldin\n",
    "    try:\n",
    "        metrics['calinski_harabasz'] = (\n",
    "            calinski_harabasz_score(X_pca, clusters)\n",
    "            if len(np.unique(clusters)) > 1 else None\n",
    "        )\n",
    "    except Exception:\n",
    "        metrics['calinski_harabasz'] = None\n",
    "\n",
    "    try:\n",
    "        metrics['davies_bouldin'] = (\n",
    "            davies_bouldin_score(X_pca, clusters)\n",
    "            if len(np.unique(clusters)) > 1 else None\n",
    "        )\n",
    "    except Exception:\n",
    "        metrics['davies_bouldin'] = None\n",
    "\n",
    "    # Leiden objective value\n",
    "    try:\n",
    "        metrics['leiden_quality'] = float(part.quality())\n",
    "    except Exception:\n",
    "        metrics['leiden_quality'] = None\n",
    "\n",
    "    # Weighted modularity\n",
    "    try:\n",
    "        metrics['modularity'] = g.modularity(clusters.tolist(), weights=g.es['weight'])\n",
    "    except Exception:\n",
    "        metrics['modularity'] = None\n",
    "\n",
    "    # Cluster sizes\n",
    "    unique, counts = np.unique(clusters, return_counts=True)\n",
    "    metrics['cluster_sizes'] = dict(zip(unique.tolist(), counts.tolist()))\n",
    "\n",
    "\n",
    "    return df, X_pca, part, clusters, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## SWEEP TO FIND THE OPTIMAL PARAMETERS TO GET 6 CLUSTERS\n",
    "\n",
    "# # 1) Prepare embedding once (transform -> optional scale -> PCA)\n",
    "# def prepare_embedding(\n",
    "#     df,\n",
    "#     nan_threshold=0.7,\n",
    "#     nan_method='drop',         # your handling_NaN must implement this\n",
    "#     transform='arcsine',       # 'none' | 'logit' | 'arcsine'\n",
    "#     scaling=False,\n",
    "#     pca_or_not=True,\n",
    "#     n_pcs=None,                # None => 95% variance\n",
    "#     seed=42\n",
    "# ):\n",
    "#     \"\"\"Clean, transform, optionally scale, and perform PCA on data.\"\"\"\n",
    "\n",
    "#     df_used = handling_NaN(df, nan_threshold, nan_method)\n",
    "#     X = df_used.to_numpy(float)\n",
    "\n",
    "#     if np.isnan(X).any():\n",
    "#         raise ValueError(\"NaNs remain after handling_NaN; impute before PCA.\")\n",
    "\n",
    "#     # --- Transform ---\n",
    "#     if transform == 'logit':\n",
    "#         X = np.clip(X, 1e-3, 1 - 1e-3)\n",
    "#         X = np.log(X / (1 - X))\n",
    "#     elif transform == 'arcsine':\n",
    "#         X = np.arcsin(np.sqrt(np.clip(X, 0.0, 1.0)))\n",
    "#     elif transform == 'none':\n",
    "#         pass\n",
    "#     else:\n",
    "#         raise ValueError(\"transform must be 'none', 'logit', or 'arcsine'\")\n",
    "\n",
    "#     # --- Optional scaling ---\n",
    "#     if scaling:\n",
    "#         X = StandardScaler(with_mean=True, with_std=True).fit_transform(X)\n",
    "\n",
    "#     # --- PCA ---\n",
    "#     if pca_or_not:\n",
    "#         if n_pcs is None:\n",
    "#             pca = PCA(n_components=0.95, random_state=seed)\n",
    "#         else:\n",
    "#             n_pcs = min(int(n_pcs), min(X.shape) - 1)\n",
    "#             pca = PCA(n_components=n_pcs, random_state=seed)\n",
    "\n",
    "#         X_pca = pca.fit_transform(X)\n",
    "#         n_comp = pca.n_components_\n",
    "#     else:\n",
    "#         X_pca = X\n",
    "#         n_comp = X_pca.shape[1]\n",
    "\n",
    "#     return df_used, X_pca, n_comp\n",
    "\n",
    "\n",
    "# # 2) Compute kNN once at max_k, slice for smaller k\n",
    "# def kneighbors_upto_k(X, max_k, metric='euclidean'):\n",
    "#     \"\"\"Compute neighbors up to max_k and reuse slices for smaller k.\"\"\"\n",
    "#     nn = NearestNeighbors(n_neighbors=int(max_k), metric=metric)\n",
    "#     nn.fit(X)\n",
    "#     dist, idx = nn.kneighbors(X, return_distance=True)  # (N, max_k)\n",
    "\n",
    "#     self_included = np.all(idx[:, 0] == np.arange(X.shape[0]))\n",
    "#     return dist, idx, self_included\n",
    "\n",
    "\n",
    "# # 3) Build graph from a slice of neighbors (vectorized)\n",
    "# def graph_from_neighbors(dist_slice, idx_slice, self_included, kernel_type='laplacian'):\n",
    "#     \"\"\"Construct undirected weighted graph from neighbor distances.\"\"\"\n",
    "#     N, k = dist_slice.shape\n",
    "\n",
    "#     # --- Adaptive scales ---\n",
    "#     dist_for_scale = dist_slice[:, 1:] if self_included else dist_slice\n",
    "#     tau = np.median(dist_for_scale, axis=1) + 1e-6\n",
    "\n",
    "#     # --- Similarity (Laplacian kernel) ---\n",
    "#     sim = np.exp(-dist_slice / tau[:, None])\n",
    "\n",
    "#     # --- Vectorized edge construction: union kNN with max weight ---\n",
    "#     I = np.repeat(np.arange(N), k)\n",
    "#     J = idx_slice.ravel()\n",
    "#     W = sim.ravel()\n",
    "\n",
    "#     # Drop self edges\n",
    "#     mask = I != J\n",
    "#     I, J, W = I[mask], J[mask], W[mask]\n",
    "\n",
    "#     # Undirected: keep i < j, combine duplicates with max weight\n",
    "#     a = np.minimum(I, J)\n",
    "#     b = np.maximum(I, J)\n",
    "\n",
    "#     edf = pd.DataFrame({'a': a, 'b': b, 'w': W})\n",
    "#     edf = edf.groupby(['a', 'b'], as_index=False)['w'].max()\n",
    "\n",
    "#     g = ig.Graph(n=N, edges=list(zip(edf['a'], edf['b'])), directed=False)\n",
    "#     g.es['weight'] = edf['w'].to_numpy()\n",
    "\n",
    "#     return g\n",
    "\n",
    "\n",
    "# # 4) Run Leiden and compute metrics (optionally sample for silhouette)\n",
    "# def run_leiden_and_metrics(\n",
    "#     g,\n",
    "#     X_embed,\n",
    "#     resolution,\n",
    "#     metric='euclidean',\n",
    "#     seed=42,\n",
    "#     silhouette_sample=None\n",
    "# ):\n",
    "#     \"\"\"Run Leiden clustering and compute evaluation metrics.\"\"\"\n",
    "#     part = la.find_partition(\n",
    "#         g,\n",
    "#         la.RBConfigurationVertexPartition,\n",
    "#         weights=g.es['weight'],\n",
    "#         resolution_parameter=float(resolution),\n",
    "#         seed=int(seed)\n",
    "#     )\n",
    "#     labels = np.array(part.membership)\n",
    "\n",
    "#     out = {\n",
    "#         'part': part,\n",
    "#         'clusters': labels,\n",
    "#         'n_clusters': int(len(np.unique(labels))),\n",
    "#         'leiden_quality': float(part.quality()),\n",
    "#         'modularity': float(g.modularity(labels.tolist(), weights=g.es['weight']))\n",
    "#     }\n",
    "\n",
    "#     # --- Only compute metrics if >1 cluster ---\n",
    "#     if out['n_clusters'] > 1:\n",
    "#         X_eval, y_eval = X_embed, labels\n",
    "\n",
    "#         if silhouette_sample is not None and X_embed.shape[0] > silhouette_sample:\n",
    "#             rng = np.random.default_rng(seed)\n",
    "#             idx = rng.choice(X_embed.shape[0], size=int(silhouette_sample), replace=False)\n",
    "#             X_eval = X_embed[idx]\n",
    "#             y_eval = labels[idx]\n",
    "\n",
    "#         try:\n",
    "#             out['silhouette'] = float(silhouette_score(X_eval, y_eval, metric=metric))\n",
    "#         except Exception:\n",
    "#             out['silhouette'] = None\n",
    "\n",
    "#         try:\n",
    "#             out['calinski_harabasz'] = float(calinski_harabasz_score(X_eval, y_eval))\n",
    "#         except Exception:\n",
    "#             out['calinski_harabasz'] = None\n",
    "\n",
    "#         try:\n",
    "#             out['davies_bouldin'] = float(davies_bouldin_score(X_eval, y_eval))\n",
    "#         except Exception:\n",
    "#             out['davies_bouldin'] = None\n",
    "\n",
    "#     else:\n",
    "#         out['silhouette'] = None\n",
    "#         out['calinski_harabasz'] = None\n",
    "#         out['davies_bouldin'] = None\n",
    "\n",
    "#     return out\n",
    "\n",
    "\n",
    "# # 5) Adaptive resolution search on a fixed graph (fast)\n",
    "# def search_resolution_on_graph(\n",
    "#     g,\n",
    "#     X_embed,\n",
    "#     target=6,\n",
    "#     res_init=1.0,\n",
    "#     res_min=0.02,\n",
    "#     res_max=10.0,\n",
    "#     up_factor=1.5,\n",
    "#     max_iter=12,\n",
    "#     metric='euclidean',\n",
    "#     seed=42,\n",
    "#     silhouette_sample=None\n",
    "# ):\n",
    "#     \"\"\"Search for Leiden resolution yielding target cluster count.\"\"\"\n",
    "#     history = []\n",
    "#     res = float(res_init)\n",
    "\n",
    "#     for _ in range(int(max_iter)):\n",
    "#         out = run_leiden_and_metrics(\n",
    "#             g, X_embed, resolution=res,\n",
    "#             metric=metric, seed=seed, silhouette_sample=silhouette_sample\n",
    "#         )\n",
    "#         out['resolution'] = res\n",
    "#         history.append(out)\n",
    "\n",
    "#         c = out['n_clusters']\n",
    "#         if c == int(target):\n",
    "#             break\n",
    "#         if c < target:\n",
    "#             res = min(res * up_factor, res_max)\n",
    "#         else:\n",
    "#             res = max(res / up_factor, res_min)\n",
    "\n",
    "#     return history\n",
    "\n",
    "\n",
    "# # 6) Full sweep over k (reusing PCA and kNN)\n",
    "# def sweep_k_for_target_fast(\n",
    "#     df,\n",
    "#     k_list,\n",
    "#     target=6,\n",
    "#     transform='arcsine',       # or 'logit'\n",
    "#     metric='euclidean',\n",
    "#     scaling=False,\n",
    "#     pca_or_not=True,\n",
    "#     n_pcs=None,\n",
    "#     kernel_type='laplacian',\n",
    "#     nan_threshold=0.7,\n",
    "#     nan_method='drop',\n",
    "#     res_init=1.0,\n",
    "#     res_min=0.02,\n",
    "#     res_max=10.0,\n",
    "#     up_factor=1.5,\n",
    "#     max_iter=12,\n",
    "#     silhouette_sample=None,    # e.g., 2000 to speed up silhouette on large N\n",
    "#     seed=42\n",
    "# ):\n",
    "#     \"\"\"Sweep over multiple k values to find best clustering.\"\"\"\n",
    "#     # --- Prepare embedding once ---\n",
    "#     df_used, X_pca, n_comp = prepare_embedding(\n",
    "#         df, nan_threshold, nan_method, transform,\n",
    "#         scaling, pca_or_not, n_pcs, seed\n",
    "#     )\n",
    "\n",
    "#     # --- Compute neighbors once at max k ---\n",
    "#     max_k = int(max(k_list))\n",
    "#     dist_max, idx_max, self_included = kneighbors_upto_k(\n",
    "#         X_pca, max_k=max_k, metric=metric\n",
    "#     )\n",
    "\n",
    "#     all_rows = []\n",
    "#     best = None\n",
    "#     best_score = (-np.inf, np.inf)  # (silhouette, |n_clusters - target|)\n",
    "\n",
    "#     # --- Loop over k ---\n",
    "#     for k in k_list:\n",
    "#         k = int(k)\n",
    "#         dist = dist_max[:, :k]\n",
    "#         idx = idx_max[:, :k]\n",
    "\n",
    "#         # Build graph for this k\n",
    "#         g = graph_from_neighbors(dist, idx, self_included, kernel_type=kernel_type)\n",
    "\n",
    "#         # Sweep resolution on this fixed graph\n",
    "#         hist = search_resolution_on_graph(\n",
    "#             g, X_pca, target=target, res_init=res_init, res_min=res_min,\n",
    "#             res_max=res_max, up_factor=up_factor, max_iter=max_iter,\n",
    "#             metric=metric, seed=seed, silhouette_sample=silhouette_sample\n",
    "#         )\n",
    "\n",
    "#         # Collect summary\n",
    "#         for h in hist:\n",
    "#             all_rows.append({\n",
    "#                 'k': k,\n",
    "#                 'resolution': h['resolution'],\n",
    "#                 'n_clusters': h['n_clusters'],\n",
    "#                 'silhouette': h['silhouette'],\n",
    "#                 'calinski_harabasz': h['calinski_harabasz'],\n",
    "#                 'davies_bouldin': h['davies_bouldin'],\n",
    "#                 'leiden_quality': h['leiden_quality'],\n",
    "#                 'modularity': h['modularity']\n",
    "#             })\n",
    "\n",
    "#         # Select best for this k (prefer exact target + high silhouette)\n",
    "#         best_k_run = sorted(\n",
    "#             hist,\n",
    "#             key=lambda h: (\n",
    "#                 -(h['n_clusters'] == target),\n",
    "#                 h['silhouette'] or -1e9,\n",
    "#                 -abs(h['n_clusters'] - target)\n",
    "#             )\n",
    "#         )[-1]\n",
    "\n",
    "#         # Track global best\n",
    "#         cur_sil = best_k_run['silhouette'] or -np.inf\n",
    "#         cur_delta = abs(best_k_run['n_clusters'] - target)\n",
    "\n",
    "#         if (cur_sil > best_score[0]) or (cur_sil == best_score[0] and cur_delta < best_score[1]):\n",
    "#             best_score = (cur_sil, cur_delta)\n",
    "#             best = {\n",
    "#                 'k': k,\n",
    "#                 'resolution': best_k_run['resolution'],\n",
    "#                 'n_clusters': best_k_run['n_clusters'],\n",
    "#                 'clusters': best_k_run['clusters'],\n",
    "#                 'part': best_k_run['part'],\n",
    "#                 'graph': g,\n",
    "#                 'X_pca': X_pca,\n",
    "#                 'df_used': df_used\n",
    "#             }\n",
    "\n",
    "#     summary = pd.DataFrame(all_rows)\n",
    "#     return summary, best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipelines_on_genes(gene_names, gene_df, pipeline_configs):\n",
    "    \"\"\"\n",
    "    Run multiple clustering pipelines on multiple gene dataframes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    genes_names\n",
    "        List of gene names\n",
    "    genes_df\n",
    "        List of the dataframes associated with these genes\n",
    "    pipeline_configs : list of dict\n",
    "        Each dictionary must contain at least {'name': str}, plus kwargs for clustering_final().\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    results_df : pandas.DataFrame\n",
    "        Tidy DataFrame with clustering metrics per gene × pipeline.\n",
    "    outputs : dict\n",
    "        Nested dictionary for detailed outputs:\n",
    "        outputs[gene_id][pipeline_name] = {\n",
    "            'X_pca', 'part', 'clusters', 'metrics', 'graph'\n",
    "        }\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    outputs = {}\n",
    "    # Create the dictionary\n",
    "    genes_dict = dict(zip(gene_names, gene_df))\n",
    "\n",
    "    for gene_id, df in genes_dict.items():\n",
    "        outputs[gene_id] = {}\n",
    "\n",
    "        for cfg in pipeline_configs:\n",
    "            name = cfg.get('name', 'pipeline')\n",
    "            kwargs = {k: v for k, v in cfg.items() if k != 'name'}\n",
    "\n",
    "            try:\n",
    "                _, X_pca, part, clusters, metrics = clustering_final(df, **kwargs)\n",
    "\n",
    "                row = {\n",
    "                    'gene': gene_id,\n",
    "                    'pipeline': name,\n",
    "                    'silhouette': metrics.get('silhouette'),\n",
    "                    'calinski_harabasz': metrics.get('calinski_harabasz'),\n",
    "                    'davies_bouldin': metrics.get('davies_bouldin'),\n",
    "                    'leiden_quality': metrics.get('leiden_quality'),\n",
    "                    'modularity': metrics.get('modularity'),\n",
    "                    'n_clusters': len(np.unique(clusters)),\n",
    "                    'n_nodes': len(df),\n",
    "                    'n_edges': len(part.graph.es) if hasattr(part, 'graph') else None\n",
    "                }\n",
    "\n",
    "                rows.append(row)\n",
    "\n",
    "                outputs[gene_id][name] = {\n",
    "                    'X_pca': X_pca,\n",
    "                    'part': part,\n",
    "                    'clusters': clusters,\n",
    "                    'metrics': metrics\n",
    "                }\n",
    "\n",
    "            except Exception as e:\n",
    "                row = {\n",
    "                    'gene': gene_id,\n",
    "                    'pipeline': name,\n",
    "                    'error': str(e)\n",
    "                }\n",
    "                rows.append(row)\n",
    "                outputs[gene_id][name] = {'error': str(e)}\n",
    "\n",
    "    results_df = pd.DataFrame(rows)\n",
    "    return results_df, outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_violin_scatter(\n",
    "    results_df,\n",
    "    metrics=('silhouette', 'calinski_harabasz', 'davies_bouldin',\n",
    "             'leiden_quality', 'modularity', 'n_clusters')\n",
    "):\n",
    "    \"\"\"\n",
    "    Violin + scatter (strip) plots comparing pipelines for each metric across genes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    results_df : pandas.DataFrame\n",
    "        Output from run_pipelines_on_genes().\n",
    "    metrics : tuple of str\n",
    "        Metrics to visualize.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    figs : dict\n",
    "        Dictionary of matplotlib Figures, keyed by metric name.\n",
    "    \"\"\"\n",
    "    # Melt to long format\n",
    "    df_long = results_df.melt(\n",
    "        id_vars=['gene', 'pipeline'],\n",
    "        value_vars=[m for m in metrics if m in results_df.columns],\n",
    "        var_name='metric',\n",
    "        value_name='value'\n",
    "    )\n",
    "\n",
    "    # Drop missing or failed results\n",
    "    df_long = df_long.dropna(subset=['value'])\n",
    "\n",
    "    # Plot one figure per metric\n",
    "    figs = {}\n",
    "    for metric_name, sub in df_long.groupby('metric'):\n",
    "        plt.figure(figsize=(7, 5))\n",
    "        ax = sns.violinplot(\n",
    "            data=sub,\n",
    "            x='pipeline',\n",
    "            y='value',\n",
    "            inner=None,\n",
    "            cut=0\n",
    "        )\n",
    "        sns.stripplot(\n",
    "            data=sub,\n",
    "            x='pipeline',\n",
    "            y='value',\n",
    "            color='k',\n",
    "            size=4,\n",
    "            alpha=0.6,\n",
    "            jitter=0.2\n",
    "        )\n",
    "        ax.set_title(f'Comparison by {metric_name}')\n",
    "        ax.set_xlabel('')\n",
    "        ax.set_ylabel(metric_name)\n",
    "        ax.grid(axis='y', linestyle='--')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        figs[metric_name] = ax.get_figure()\n",
    "        \n",
    "\n",
    "    return figs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_compare_pipelines_grid(\n",
    "    results_df: pd.DataFrame,\n",
    "    pipelines_order=('pca_euclidean', 'cosine_no_pca'),\n",
    "    metrics=('silhouette', 'calinski_harabasz', 'davies_bouldin',\n",
    "             'leiden_quality', 'modularity', 'n_clusters'),\n",
    "    kind='violin',                # 'violin' or 'box'\n",
    "    show_points=True,\n",
    "    connect_pairs=True,           # draw line per gene connecting the two pipelines\n",
    "    figsize=None,\n",
    "    point_kwargs=None,\n",
    "    violin_kwargs=None,\n",
    "    box_kwargs=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Make ONE figure with one subplot per metric; within each subplot, \n",
    "    show pipelines side-by-side.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    results_df : pd.DataFrame\n",
    "        Must contain columns ['gene', 'pipeline', <metrics...>].\n",
    "    pipelines_order : tuple\n",
    "        Order of pipelines to display.\n",
    "    metrics : tuple\n",
    "        Which metrics to plot.\n",
    "    kind : {'violin', 'box'}\n",
    "        Plot type for distributions.\n",
    "    show_points : bool\n",
    "        Whether to overlay individual gene points.\n",
    "    connect_pairs : bool\n",
    "        Whether to connect paired genes across pipelines.\n",
    "    figsize : tuple or None\n",
    "        Figure size; computed automatically if None.\n",
    "    *_kwargs : dict or None\n",
    "        Style options passed to Seaborn plotting functions.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    fig, axes : matplotlib Figure and Axes objects\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Filter metrics and pipelines ---\n",
    "    metrics = [m for m in metrics if m in results_df.columns]\n",
    "    if not metrics:\n",
    "        raise ValueError(\"No requested metrics found in results_df.\")\n",
    "\n",
    "    df = results_df.copy()\n",
    "    df = df[df['pipeline'].isin(pipelines_order)]\n",
    "    if df.empty:\n",
    "        raise ValueError(\"results_df has no rows for the requested pipelines_order.\")\n",
    "\n",
    "    # Ensure pipelines are ordered consistently\n",
    "    df['pipeline'] = pd.Categorical(df['pipeline'],\n",
    "                                    categories=list(pipelines_order),\n",
    "                                    ordered=True)\n",
    "\n",
    "    # --- Melt to long format ---\n",
    "    df_long = (\n",
    "        df.melt(\n",
    "            id_vars=['gene', 'pipeline'],\n",
    "            value_vars=metrics,\n",
    "            var_name='metric',\n",
    "            value_name='value'\n",
    "        )\n",
    "        .dropna(subset=['value'])\n",
    "    )\n",
    "\n",
    "    # --- Set up figure ---\n",
    "    n_cols = len(metrics)\n",
    "    if figsize is None:\n",
    "        figsize = (3.2 * n_cols, 4.2)  # scale width by number of metrics\n",
    "\n",
    "    fig, axes = plt.subplots(1, n_cols, figsize=figsize, sharey=False)\n",
    "    if n_cols == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    # --- Default style dictionaries ---\n",
    "    if point_kwargs is None:\n",
    "        point_kwargs = dict(color='k', s=18, alpha=0.7)\n",
    "    if violin_kwargs is None:\n",
    "        violin_kwargs = dict(inner=None, cut=0, linewidth=0)\n",
    "    if box_kwargs is None:\n",
    "        box_kwargs = dict(fliersize=0, linewidth=1.2)\n",
    "\n",
    "    # --- Color palette ---\n",
    "    palette = sns.color_palette('Set2', n_colors=len(pipelines_order))\n",
    "    pipeline_colors = dict(zip(pipelines_order, palette))\n",
    "\n",
    "    # --- Plot each metric ---\n",
    "    for ax, metric_name in zip(axes, metrics):\n",
    "        sub = df_long[df_long['metric'] == metric_name]\n",
    "\n",
    "        # Main distribution plot\n",
    "        if kind == 'violin':\n",
    "            sns.violinplot(\n",
    "                data=sub,\n",
    "                x='pipeline', y='value',\n",
    "                order=list(pipelines_order),\n",
    "                palette=pipeline_colors,\n",
    "                ax=ax, **violin_kwargs\n",
    "            )\n",
    "        elif kind == 'box':\n",
    "            sns.boxplot(\n",
    "                data=sub,\n",
    "                x='pipeline', y='value',\n",
    "                order=list(pipelines_order),\n",
    "                palette=pipeline_colors,\n",
    "                ax=ax, **box_kwargs\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"kind must be 'violin' or 'box'\")\n",
    "\n",
    "        # Overlay points\n",
    "        if show_points:\n",
    "            sns.stripplot(\n",
    "                data=sub,\n",
    "                x='pipeline', y='value',\n",
    "                order=list(pipelines_order),\n",
    "                dodge=False, jitter=0.15,\n",
    "                color='k', size=4, alpha=0.6, ax=ax\n",
    "            )\n",
    "\n",
    "        # Connect paired points (same gene across pipelines)\n",
    "        if connect_pairs and len(pipelines_order) == 2:\n",
    "            p0, p1 = pipelines_order\n",
    "\n",
    "            # Pivot to wide format (gene × pipeline)\n",
    "            wide = sub.pivot_table(index='gene', columns='pipeline',\n",
    "                                   values='value', aggfunc='first')\n",
    "            wide = wide.dropna(subset=[p0, p1], how='any')\n",
    "\n",
    "            # Draw connecting lines\n",
    "            for _, row in wide.iterrows():\n",
    "                ax.plot([0, 1], [row[p0], row[p1]],\n",
    "                        color='gray', alpha=0.35, linewidth=1)\n",
    "\n",
    "        # Axis formatting\n",
    "        ax.set_title(metric_name.replace('_', ' ').title())\n",
    "        ax.set_xlabel('')\n",
    "        ax.grid(axis='y', linestyle='--', alpha=0.25)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig, axes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def silhouettes_multi(\n",
    "    X_space,\n",
    "    clusters,\n",
    "    metric_main='cosine',\n",
    "    extra_metrics=('correlation', 'euclidean')\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute silhouette scores for multiple distance metrics.\n",
    "\n",
    "    Args:\n",
    "        X_space (array-like): Feature matrix (n_samples x n_features).\n",
    "        clusters (array-like): Cluster labels.\n",
    "        metric_main (str): Primary metric to evaluate (default: 'cosine').\n",
    "        extra_metrics (tuple): Additional metrics to test (default: ('correlation', 'euclidean')).\n",
    "\n",
    "    Returns:\n",
    "        dict: Silhouette scores for each metric, using direct or precomputed distances.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    # Main metric\n",
    "    try:\n",
    "        results[f'silhouette_{metric_main}'] = silhouette_score(\n",
    "            X_space, clusters, metric=metric_main\n",
    "        )\n",
    "    except Exception:\n",
    "        D = pairwise_distances(X_space, metric=metric_main)\n",
    "        results[f'silhouette_{metric_main}'] = silhouette_score(\n",
    "            D, clusters, metric='precomputed'\n",
    "        )\n",
    "\n",
    "    # Extra metrics\n",
    "    for m in extra_metrics:\n",
    "        try:\n",
    "            results[f'silhouette_{m}'] = silhouette_score(\n",
    "                X_space, clusters, metric=m\n",
    "            )\n",
    "        except Exception:\n",
    "            D = pairwise_distances(X_space, metric=m)\n",
    "            results[f'silhouette_{m}'] = silhouette_score(\n",
    "                D, clusters, metric='precomputed'\n",
    "            )\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3.1 ALTERNATE PIPELINE FOR CLUSTERING\n",
    "\n",
    "def filter_by_missingness(\n",
    "    df: pd.DataFrame,\n",
    "    min_bin_non_nan_frac: float = 0.2,  # keep bins (columns) seen in >= 20% of reads\n",
    "    min_read_non_nan_frac: float = 0.5  # keep reads (rows) with >= 50% bins observed\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Filter reads (rows) and bins (columns) based on missingness thresholds.\"\"\"\n",
    "    # rows = reads, columns = bins\n",
    "    if df.shape[0] < 2 or df.shape[1] < 2:\n",
    "        return df.copy()\n",
    "\n",
    "    keep_cols = df.notna().mean(axis=0) >= float(min_bin_non_nan_frac)\n",
    "    keep_rows = df.notna().mean(axis=1) >= float(min_read_non_nan_frac)\n",
    "    df_f = df.loc[keep_rows, keep_cols].copy()\n",
    "\n",
    "    return df_f\n",
    "\n",
    "\n",
    "def masked_pearson_correlation_matrix(\n",
    "    df: pd.DataFrame,\n",
    "    min_overlap: int = 30\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Pairwise-complete Pearson correlation between reads (rows), computed only on shared\n",
    "    non-NaN bins, requiring at least min_overlap shared bins.\n",
    "\n",
    "    Returns an (N x N) DataFrame; entries with overlap < min_overlap are NaN.\n",
    "    \"\"\"\n",
    "    # df.T.corr computes correlation among rows of df\n",
    "    R = df.T.corr(min_periods=int(min_overlap))\n",
    "    return R\n",
    "\n",
    "\n",
    "def overlap_matrix(df: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"Compute number of shared non-NaN bins for each read pair (N x N).\"\"\"\n",
    "    M = (~df.isna()).to_numpy(dtype=bool)  # shape (N, D)\n",
    "    O = M @ M.T\n",
    "    return O\n",
    "\n",
    "\n",
    "def build_masked_corr_knn_graph(\n",
    "    df: pd.DataFrame,\n",
    "    k: int = 15,\n",
    "    min_overlap: int = 30,\n",
    "    positive_only: bool = True,\n",
    "    shrink_c: float = 30.0,  # overlap-based shrink: w *= n / (n + shrink_c)\n",
    "    mutual: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Build a weighted kNN graph from masked Pearson correlations.\n",
    "\n",
    "    - df: rows = reads, columns = bins, values = methylation fractions (NaN allowed).\n",
    "\n",
    "    Returns:\n",
    "        igraph.Graph, weight list (in edge order), neighbor index list per node.\n",
    "    \"\"\"\n",
    "    N = df.shape[0]\n",
    "    if N < 2:\n",
    "        raise ValueError(\"Need at least 2 reads to build a graph.\")\n",
    "\n",
    "    # 1) Pairwise-complete correlation and overlap counts\n",
    "    R_df = masked_pearson_correlation_matrix(df, min_overlap=min_overlap)  # (N x N)\n",
    "    O = overlap_matrix(df)  # (N x N), shared bin counts\n",
    "\n",
    "    # 2) Convert to weights\n",
    "    R = R_df.to_numpy()\n",
    "    mask_sufficient = (O >= int(min_overlap))\n",
    "    W = np.where(mask_sufficient, R, 0.0)\n",
    "\n",
    "    # Keep only nonnegative similarities if desired\n",
    "    if positive_only:\n",
    "        W = np.maximum(W, 0.0)\n",
    "\n",
    "    # Optional shrink by overlap size (softly downweight low-overlap edges)\n",
    "    if shrink_c is not None and shrink_c > 0:\n",
    "        shrink = O / (O + float(shrink_c))\n",
    "        W = W * shrink\n",
    "\n",
    "    # Zero diagonal\n",
    "    np.fill_diagonal(W, 0.0)\n",
    "\n",
    "    # 3) Build kNN (top-k by weight, skipping zeros)\n",
    "    neighbors = []\n",
    "    weights = []\n",
    "\n",
    "    for i in range(N):\n",
    "        row = W[i].copy()\n",
    "        idx_sorted = np.argsort(row)[::-1]  # sort by weight descending\n",
    "        idx_sorted = idx_sorted[row[idx_sorted] > 0]  # filter out zeros\n",
    "        topk = idx_sorted[:k]\n",
    "        neighbors.append(topk.tolist())\n",
    "        weights.append(row[topk].tolist())\n",
    "\n",
    "    # 4) Symmetrize (mutual kNN) and build edge list\n",
    "    edges = {}\n",
    "\n",
    "    if mutual:\n",
    "        neighbor_sets = [set(ns) for ns in neighbors]\n",
    "        for i in range(N):\n",
    "            for j in neighbors[i]:\n",
    "                if i in neighbor_sets[j]:\n",
    "                    a, b = (i, j) if i < j else (j, i)\n",
    "                    wij = W[i, j]\n",
    "                    wji = W[j, i]\n",
    "                    w = max(wij, wji) # here i could change and put mean also\n",
    "                    if w > 0:\n",
    "                        edges[(a, b)] = max(edges.get((a, b), 0.0), w)\n",
    "    else:\n",
    "        for i in range(N):\n",
    "            for j, w in zip(neighbors[i], weights[i]):\n",
    "                a, b = (i, j) if i < j else (j, i)\n",
    "                edges[(a, b)] = max(edges.get((a, b), 0.0), w)\n",
    "\n",
    "    e_list = list(edges.keys())\n",
    "    w_list = [edges[e] for e in e_list]\n",
    "\n",
    "    g = ig.Graph(n=N, edges=e_list, directed=False)\n",
    "    g.es[\"weight\"] = w_list\n",
    "\n",
    "    return g, w_list, neighbors\n",
    "\n",
    "\n",
    "def alternate_clustering(\n",
    "    df: pd.DataFrame,\n",
    "    apply_filter: bool = True,\n",
    "    min_bin_non_nan_frac: float = 0.2,\n",
    "    min_read_non_nan_frac: float = 0.5,\n",
    "    transform: str | None = None,  # None | 'arcsine' | 'logit'\n",
    "    min_overlap: int = 30,\n",
    "    k: int = 15,\n",
    "    positive_only: bool = True,\n",
    "    shrink_c: float = 30.0,\n",
    "    mutual: bool = True,\n",
    "    leiden_resolution: float = 1.0,\n",
    "    seed: int = 42\n",
    "):\n",
    "    \"\"\"\n",
    "    Pipeline: Masked Pearson correlation (no imputation)\n",
    "    -> weighted mutual kNN graph -> Leiden clustering.\n",
    "\n",
    "    Args:\n",
    "        df: rows = reads, columns = bins, values in [0,1] with NaNs allowed.\n",
    "\n",
    "    Returns:\n",
    "        clusters (np.ndarray of shape (N,)),\n",
    "        graph (igraph.Graph),\n",
    "        df_used (pd.DataFrame after optional filtering/transform)\n",
    "    \"\"\"\n",
    "    if apply_filter:\n",
    "        df_used = filter_by_missingness(\n",
    "            df,\n",
    "            min_bin_non_nan_frac=min_bin_non_nan_frac,\n",
    "            min_read_non_nan_frac=min_read_non_nan_frac\n",
    "        )\n",
    "    else:\n",
    "        df_used = df.copy()\n",
    "\n",
    "    if df_used.shape[0] < 2 or df_used.shape[1] < 2:\n",
    "        raise ValueError(\"Not enough reads or bins after filtering.\")\n",
    "\n",
    "    # Optional transform for proportions\n",
    "    if transform is not None:\n",
    "        if transform == 'arcsine':\n",
    "            arr = np.arcsin(np.sqrt(np.clip(df_used.to_numpy(float), 0.0, 1.0)))\n",
    "            df_used = pd.DataFrame(arr, index=df_used.index, columns=df_used.columns)\n",
    "        elif transform == 'logit':\n",
    "            Xc = np.clip(df_used.to_numpy(float), 1e-3, 1 - 1e-3)\n",
    "            arr = np.log(Xc / (1 - Xc))\n",
    "            df_used = pd.DataFrame(arr, index=df_used.index, columns=df_used.columns)\n",
    "        else:\n",
    "            raise ValueError(\"transform must be None, 'arcsine', or 'logit'\")\n",
    "\n",
    "    # Build graph from masked correlation\n",
    "    g, w_list, _ = build_masked_corr_knn_graph(\n",
    "        df_used,\n",
    "        k=k,\n",
    "        min_overlap=min_overlap,\n",
    "        positive_only=positive_only,\n",
    "        shrink_c=shrink_c,\n",
    "        mutual=mutual\n",
    "    )\n",
    "\n",
    "    # Leiden clustering\n",
    "    part = la.find_partition(\n",
    "        g,\n",
    "        la.RBConfigurationVertexPartition,\n",
    "        weights=g.es[\"weight\"],\n",
    "        resolution_parameter=float(leiden_resolution),\n",
    "        seed=int(seed)\n",
    "    )\n",
    "\n",
    "    clusters = np.array(part.membership, dtype=int)\n",
    "    return clusters, g, df_used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_umap(\n",
    "    X,\n",
    "    clusters,\n",
    "    n_neighbors=15,\n",
    "    min_dist=0.1,\n",
    "    metric='euclidean',      # for Pipeline 2, use 'euclidean'\n",
    "    transform=None,          # None | 'logit' | 'arcsine' (use only if X are raw proportions)\n",
    "    n_pcs=None,              # set if X are raw features; None if X are already PCs\n",
    "    standardize=False,       # True if using raw features without PCA\n",
    "    seed=42,\n",
    "    palette=None,            # optional list/array or matplotlib colormap name\n",
    "    title=\"UMAP embedding\",\n",
    "    gene = 'Gene'\n",
    "):\n",
    "\n",
    "    X_in = np.asarray(X, dtype=float)\n",
    "\n",
    "    # Optional transform for proportions (only if X are raw fractions)\n",
    "    if transform is not None:\n",
    "        if transform == 'logit':\n",
    "            Xc = np.clip(X_in, 1e-3, 1 - 1e-3)\n",
    "            X_in = np.log(Xc / (1 - Xc))\n",
    "        elif transform == 'arcsine':\n",
    "            X_in = np.arcsin(np.sqrt(np.clip(X_in, 0.0, 1.0)))\n",
    "        else:\n",
    "            raise ValueError(\"transform must be None, 'logit', or 'arcsine'\")\n",
    "\n",
    "    # Optional standardization (useful if running UMAP on raw features)\n",
    "    if standardize:\n",
    "        scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "        X_in = scaler.fit_transform(X_in)\n",
    "\n",
    "    # Optional PCA (skip if X are already PCA scores)\n",
    "    if n_pcs is not None and 0 < n_pcs < X_in.shape[1]:\n",
    "        pca = PCA(n_components=n_pcs, random_state=seed)\n",
    "        X_umap = pca.fit_transform(X_in)\n",
    "    else:\n",
    "        X_umap = X_in\n",
    "\n",
    "    # Sanity checks for metric\n",
    "    if metric == 'jaccard':\n",
    "        # Warn if data are not binary\n",
    "        if not np.array_equal(X_umap, X_umap.astype(bool)) and not np.array_equal(\n",
    "            X_umap, (X_umap > 0).astype(int)\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                \"Jaccard metric requires binary data; \"\n",
    "                \"use euclidean/cosine/correlation for continuous features.\"\n",
    "            )\n",
    "\n",
    "    # UMAP embedding\n",
    "    reducer = umap.UMAP(\n",
    "        n_neighbors=n_neighbors,\n",
    "        min_dist=min_dist,\n",
    "        metric=metric,\n",
    "        random_state=seed\n",
    "    )\n",
    "    embedding = reducer.fit_transform(X_umap)\n",
    "\n",
    "    # Colors\n",
    "    unique_clusters = np.unique(clusters)\n",
    "    if palette is None:\n",
    "        # fallback to a matplotlib qualitative colormap\n",
    "        cmap = plt.get_cmap('tab20')\n",
    "        color_map = {c: cmap(i % cmap.N) for i, c in enumerate(unique_clusters)}\n",
    "    elif isinstance(palette, str):\n",
    "        cmap = plt.get_cmap(palette)\n",
    "        color_map = {c: cmap(i % cmap.N) for i, c in enumerate(unique_clusters)}\n",
    "    else:\n",
    "        # palette is a list/array\n",
    "        color_map = {c: palette[i % len(palette)] for i, c in enumerate(unique_clusters)}\n",
    "\n",
    "    colors = [color_map[c] for c in clusters]\n",
    "\n",
    "    # Plot\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    plt.scatter(\n",
    "        embedding[:, 0],\n",
    "        embedding[:, 1],\n",
    "        c=colors,\n",
    "        alpha=0.85,\n",
    "    )\n",
    "    plt.xlabel(\"UMAP1\")\n",
    "    plt.ylabel(\"UMAP2\")\n",
    "    plt.title(f'{gene} : {title}')\n",
    "    plt.grid()\n",
    "\n",
    "    # Legend\n",
    "    handles = [\n",
    "        plt.Line2D(\n",
    "            [0], [0],\n",
    "            marker='o',\n",
    "            color='w',\n",
    "            label=str(c),\n",
    "            markerfacecolor=color_map[c],\n",
    "            markersize=8\n",
    "        )\n",
    "        for c in unique_clusters\n",
    "    ]\n",
    "    plt.legend(\n",
    "        handles=handles,\n",
    "        title=\"Cluster\",\n",
    "        bbox_to_anchor=(1.02, 1.0),\n",
    "        loc='upper left',\n",
    "        frameon=False\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return embedding, fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "#### Annex functions needed to gather clustering information - dictionnary manipulation + start/end/coord\n",
    "1. 'dict_id_cluster_color'\n",
    "2. 'dict_to_df'\n",
    "3. 'merge'\n",
    "4. 'start_end_center'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. DICTIONNARY TO KEEP TRACK OF THE ASSOCIATION READID, CLUSTER AND COLOR\n",
    "def dict_id_cluster_color(\n",
    "        df: pd.DataFrame,\n",
    "        clusters : list,\n",
    "        hex_colors: list,\n",
    "):\n",
    "    \"\"\"\n",
    "    Creating a dictionnary that keeps track of the association readid, cluster associated, and the color attributed\n",
    "    df: dataframe that is binned, in a matrix form without any missing values (the dataframe used to cluster)\n",
    "    clusters: the list containing the clusters after clustering(df)\n",
    "    hex_colors: the hand defined color list (to not rely on the colormaps)\n",
    "    \"\"\"\n",
    "    #creating a dictionnary to get the colors associated to the clusters\n",
    "\n",
    "    unique_clusters = np.unique(clusters)\n",
    "    cluster_colors = {\n",
    "        cluster_id: hex_colors[i % len(hex_colors)]\n",
    "        for i, cluster_id in enumerate(unique_clusters)\n",
    "    }\n",
    "\n",
    "#creating a dictionnary where the key is the readid and the values both the cluster id and the color\n",
    "    read_dict = {\n",
    "        read_id: {\n",
    "            \"cluster\": cluster_id,\n",
    "            \"color\": cluster_colors[cluster_id]\n",
    "        }\n",
    "        for read_id, cluster_id in zip(df.index.get_level_values('readid'), clusters)\n",
    "    }\n",
    "    return read_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. TRANSFORMING THE DICTIONNARY INTO A DATAFRAME\n",
    "def dict_to_df(\n",
    "        read_dict: dict\n",
    "):\n",
    "    \"\"\"\n",
    "    Transforming the dictionnary into a dataframe, (to be able to merge it with the original dataframe later on)\n",
    "    read_dict: the dictionnary created with the function dict_id_cluster_color\n",
    "    \"\"\"\n",
    "    df_dict = pd.DataFrame.from_dict(read_dict, orient='index')\n",
    "    df_dict.index.name = 'readid'\n",
    "    df_dict.reset_index(inplace=True)\n",
    "    df_dict.rename(columns={\"cluster\": \"locus_cluster\", \"color\": \"locus_cluster_color\"}, inplace=True)\n",
    "    \n",
    "    return df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. MERGING THE DICTIONNARY DATAFRAME WITH THE ORIGINAL DATAFRAME (TO GET THE CLUSTER AND COLOR INFORMATION FOR EACH READ)\n",
    "def merge(\n",
    "        df,\n",
    "        df_dict: pd.DataFrame  \n",
    "):\n",
    "    # Ensure readid is a column\n",
    "    if \"readid\" not in df.columns:\n",
    "        df_merged = df.reset_index().rename(columns={\"index\": \"readid\"}).copy()\n",
    "    else:\n",
    "        df_merged = df.copy()    \n",
    "\n",
    "    #Merging the two dataframes on'readid' to get the cluster and color information for each read\n",
    "    df_merged = df_merged.merge(df_dict, on=\"readid\", how=\"inner\")\n",
    "    df_merged.dropna(subset=['locus_cluster'], inplace=True)\n",
    "    \n",
    "    return df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. FUNCTION TO GET THE START, END AND CENTER COORDINATES OF A GENE\n",
    "def start_end_center(\n",
    "        df: pd.DataFrame\n",
    "):\n",
    "    if 'read_start' in df.columns and 'read_end' in df.columns and 'pol2_pos' in df.columns:\n",
    "        start = df['read_start'].min()\n",
    "        end = df['read_end'].max()\n",
    "        center_coord = df['pol2_pos'].unique()[0]  \n",
    "\n",
    "        return start, end, center_coord\n",
    "    else:\n",
    "        raise ValueError(\"DataFrame must contain 'read_start', 'read_end', and 'pol2_pos' columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "#### Analysis functions: heatmaps, average plots\n",
    "1. 'summary'\n",
    "2. (unused )'plot_avg_methylation_profile'\n",
    "- (unused) 'plot_avg_methylation_profile_bulk'\n",
    "- 'compute_bulk_centroids'\n",
    "- 'compute_gene_centroids'\n",
    "- 'plot_centroids_with_shading'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. TO COMPARE THE LOCUS SPECIFIC CLUSTERS RELATIVELY TO THE BULK CLUSTERS AND THE GROUPS\n",
    "\n",
    "def summary(\n",
    "        df: pd.DataFrame,\n",
    "        clusters: list,\n",
    "        gene='Gene'\n",
    "):\n",
    "    df_summary = df.copy()\n",
    "    df_summary['locus_cluster'] = clusters\n",
    "\n",
    "    total_reads = len(df_summary)  # <-- denominator for global %s\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6), constrained_layout=True)\n",
    "\n",
    "    # --- First heatmap: per group (global percentages) ---\n",
    "    if 'group' in df_summary.index.names:\n",
    "        # group is part of the index\n",
    "        summary_per_group = (\n",
    "            df_summary[['locus_cluster']]\n",
    "            .groupby(df_summary.index.get_level_values('group'))['locus_cluster']\n",
    "            .value_counts()\n",
    "            .div(total_reads).mul(100)\n",
    "            .reset_index(name=\"percentage\")\n",
    "        )\n",
    "    else:\n",
    "        # group is a normal column\n",
    "        summary_per_group = (\n",
    "            df_summary[['group', 'locus_cluster']]\n",
    "            .groupby('group')['locus_cluster']\n",
    "            .value_counts()\n",
    "            .div(total_reads).mul(100)\n",
    "            .reset_index(name=\"percentage\")\n",
    "        )\n",
    "\n",
    "    pivot_group = summary_per_group.pivot(\n",
    "        index=\"group\", columns=\"locus_cluster\", values=\"percentage\"\n",
    "    ).fillna(0)\n",
    "\n",
    "    sns.heatmap(pivot_group, annot=True, fmt=\".2f\", cmap=\"viridis\", ax=axes[0])\n",
    "    axes[0].set_ylabel(\"Group\")\n",
    "    axes[0].set_xlabel(\"Cluster\")\n",
    "    axes[0].set_title(\"Reads per cluster in each group (% of all reads)\")\n",
    "\n",
    "    # --- Second heatmap: per bulk cluster (global percentages) ---\n",
    "\n",
    "    if 'cluster' in df_summary.index.names:\n",
    "        # group is part of the index\n",
    "        summary_per_bulk_cluster = (\n",
    "            df_summary[['locus_cluster']]\n",
    "            .groupby(df_summary.index.get_level_values('cluster'))['locus_cluster']\n",
    "            .value_counts()\n",
    "            .div(total_reads).mul(100)\n",
    "            .reset_index(name=\"percentage\")\n",
    "        )\n",
    "    else:\n",
    "        # group is a normal column\n",
    "        summary_per_bulk_cluster = (\n",
    "            df_summary[['cluster', 'locus_cluster']]\n",
    "            .groupby('cluster')['locus_cluster']\n",
    "            .value_counts()\n",
    "            .div(total_reads).mul(100)\n",
    "            .reset_index(name=\"percentage\")\n",
    "        )\n",
    "\n",
    "    pivot_cluster = summary_per_bulk_cluster.pivot(\n",
    "        index=\"cluster\", columns=\"locus_cluster\", values=\"percentage\"\n",
    "    ).fillna(0)\n",
    "\n",
    "    sns.heatmap(pivot_cluster, annot=True, fmt=\".2f\", cmap=\"viridis\", ax=axes[1])\n",
    "    axes[1].set_ylabel(\"Bulk Cluster\")\n",
    "    axes[1].set_xlabel(\"Cluster\")\n",
    "    axes[1].set_title(\"Reads per cluster in each bulk cluster (% of all reads)\")\n",
    "\n",
    "    plt.title(f'{gene} : Heatmaps of repartitions')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "### (unused) 2. FUNCTION TO PLOT THE AVERAGE METHYLATION PROFILES PER CLUSTER (WITH THE COLORS ATTRIBUTED TO EACH CLUSTER)\n",
    "def plot_avg_methylation_profile(\n",
    "    df: pd.DataFrame,\n",
    "    df_dict: pd.DataFrame,  # dataframe made out of the read_dict\n",
    "    start: int,\n",
    "    end: int,\n",
    "    center_coord: int,\n",
    "    read_dict: dict, # {read_id: {'cluster': cluster_id, 'color': hex_color}}\n",
    "    gene= 'Gene'\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot average methylation profiles per cluster using cluster-specific colors.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Each row = read, columns = genomic positions (bins), values = mean methylation (0-1)\n",
    "    start : int\n",
    "        Start genomic coordinate for plotting\n",
    "    end : int\n",
    "        End genomic coordinate for plotting\n",
    "    center_coord : int\n",
    "        Central position (e.g., Pol2)\n",
    "    partition : ig.VertexClustering\n",
    "        Clustering result (used to determine number of clusters if needed)\n",
    "    read_dict : dict\n",
    "        Mapping of read IDs to cluster and color: {read_id: {'cluster': id, 'color': '#hex'}}\n",
    "    \"\"\"\n",
    "    # -------------------------\n",
    "    # Step 1: Prepare DataFrame\n",
    "    # -------------------------\n",
    "    df_avg= df.copy()\n",
    "    df_avg = df_avg.reset_index(level='readid', drop=False) \n",
    "    df_avg.index = range(len(df_avg)) \n",
    "\n",
    "    # -------------------------\n",
    "    # Step 2: Merge with df_dict\n",
    "    # -------------------------\n",
    "    df_merged = pd.merge(df_avg, df_dict, on='readid', how='inner')\n",
    "\n",
    "    # -------------------------\n",
    "    # Step 3: Identify numeric position columns\n",
    "    # -------------------------\n",
    "    metadata_cols = ['readid', 'locus_cluster', 'locus_cluster_color']\n",
    "    position_cols = [col for col in df_merged.columns if col not in metadata_cols]\n",
    "    positions_sorted = sorted([int(col) for col in position_cols])\n",
    "\n",
    "    # -------------------------\n",
    "    # Step 4: Cluster colors mapping\n",
    "    # -------------------------\n",
    "    cluster_colors = {}\n",
    "    for rid, info in read_dict.items():\n",
    "        cid = int(info['cluster'])\n",
    "        if cid not in cluster_colors:\n",
    "            cluster_colors[cid] = info['color']\n",
    "\n",
    "    # -------------------------\n",
    "    # Step 5: Compute cluster proportions\n",
    "    # -------------------------\n",
    "    unique_clusters = sorted(df_merged['locus_cluster'].dropna().astype(int).unique())\n",
    "    n_clusters = len(unique_clusters)\n",
    "    proportion_df = (\n",
    "        df_merged['locus_cluster']\n",
    "        .value_counts(normalize=True)\n",
    "        .mul(100)\n",
    "        .reindex(unique_clusters, fill_value=0)\n",
    "        .reset_index(name=\"percentage\")\n",
    "        .rename(columns={\"index\": \"locus_cluster\"})\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # Step 6: Create subplots\n",
    "    # -------------------------\n",
    "    height_ratios = proportion_df['percentage'].values\n",
    "    fig, axes = plt.subplots(n_clusters, 1, figsize=(10, 12), sharex=True,\n",
    "                             gridspec_kw={'height_ratios': height_ratios})\n",
    "    if n_clusters == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    fig.suptitle(\n",
    "    f\"{gene}: Average DNA Methylation Profiles per Cluster\",  # Global title\n",
    "    fontsize=14,  # Title font size\n",
    "    y=0.97  # Adjust title vertical position (closer to the top of the figure)\n",
    ")\n",
    "    # -------------------------\n",
    "    # Step 7: Plot each cluster\n",
    "    # -------------------------\n",
    "    meth_arrays=[]\n",
    "\n",
    "    for i, cluster_id in enumerate(unique_clusters):\n",
    "        ax = axes[i]\n",
    "        cluster_rows = df_merged[df_merged['locus_cluster'] == cluster_id]\n",
    "\n",
    "        if cluster_rows.empty:\n",
    "            continue\n",
    "\n",
    "        # Compute mean methylation only over position columns (integers!)\n",
    "        df_meth = cluster_rows[positions_sorted]\n",
    "        meth_sorted = df_meth.mean(axis=0).values\n",
    "\n",
    "\n",
    "        # Smoothing\n",
    "        meth_smooth = gaussian_filter1d(meth_sorted, sigma=0.3)\n",
    "\n",
    "        meth_arrays.append(meth_sorted)\n",
    "\n",
    "        # Cluster color\n",
    "        color = cluster_rows['locus_cluster_color'].iloc[0]\n",
    "\n",
    "        # Plot\n",
    "\n",
    "        # Plot with the cluster color\n",
    "        ax.set_facecolor(color)\n",
    "\n",
    "        # Fill under the curve for the background as white\n",
    "        ax.fill_between(positions_sorted, meth_smooth, color='white')\n",
    "\n",
    "        # Optionally, overlay a line for contrast\n",
    "        # ax.plot(positions_sorted, meth_smooth, color='white', linewidth=1)\n",
    "\n",
    "        # Y-axis label with percentage\n",
    "        percentage = proportion_df.loc[\n",
    "            proportion_df['locus_cluster'] == cluster_id, 'percentage'\n",
    "        ].values[0]\n",
    "        ax.set_ylabel(f\"Cluster {cluster_id} | {percentage:.1f}%\", fontsize=8)\n",
    "\n",
    "        # Axis limits and grid\n",
    "        ax.set_ylim(0, 1.1)\n",
    "        ax.set_xlim(min(positions_sorted), max(positions_sorted))\n",
    "    \n",
    "        # Assign new labels\n",
    "        ax.set_xticklabels([str(start), \"50\", str(end)])\n",
    "        ax.grid(True, axis='x', linestyle='--', linewidth=0.5, color='gray')\n",
    "\n",
    "        # Reference lines\n",
    "        ax.axhline(y=1, color='grey', linestyle='--', linewidth=0.5)\n",
    "        ax.axvline(x=0, color=\"#C80028\", linestyle=\"-\", linewidth=2, label=\"Pol2 position\")\n",
    "        ax.xaxis.set_major_locator(MultipleLocator(100))\n",
    "    \n",
    "\n",
    "    # -------------------------\n",
    "    # Step 8: Global labels & legend\n",
    "    # -------------------------\n",
    "    \n",
    "\n",
    "    # Show x tick labels only on the bottom axis\n",
    "    for ax in axes[:-1]:\n",
    "        ax.tick_params(labelbottom=False)\n",
    "\n",
    "    # Ticks every 100 on the bottom axis\n",
    "    axes[-1].xaxis.set_major_locator(MultipleLocator(100))\n",
    "\n",
    "    # Format bottom tick labels as absolute genomic coordinates\n",
    "    def rel_to_abs_label(x, pos):\n",
    "        # Optionally use thousands separators: f\"{int(x + center_coord):,}\"\n",
    "        return f\"{int(x + center_coord)}\"\n",
    "\n",
    "    axes[-1].xaxis.set_major_formatter(FuncFormatter(rel_to_abs_label))\n",
    "\n",
    "    # Hide any potential offset text\n",
    "    axes[-1].get_xaxis().get_offset_text().set_visible(False)\n",
    "\n",
    "    # Global labels & legend\n",
    "    fig.text(0.95, 0.5, 'Methylation level', va='center', ha='center', rotation=-90, fontsize=10)\n",
    "    fig.text(\n",
    "        0.05, 0.5,\n",
    "        \"Cluster number | Cluster proportion\\n(ordered by cluster mean methylation)\",\n",
    "        fontsize=10, multialignment='center', rotation=90, va='center', ha='center'\n",
    "    )\n",
    "    fig.text(0.5, 0.04, \"Genomic coordinate\", va='center', ha='center', fontsize=10)\n",
    "\n",
    "    center_line = mlines.Line2D([], [], color='red', linestyle='-', linewidth=1, label='pol2 position')\n",
    "    fig.legend(handles=[center_line], loc='upper right', bbox_to_anchor=(0.97, 0.955), fontsize=9, frameon=False)\n",
    "\n",
    "    plt.subplots_adjust(hspace=0.05, top=0.92)\n",
    "    plt.setp(axes[-1].get_xticklabels(), rotation=45, ha='right', rotation_mode='anchor')\n",
    "    axes[-1].tick_params(axis='x', labelsize=8)\n",
    "    fig.subplots_adjust(bottom=0.18)  # extra bottom margin to avoid clipping\n",
    "\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    return position_cols, meth_arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # (unused) FUNCTION TO PLOT AVERAGE METHYLATION PROFILES FOR THE BULK DATA\n",
    "# def plot_avg_methylation_profile_bulk(\n",
    "#     df: pd.DataFrame,\n",
    "#     start: int | None = None,\n",
    "#     end: int | None = None,\n",
    "#     hex_colors=None,\n",
    "#     smooth_sigma: float = 0.5,\n",
    "#     order_by: str = \"mean\",  # 'mean' | 'cluster' | None\n",
    "#     use_cov_for_proportion: bool = True\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Plot average methylation profiles per cluster for bulk data.\n",
    "\n",
    "#     Input df columns:\n",
    "#     - 'cluster': cluster id (int or str)\n",
    "#     - 'C_pos': genomic coordinate (int; already absolute or relative)\n",
    "#     - 'meth': average methylation at C_pos for that cluster (0..1)\n",
    "#     - 'cov': per-cluster coverage/size (assumed constant across rows or summable)\n",
    "\n",
    "#     Args:\n",
    "#     - start, end: optional x-range to display; if None, inferred from data.\n",
    "#     - hex_colors: list of color hex strings; if None, use matplotlib tab20.\n",
    "#     - smooth_sigma: Gaussian smoothing sigma for the curve.\n",
    "#     - order_by: order subplots by 'mean' methylation, by 'cluster', or leave as-is (None).\n",
    "#     - use_cov_for_proportion: if True, compute proportion from cov; else from row counts.\n",
    "\n",
    "#     Returns:\n",
    "#     - fig, axes\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Validate input\n",
    "#     required = {'cluster', 'C_pos', 'meth', 'cov'}\n",
    "#     missing = required - set(df.columns)\n",
    "#     if missing:\n",
    "#         raise ValueError(f\"Dataframe missing columns: {missing}\")\n",
    "\n",
    "#     # Ensure numeric types\n",
    "#     df = df.copy()\n",
    "#     for c in ['C_pos', 'meth', 'cov']:\n",
    "#         df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "#     df = df.dropna(subset=['cluster', 'C_pos', 'meth'])\n",
    "\n",
    "#     # Build cluster list and color map\n",
    "#     unique_clusters = list(pd.unique(df['cluster']))\n",
    "#     n_clusters = len(unique_clusters)\n",
    "\n",
    "#     if hex_colors is None:\n",
    "#         import matplotlib.cm as cm\n",
    "#         cmap = cm.get_cmap('tab20', n_clusters)\n",
    "#         hex_colors = [cm.colors.to_hex(cmap(i)) for i in range(n_clusters)]\n",
    "#     color_map = {cid: hex_colors[i % len(hex_colors)] for i, cid in enumerate(unique_clusters)}\n",
    "\n",
    "#     # Compute cluster stats\n",
    "#     g = df.groupby('cluster')\n",
    "#     cluster_mean = g['meth'].mean()\n",
    "\n",
    "#     # Proportion from coverage\n",
    "#     if use_cov_for_proportion:\n",
    "#         cov_first = g['cov'].first()\n",
    "#         cov_var = g['cov'].var()\n",
    "#         if np.nanmax(cov_var.fillna(0)) > 0:\n",
    "#             cov = g['cov'].sum()\n",
    "#         else:\n",
    "#             cov = cov_first\n",
    "#         proportions = (cov / cov.sum()) * 100.0\n",
    "#     else:\n",
    "#         counts = g.size()\n",
    "#         proportions = (counts / counts.sum()) * 100.0\n",
    "\n",
    "#     # Order clusters\n",
    "#     if order_by == \"mean\":\n",
    "#         ordered_clusters = list(cluster_mean.sort_values(ascending=False).index)\n",
    "#     elif order_by == \"cluster\":\n",
    "#         try:\n",
    "#             ordered_clusters = sorted(unique_clusters)\n",
    "#         except Exception:\n",
    "#             ordered_clusters = unique_clusters\n",
    "#     else:\n",
    "#         ordered_clusters = unique_clusters\n",
    "\n",
    "#     # Height ratios from proportions (avoid zeros)\n",
    "#     height_ratios = [max(1e-3, float(proportions.get(cid, 0.0))) for cid in ordered_clusters]\n",
    "\n",
    "#     # Figure and axes\n",
    "#     fig, axes = plt.subplots(\n",
    "#         n_clusters,\n",
    "#         1,\n",
    "#         figsize=(16, 18),\n",
    "#         sharex=True,\n",
    "#         gridspec_kw={'height_ratios': height_ratios}\n",
    "#     )\n",
    "#     if n_clusters == 1:\n",
    "#         axes = [axes]\n",
    "\n",
    "#     fig.suptitle(\"Average DNA Methylation Profiles per Cluster\", fontsize=14, y=0.97)\n",
    "\n",
    "#     # Plot each cluster\n",
    "#     for i, cid in enumerate(ordered_clusters):\n",
    "#         ax = axes[i]\n",
    "#         sub = df[df['cluster'] == cid].copy()\n",
    "#         if sub.empty:\n",
    "#             ax.set_visible(False)\n",
    "#             continue\n",
    "\n",
    "#         # Aggregate duplicates at the same position\n",
    "#         sub = sub.groupby('C_pos', as_index=False).agg({'meth': 'mean', 'cov': 'first'})\n",
    "#         sub = sub.sort_values('C_pos')\n",
    "\n",
    "#         # Extract arrays\n",
    "#         x = sub['C_pos'].to_numpy()\n",
    "#         y = sub['meth'].to_numpy()\n",
    "\n",
    "#         # Optional smoothing\n",
    "#         if smooth_sigma and smooth_sigma > 0:\n",
    "#             y_smooth = gaussian_filter1d(y, sigma=float(smooth_sigma))\n",
    "#         else:\n",
    "#             y_smooth = y\n",
    "\n",
    "#         color = color_map[cid]\n",
    "\n",
    "#         # Background\n",
    "#         ax.set_facecolor(color)\n",
    "#         # Fill and line\n",
    "#         ax.fill_between(x, y_smooth, color='white')\n",
    "#         # ax.plot(x, y_smooth, color='white', linewidth=1)\n",
    "\n",
    "#         # Labels and axes\n",
    "#         pct = float(proportions.get(cid, 0.0))\n",
    "#         ax.set_ylabel(f\"Cluster {cid} | {pct:.1f}%\", fontsize=8)\n",
    "#         ax.set_ylim(0, 1.05)\n",
    "\n",
    "#         # Determine x-range\n",
    "#         x_min = np.nanmin(x) if start is None else start\n",
    "#         x_max = np.nanmax(x) if end is None else end\n",
    "#         ax.set_xlim(x_min, x_max)\n",
    "\n",
    "#         # Grid\n",
    "#         ax.grid(True, axis='x', linestyle='--', linewidth=0.5, color='gray')\n",
    "\n",
    "#     # Shared x formatting\n",
    "#     for ax in axes[:-1]:\n",
    "#         ax.tick_params(labelbottom=False)\n",
    "\n",
    "#     axes[-1].xaxis.set_major_locator(MultipleLocator(100))\n",
    "#     axes[-1].xaxis.set_major_formatter(FuncFormatter(lambda v, pos: f\"{int(v)}\"))\n",
    "#     axes[-1].get_xaxis().get_offset_text().set_visible(False)\n",
    "\n",
    "#     # Global labels\n",
    "#     fig.text(0.95, 0.5, 'Methylation level', va='center', ha='center', rotation=-90, fontsize=10)\n",
    "#     fig.text(\n",
    "#         0.05,\n",
    "#         0.5,\n",
    "#         \"Cluster number | Cluster proportion (ordered by cluster mean methylation)\",\n",
    "#         fontsize=10,\n",
    "#         rotation=90,\n",
    "#         va='center',\n",
    "#         ha='center'\n",
    "#     )\n",
    "#     fig.text(0.5, 0.04, \"Genomic coordinate\", va='center', ha='center', fontsize=10)\n",
    "\n",
    "#     plt.subplots_adjust(hspace=0.05, top=0.92)\n",
    "#     plt.setp(axes[-1].get_xticklabels(), rotation=45, ha='right', rotation_mode='anchor')\n",
    "#     axes[-1].tick_params(axis='x', labelsize=8)\n",
    "#     fig.subplots_adjust(bottom=0.18)\n",
    "\n",
    "#     plt.show()\n",
    "#     return fig, axes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bulk_centroids(\n",
    "    df: pd.DataFrame,\n",
    "    use_cov_for_proportion: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute per-cluster centroids across the entire bulk table.\n",
    "\n",
    "    Input df columns: ['cluster','C_pos','meth','cov'].\n",
    "    Returns:\n",
    "      P_df: clusters × bins matrix (mean methylation per bin)\n",
    "      cov_df: clusters × bins matrix (coverage per bin)\n",
    "      meta_df: per-cluster summary with ['cluster_id','n_rows_or_cov','proportion']\n",
    "      positions: sorted integer bin coordinates (C_pos)\n",
    "    \"\"\"\n",
    "    required = {'cluster','C_pos','meth','cov'}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Dataframe missing columns: {missing}\")\n",
    "\n",
    "    d = df.copy()\n",
    "    d['C_pos'] = pd.to_numeric(d['C_pos'], errors='coerce')\n",
    "    d['meth']  = pd.to_numeric(d['meth'],  errors='coerce')\n",
    "    d['cov']   = pd.to_numeric(d['cov'],   errors='coerce')\n",
    "    d = d.dropna(subset=['cluster','C_pos','meth'])\n",
    "\n",
    "    # Aggregate duplicates at the same position within cluster\n",
    "    agg = (\n",
    "        d.groupby(['cluster','C_pos'], as_index=False)\n",
    "         .agg(meth_mean=('meth','mean'),\n",
    "              cov_agg=('cov','first'))  # use 'sum' if cov varies per row\n",
    "    )\n",
    "\n",
    "    # Pivot to wide: cluster × position\n",
    "    P_df = agg.pivot(index='cluster', columns='C_pos', values='meth_mean')\n",
    "    cov_df = agg.pivot(index='cluster', columns='C_pos', values='cov_agg')\n",
    "\n",
    "    # Sort columns and convert to ints\n",
    "    positions = np.array(sorted(P_df.columns.astype(int)), dtype=int)\n",
    "    P_df = P_df.reindex(columns=positions)\n",
    "    cov_df = cov_df.reindex(columns=positions)\n",
    "\n",
    "    # Proportions (height ratios)\n",
    "    g = d.groupby('cluster')\n",
    "    if use_cov_for_proportion:\n",
    "        cov_first = g['cov'].first()\n",
    "        cov_var   = g['cov'].var()\n",
    "        cov_tot   = g['cov'].sum() if np.nanmax(cov_var.fillna(0)) > 0 else cov_first\n",
    "        total_cov = float(cov_tot.sum()) if cov_tot.sum() is not None else 0.0\n",
    "        proportions = (cov_tot / (total_cov if total_cov > 0 else 1.0)) * 100.0\n",
    "        n_metric = cov_tot.astype(float)\n",
    "        n_label = 'cov_total'\n",
    "    else:\n",
    "        counts = g.size()\n",
    "        proportions = (counts / counts.sum()) * 100.0\n",
    "        n_metric = counts.astype(float)\n",
    "        n_label = 'n_rows'\n",
    "\n",
    "    meta_rows = []\n",
    "    for cid in P_df.index:\n",
    "        meta_rows.append({\n",
    "            'cluster_id': cid if isinstance(cid, (int, np.integer)) else str(cid),\n",
    "            n_label: float(n_metric.get(cid, np.nan)),\n",
    "            'proportion': float(proportions.get(cid, 0.0))\n",
    "        })\n",
    "    meta_df = pd.DataFrame(meta_rows).sort_values('cluster_id').reset_index(drop=True)\n",
    "\n",
    "    return P_df, cov_df, meta_df, positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gene_centroids(\n",
    "    df_reads: pd.DataFrame,\n",
    "    df_map: pd.DataFrame,  # ['readid', 'locus_cluster', 'locus_cluster_color']\n",
    "    gene_tss: str,\n",
    "    read_id_col: str = 'readid',\n",
    "    cluster_col: str = 'locus_cluster',\n",
    "    color_col: str = 'locus_cluster_color',\n",
    "    extra_meta_cols: list | None = None  # e.g., ['gene_tss', 'group']\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute per-cluster average methylation per bin (and coverage) for one gene.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    profiles_df : pd.DataFrame\n",
    "        Mean methylation per cluster × bin.\n",
    "    coverage_df : pd.DataFrame\n",
    "        Number of reads with non-NaN methylation per cluster × bin.\n",
    "    meta_df : pd.DataFrame\n",
    "        Metadata for each cluster: [gene_tss, cluster_id, n_reads, proportion, color].\n",
    "    positions : np.ndarray\n",
    "        Sorted numeric bin positions (integers).\n",
    "    \"\"\"\n",
    "    # --- Ensure read IDs are present as a column ---\n",
    "    if read_id_col not in df_reads.columns:\n",
    "        df_w = df_reads.reset_index().rename(columns={'index': read_id_col})\n",
    "    else:\n",
    "        df_w = df_reads.copy()\n",
    "\n",
    "    # --- Merge cluster/color mapping ---\n",
    "    dfg = pd.merge(\n",
    "        df_w,\n",
    "        df_map[[read_id_col, cluster_col, color_col]],\n",
    "        on=read_id_col,\n",
    "        how='inner'\n",
    "    )\n",
    "\n",
    "    if dfg.empty:\n",
    "        raise ValueError(\"No overlapping reads between df_reads and df_map.\")\n",
    "\n",
    "    # --- Identify metadata columns ---\n",
    "    meta_cols = {read_id_col, cluster_col, color_col}\n",
    "    if extra_meta_cols:\n",
    "        meta_cols |= set(extra_meta_cols)\n",
    "\n",
    "    # --- Select candidate bin columns ---\n",
    "    candidate_cols = [c for c in dfg.columns if c not in meta_cols]\n",
    "\n",
    "    # --- Keep only columns with numeric names AND numeric data ---\n",
    "    def int_like(name):\n",
    "        return (\n",
    "            isinstance(name, (int, np.integer))\n",
    "            or (isinstance(name, str) and name.strip().lstrip('-').isdigit())\n",
    "        )\n",
    "\n",
    "    bin_cols_num = []\n",
    "    positions = []\n",
    "    for c in candidate_cols:\n",
    "        if int_like(c) and pd.api.types.is_numeric_dtype(dfg[c]):\n",
    "            bin_cols_num.append(c)\n",
    "            positions.append(int(c) if not isinstance(c, (int, np.integer)) else int(c))\n",
    "\n",
    "    if not bin_cols_num:\n",
    "        raise ValueError(\n",
    "            \"No numeric bin columns found. Check df_reads columns and extra_meta_cols.\"\n",
    "        )\n",
    "\n",
    "    positions = np.array(sorted(positions), dtype=int)\n",
    "\n",
    "    # --- Build clean numeric-column version ---\n",
    "    col_map = {\n",
    "        c: (int(c) if not isinstance(c, (int, np.integer)) else int(c))\n",
    "        for c in bin_cols_num\n",
    "    }\n",
    "    dfg_bins = dfg[[cluster_col, color_col] + bin_cols_num].rename(columns=col_map)\n",
    "    dfg_bins = dfg_bins[[cluster_col, color_col] + positions.tolist()]\n",
    "\n",
    "    # --- Group by cluster and compute stats ---\n",
    "    profiles = {}\n",
    "    coverage = {}\n",
    "    colors = {}\n",
    "    counts_per_cluster = {}\n",
    "\n",
    "    for cid, sub in dfg_bins.groupby(cluster_col):\n",
    "        vals = sub[positions]\n",
    "        profiles[cid] = vals.mean(axis=0, skipna=True)\n",
    "        coverage[cid] = vals.notna().sum(axis=0)\n",
    "        colors[cid] = (\n",
    "            sub[color_col].iloc[0]\n",
    "            if color_col in sub.columns and not sub[color_col].isna().all()\n",
    "            else None\n",
    "        )\n",
    "        counts_per_cluster[cid] = int(len(sub))\n",
    "\n",
    "    profiles_df = pd.DataFrame(profiles).T.reindex(columns=positions)\n",
    "    coverage_df = pd.DataFrame(coverage).T.reindex(columns=positions)\n",
    "\n",
    "    profiles_df.index.name = 'cluster'\n",
    "    profiles_df.columns.name = 'C_pos'\n",
    "\n",
    "    # --- Build meta table ---\n",
    "    total_reads = sum(counts_per_cluster.values())\n",
    "    meta_rows = []\n",
    "    for cid in profiles_df.index:\n",
    "        n_reads = counts_per_cluster.get(cid, 0)\n",
    "        pct = (n_reads / total_reads * 100.0) if total_reads > 0 else 0.0\n",
    "        meta_rows.append({\n",
    "            'gene_tss': gene_tss,\n",
    "            'cluster_id': int(cid),\n",
    "            'n_reads': int(n_reads),\n",
    "            'proportion': float(pct),\n",
    "            'cluster_color': colors.get(cid)\n",
    "        })\n",
    "\n",
    "    meta_df = (\n",
    "        pd.DataFrame(meta_rows)\n",
    "        .sort_values('cluster_id')\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "\n",
    "    return profiles_df, coverage_df, meta_df, positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_centroids_with_shading(\n",
    "    P_df: pd.DataFrame,                 # index = cluster_id, columns = positions (int), values = mean methylation\n",
    "    positions: np.ndarray,              # sorted integer positions\n",
    "    meta_df: pd.DataFrame,              # must include ['cluster_id', 'proportion']; optional ['n_reads', 'cluster_color']\n",
    "    coverage_df: pd.DataFrame | None = None,  # same shape as P_df; per-bin contributing read counts\n",
    "    hex_colors=None,\n",
    "    smooth_sigma: float = 0.5,\n",
    "    start: int | None = None,\n",
    "    end: int | None = None,\n",
    "    title: str = \"Average DNA Methylation Profiles per Cluster\",\n",
    "    show_pol2_line: bool = True,\n",
    "    shade_missing: bool = True,          # enable/disable shading\n",
    "    missingness_threshold: float = 0.7,  # shade sites where missingness >= threshold\n",
    "    bin_width: int | None = None         # if None, inferred from median bin spacing\n",
    "):\n",
    "    # --- Clusters and proportions ---\n",
    "    clusters = list(P_df.index)\n",
    "    n_clusters = len(clusters)\n",
    "\n",
    "    if 'proportion' in meta_df.columns:\n",
    "        prop_map = {int(row['cluster_id']): float(row['proportion'])\n",
    "                    for _, row in meta_df.iterrows()}\n",
    "    else:\n",
    "        prop_map = {int(cid): 100.0 / max(n_clusters, 1) for cid in clusters}\n",
    "\n",
    "    # --- Colors ---\n",
    "    if 'cluster_color' in meta_df.columns and meta_df['cluster_color'].notna().any():\n",
    "        color_map = {int(row['cluster_id']): row['cluster_color']\n",
    "                     for _, row in meta_df.iterrows()}\n",
    "    else:\n",
    "        import matplotlib.cm as cm\n",
    "        if hex_colors is None:\n",
    "            cmap = cm.get_cmap('tab20', n_clusters)\n",
    "            hex_colors = [cm.colors.to_hex(cmap(i)) for i in range(n_clusters)]\n",
    "        color_map = {cid: hex_colors[i % len(hex_colors)]\n",
    "                     for i, cid in enumerate(clusters)}\n",
    "\n",
    "    # --- Height ratios from proportions ---\n",
    "    height_ratios = [max(1e-3, float(prop_map.get(int(cid), 0.0)))\n",
    "                     for cid in clusters]\n",
    "\n",
    "    # --- Infer bin width ---\n",
    "    if bin_width is None and len(positions) > 1:\n",
    "        diffs = np.diff(positions)\n",
    "        bin_width = int(np.median(diffs)) if len(diffs) else 1\n",
    "    if bin_width is None:\n",
    "        bin_width = 1\n",
    "\n",
    "    # --- Estimate per-cluster size for missingness ---\n",
    "    n_reads_map = {}\n",
    "    if 'n_reads' in meta_df.columns:\n",
    "        n_reads_map = {int(row['cluster_id']): int(row['n_reads'])\n",
    "                       for _, row in meta_df.iterrows()}\n",
    "    elif coverage_df is not None:\n",
    "        for cid in clusters:\n",
    "            try:\n",
    "                n_reads_map[int(cid)] = int(np.nanmax(\n",
    "                    coverage_df.loc[cid].to_numpy(float)))\n",
    "            except Exception:\n",
    "                n_reads_map[int(cid)] = 0\n",
    "\n",
    "    # --- Create figure ---\n",
    "    fig, axes = plt.subplots(\n",
    "        n_clusters, 1, figsize=(16, 18), sharex=True,\n",
    "        gridspec_kw={'height_ratios': height_ratios}\n",
    "    )\n",
    "    if n_clusters == 1:\n",
    "        axes = [axes]\n",
    "    fig.suptitle(title, fontsize=14, y=0.97)\n",
    "\n",
    "    # --- Plot each cluster ---\n",
    "    for i, cid in enumerate(clusters):\n",
    "        ax = axes[i]\n",
    "        y = P_df.loc[cid].to_numpy(float)\n",
    "        x = positions\n",
    "        y_smooth = (gaussian_filter1d(y, sigma=float(smooth_sigma))\n",
    "                    if smooth_sigma and smooth_sigma > 0 else y)\n",
    "\n",
    "        # --- Shading of high missingness sites ---\n",
    "        if shade_missing and coverage_df is not None:\n",
    "            cs = int(n_reads_map.get(int(cid), 0))\n",
    "            if cs > 0:\n",
    "                cov = coverage_df.loc[cid].to_numpy(float)\n",
    "                miss_frac = 1.0 - (cov / cs)\n",
    "                mask = miss_frac >= float(missingness_threshold)\n",
    "\n",
    "                runs = []\n",
    "                run_start = None\n",
    "                prev = None\n",
    "                for p, ok in zip(x, mask):\n",
    "                    if ok and run_start is None:\n",
    "                        run_start = p\n",
    "                        prev = p\n",
    "                    elif ok:\n",
    "                        prev = p\n",
    "                    elif (not ok) and run_start is not None:\n",
    "                        runs.append((run_start, prev))\n",
    "                        run_start = None\n",
    "                if run_start is not None:\n",
    "                    runs.append((run_start, prev))\n",
    "\n",
    "                for a, b in runs:\n",
    "                    ax.axvspan(a - bin_width / 2, b + bin_width / 2,\n",
    "                               color=\"#969696\", alpha=0.7, zorder=0.9)\n",
    "\n",
    "        # --- Plot profile ---\n",
    "        ax.set_facecolor(color_map.get(cid, '#cccccc'))\n",
    "        ax.fill_between(x, y_smooth, color='white')\n",
    "\n",
    "        # --- Axes, labels ---\n",
    "        pct = float(prop_map.get(int(cid), 0.0))\n",
    "        ax.set_ylabel(f\"Cluster {cid} | {pct:.1f}%\", fontsize=8)\n",
    "        ax.set_ylim(0, 1.05)\n",
    "\n",
    "        x_min = np.nanmin(x) if start is None else start\n",
    "        x_max = np.nanmax(x) if end is None else end\n",
    "        ax.set_xlim(x_min, x_max)\n",
    "\n",
    "        ax.grid(True, axis='x', linestyle='--',\n",
    "                linewidth=0.5, color='gray')\n",
    "        if show_pol2_line:\n",
    "            ax.axvline(x=0, color=\"#C80028\", linestyle=\"-\", linewidth=2)\n",
    "\n",
    "    # --- Shared x-axis formatting ---\n",
    "    for ax in axes[:-1]:\n",
    "        ax.tick_params(labelbottom=False)\n",
    "    axes[-1].xaxis.set_major_locator(MultipleLocator(100))\n",
    "    axes[-1].xaxis.set_major_formatter(FuncFormatter(lambda v, pos: f\"{int(v)}\"))\n",
    "    axes[-1].get_xaxis().get_offset_text().set_visible(False)\n",
    "\n",
    "    # --- Global labels ---\n",
    "    fig.text(0.95, 0.5, 'Methylation level',\n",
    "             va='center', ha='center', rotation=-90, fontsize=10)\n",
    "    fig.text(0.05, 0.5, \"Cluster number | Cluster proportion\",\n",
    "             fontsize=10, rotation=90, va='center', ha='center')\n",
    "    fig.text(0.5, 0.04, \"Genomic coordinate\",\n",
    "             va='center', ha='center', fontsize=10)\n",
    "\n",
    "    plt.subplots_adjust(hspace=0.05, top=0.92)\n",
    "    plt.setp(axes[-1].get_xticklabels(), rotation=45,\n",
    "             ha='right', rotation_mode='anchor')\n",
    "    axes[-1].tick_params(axis='x', labelsize=8)\n",
    "    fig.subplots_adjust(bottom=0.18)\n",
    "\n",
    "    plt.show()\n",
    "    return fig, axes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# 1. Masked cosine similarity with NaN handling and column alignment\n",
    "# -------------------------------------------------------------------\n",
    "def masked_cosine_cross(\n",
    "    G_df: pd.DataFrame,\n",
    "    P_df: pd.DataFrame,\n",
    "    min_overlap: int = 10,\n",
    "    center_rows: bool = True,\n",
    "    weights=None  # optional: pd.Series indexed by columns (positions), or array-like\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute m × p cosine similarities between rows of G and rows of P,\n",
    "    masking NaNs per pair, aligning columns by intersection, and requiring\n",
    "    at least `min_overlap` shared non-NaN bins.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    G_df, P_df : pd.DataFrame\n",
    "        DataFrames with columns = positions (bins). Columns may differ;\n",
    "        they will be aligned by intersection.\n",
    "    weights : pd.Series or array-like, optional\n",
    "        Per-bin reliability weights. If provided, columns are scaled by sqrt(weights).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    S : np.ndarray\n",
    "        (m × p) cosine similarity matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    # ---- 1) Coerce column labels to int and sort ----\n",
    "    def _coerce_sort(df):\n",
    "        out = df.copy()\n",
    "        col_series = pd.Series(out.columns, dtype=str).str.strip()\n",
    "        col_numeric = pd.to_numeric(col_series, errors='coerce')\n",
    "        if col_numeric.isna().any():\n",
    "            bad = col_series[col_numeric.isna()].tolist()\n",
    "            raise ValueError(f\"Non-numeric bin columns found: {bad}\")\n",
    "        out.columns = col_numeric.astype(int)\n",
    "        if out.columns.duplicated().any():\n",
    "            out = out.groupby(level=0, axis=1).mean()\n",
    "        return out.sort_index(axis=1)\n",
    "\n",
    "    Gc = _coerce_sort(G_df)\n",
    "    Pc = _coerce_sort(P_df)\n",
    "\n",
    "    # ---- 2) Align to intersection of positions ----\n",
    "    common = np.intersect1d(Gc.columns.values, Pc.columns.values)\n",
    "    if common.size == 0:\n",
    "        raise ValueError(\"No overlapping bins between G and P.\")\n",
    "\n",
    "    Gc = Gc.reindex(columns=common)\n",
    "    Pc = Pc.reindex(columns=common)\n",
    "\n",
    "    # ---- 3) Apply weights (scale columns by sqrt(weights)) ----\n",
    "    if weights is not None:\n",
    "        if isinstance(weights, pd.Series):\n",
    "            sw = np.sqrt(np.maximum(weights.reindex(common).to_numpy(float), 0.0))\n",
    "        else:\n",
    "            w = np.asarray(weights, float)\n",
    "            if w.shape[0] != common.size:\n",
    "                try:\n",
    "                    w_series = pd.Series(w, index=P_df.columns)\n",
    "                    sw = np.sqrt(np.maximum(w_series.reindex(common).to_numpy(float), 0.0))\n",
    "                except Exception:\n",
    "                    raise ValueError(f\"weights length {w.shape[0]} must equal n_bins {common.size}.\")\n",
    "            else:\n",
    "                sw = np.sqrt(np.maximum(w, 0.0))\n",
    "        Gc = Gc.mul(sw, axis=1)\n",
    "        Pc = Pc.mul(sw, axis=1)\n",
    "\n",
    "    # ---- 4) Compute masked cosine over aligned matrices ----\n",
    "    G = Gc.to_numpy(float)\n",
    "    P = Pc.to_numpy(float)\n",
    "    m, d = G.shape\n",
    "    p = P.shape[0]\n",
    "    S = np.zeros((m, p), float)\n",
    "\n",
    "    G_mask = np.isfinite(G)\n",
    "    P_mask = np.isfinite(P)\n",
    "\n",
    "    for i in range(m):\n",
    "        gi, mi = G[i], G_mask[i]\n",
    "        for j in range(p):\n",
    "            pj, mj = P[j], P_mask[j]\n",
    "            ov = mi & mj\n",
    "            n = int(ov.sum())\n",
    "            if n < int(min_overlap):\n",
    "                S[i, j] = 0.0\n",
    "                continue\n",
    "            vi, vj = gi[ov], pj[ov]\n",
    "            if center_rows:\n",
    "                vi -= vi.mean()\n",
    "                vj -= vj.mean()\n",
    "            denom = np.linalg.norm(vi) * np.linalg.norm(vj)\n",
    "            S[i, j] = float(np.dot(vi, vj) / denom) if denom > 0 else 0.0\n",
    "\n",
    "    return S\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2. Utility functions for Pearson-based similarity\n",
    "# -------------------------------------------------------------------\n",
    "def _zscore_rows(M, eps: float = 1e-6) -> np.ndarray:\n",
    "    M = np.asarray(M, float)\n",
    "    mu = np.nanmean(M, axis=1, keepdims=True)\n",
    "    sd = np.nanstd(M, axis=1, keepdims=True)\n",
    "    sd = np.where(sd < eps, eps, sd)\n",
    "    return (M - mu) / sd\n",
    "\n",
    "\n",
    "def _pairwise_pearson(G: np.ndarray, P: np.ndarray) -> np.ndarray:\n",
    "    G = np.asarray(G, float)\n",
    "    P = np.asarray(P, float)\n",
    "    m, d = G.shape\n",
    "    p = P.shape[0]\n",
    "    S = np.zeros((m, p), float)\n",
    "\n",
    "    for i in range(m):\n",
    "        gi = G[i]\n",
    "        for j in range(p):\n",
    "            pj = P[j]\n",
    "            mask = np.isfinite(gi) & np.isfinite(pj)\n",
    "            if mask.sum() < 2:\n",
    "                S[i, j] = 0.0\n",
    "                continue\n",
    "            gi_m, pj_m = gi[mask], pj[mask]\n",
    "            gi_sd, pj_sd = gi_m.std(), pj_m.std()\n",
    "            gi_m = (gi_m - gi_m.mean()) / (gi_sd if gi_sd > 0 else 1.0)\n",
    "            pj_m = (pj_m - pj_m.mean()) / (pj_sd if pj_sd > 0 else 1.0)\n",
    "            S[i, j] = float(np.dot(gi_m, pj_m) / mask.sum())\n",
    "    return S\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3. Compute similarity and distance matrices\n",
    "# -------------------------------------------------------------------\n",
    "def compute_similarity_matrix(\n",
    "    G,\n",
    "    P,\n",
    "    method: str = 'pearson',\n",
    "    use_abs: bool = False,\n",
    "    center_rows: bool = True,\n",
    "    weights=None,\n",
    "    min_overlap: int = 10\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute similarity (and corresponding distance) matrix between rows of G and P.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    G, P : array-like or pd.DataFrame\n",
    "        Matrices or DataFrames representing profiles (rows=clusters, cols=bins).\n",
    "        If DataFrames, columns must be numeric (positions).\n",
    "    method : {'pearson'}, default='pearson'\n",
    "        Similarity metric to use. Currently only Pearson is implemented here.\n",
    "    use_abs : bool, default=False\n",
    "        Take absolute value of similarity if True.\n",
    "    center_rows : bool, default=True\n",
    "        Whether to mean-center rows before computing similarity.\n",
    "    weights : array-like, optional\n",
    "        Optional positional weights (not used in current implementation).\n",
    "    min_overlap : int, default=10\n",
    "        Minimum number of overlapping bins required for valid correlation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    S : np.ndarray\n",
    "        Similarity matrix (rows=G, cols=P).\n",
    "    D : np.ndarray\n",
    "        Distance matrix (1 - S, clipped at 0).\n",
    "    prot_ids : list\n",
    "        Prototype identifiers (row labels of P if DataFrame, else range).\n",
    "    \"\"\"\n",
    "\n",
    "    if method == 'pearson':\n",
    "        # ---- Coerce and align DataFrames to common bins ----\n",
    "        if isinstance(G, pd.DataFrame) and isinstance(P, pd.DataFrame):\n",
    "\n",
    "            def _coerce_sort(df):\n",
    "                out = df.copy()\n",
    "                cols = pd.Series(out.columns, dtype=str).str.strip()\n",
    "                nums = pd.to_numeric(cols, errors='coerce')\n",
    "                if nums.isna().any():\n",
    "                    bad = cols[nums.isna()].tolist()\n",
    "                    raise ValueError(f\"Non-numeric bin columns: {bad}\")\n",
    "                out.columns = nums.astype(int)\n",
    "                if out.columns.duplicated().any():\n",
    "                    out = out.groupby(level=0, axis=1).mean()\n",
    "                return out.sort_index(axis=1)\n",
    "\n",
    "            Gc = _coerce_sort(G)\n",
    "            Pc = _coerce_sort(P)\n",
    "            common = np.intersect1d(Gc.columns.values, Pc.columns.values)\n",
    "            if common.size == 0:\n",
    "                raise ValueError(\"No overlapping bins between G and P.\")\n",
    "\n",
    "            G_arr = Gc.reindex(columns=common).to_numpy(float)\n",
    "            P_arr = Pc.reindex(columns=common).to_numpy(float)\n",
    "\n",
    "        else:\n",
    "            G_arr = np.asarray(G, float)\n",
    "            P_arr = np.asarray(P, float)\n",
    "\n",
    "        # ---- Compute Pearson correlation ----\n",
    "        Zg = _zscore_rows(G_arr)\n",
    "        Zp = _zscore_rows(P_arr)\n",
    "        S = _pairwise_pearson(Zg, Zp)  # masks NaNs per pair\n",
    "        S = np.nan_to_num(S, nan=0.0)\n",
    "\n",
    "        if use_abs:\n",
    "            S = np.abs(S)\n",
    "\n",
    "        D = 1.0 - S\n",
    "        D[D < 0] = 0.0\n",
    "\n",
    "        # ---- Preserve prototype IDs ----\n",
    "        prot_ids = list(P.index) if isinstance(P, pd.DataFrame) else list(range(P_arr.shape[0]))\n",
    "\n",
    "        return S, D, prot_ids\n",
    "    \n",
    "    elif method == 'cosine':\n",
    "        if not isinstance(G, pd.DataFrame) or not isinstance(P, pd.DataFrame):\n",
    "            raise ValueError(\"For masked cosine, pass G and P as DataFrames (columns = positions).\")\n",
    "\n",
    "        S = masked_cosine_cross(\n",
    "            G_df=G,\n",
    "            P_df=P,\n",
    "            min_overlap=min_overlap,\n",
    "            center_rows=center_rows,\n",
    "            weights=weights,\n",
    "        )\n",
    "\n",
    "        if use_abs:\n",
    "            S = np.abs(S)\n",
    "        D = 1.0 - S\n",
    "        D[D < 0] = 0.0\n",
    "        prot_ids = list(P.index)\n",
    "        return S, D, prot_ids\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"method must be 'pearson' or 'cosine'.\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 4. Assign gene-level clusters to consensus clusters\n",
    "# -------------------------------------------------------------------\n",
    "def assign_gene_clusters_to_consensus(\n",
    "    G,\n",
    "    meta: pd.DataFrame,\n",
    "    P,\n",
    "    method: str = 'pearson',\n",
    "    use_abs: bool = False,\n",
    "    center_rows: bool = True,\n",
    "    weights=None,\n",
    "    min_overlap: int = 10,\n",
    "    min_similarity: float | None = None,\n",
    "    allow_unassigned: bool = True,\n",
    "    capacity_per_type: int = 1,\n",
    "):\n",
    "    \"\"\"\n",
    "    Assign gene clusters (rows in G) to consensus clusters (rows in P)\n",
    "    using pairwise similarity + Hungarian assignment.\n",
    "    \"\"\"\n",
    "\n",
    "    S, D_base, prot_ids = compute_similarity_matrix(\n",
    "        G=G,\n",
    "        P=P,\n",
    "        method=method,\n",
    "        use_abs=use_abs,\n",
    "        center_rows=center_rows,\n",
    "        weights=weights,\n",
    "        min_overlap=min_overlap,\n",
    "    )\n",
    "\n",
    "    meta_df = meta.copy()\n",
    "    if 'size' not in meta_df.columns:\n",
    "        meta_df['size'] = np.nan\n",
    "\n",
    "    assignments = []\n",
    "    for gene_id, row_idx in meta_df.groupby('gene_tss').groups.items():\n",
    "        row_idx = list(row_idx)\n",
    "        D = D_base[row_idx, :]\n",
    "\n",
    "        if capacity_per_type > 1:\n",
    "            P_rep = np.repeat(np.arange(D.shape[1]), repeats=int(capacity_per_type))\n",
    "            D_aug = np.tile(D, reps=(1, int(capacity_per_type)))\n",
    "        else:\n",
    "            P_rep = np.arange(D.shape[1])\n",
    "            D_aug = D\n",
    "\n",
    "        if allow_unassigned:\n",
    "            penalty = (1.0 - float(min_similarity)) if min_similarity is not None else float(np.nanmax(D_aug) + 0.05)\n",
    "            dummy = np.full((D_aug.shape[0], D_aug.shape[0]), penalty, dtype=float)\n",
    "            D_final = np.hstack([D_aug, dummy])\n",
    "            col_map = np.concatenate([P_rep, -np.ones(dummy.shape[1], dtype=int)])\n",
    "        else:\n",
    "            D_final = D_aug\n",
    "            col_map = P_rep\n",
    "\n",
    "        r_idx, c_idx = linear_sum_assignment(D_final)\n",
    "\n",
    "        for r, c in zip(r_idx, c_idx):\n",
    "            cons_col = col_map[c]\n",
    "            best_cost = D_final[r, c]\n",
    "            sim = float(1.0 - best_cost)\n",
    "            row_costs = D_final[r]\n",
    "            mask = np.ones_like(row_costs, dtype=bool)\n",
    "            mask[c] = False\n",
    "            second_best_cost = np.min(row_costs[mask]) if mask.any() else np.nan\n",
    "            second_best_sim = float(1.0 - second_best_cost) if np.isfinite(second_best_cost) else np.nan\n",
    "            margin = float(second_best_cost - best_cost) if np.isfinite(second_best_cost) else np.nan\n",
    "            cons_id = None if cons_col < 0 else prot_ids[int(cons_col)]\n",
    "\n",
    "            assignments.append({\n",
    "                'gene_tss': gene_id,\n",
    "                'cluster_id': meta_df.iloc[r]['cluster_id'],\n",
    "                'size': meta_df.iloc[r]['size'],\n",
    "                'consensus_id': cons_id,\n",
    "                'similarity': sim,\n",
    "                'second_best_similarity': second_best_sim,\n",
    "                'margin': margin,\n",
    "            })\n",
    "\n",
    "    assign_df = pd.DataFrame(assignments)\n",
    "    return assign_df, S, D_base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gene_mapping_heatmap(\n",
    "    assign_df: pd.DataFrame,\n",
    "    S: np.ndarray,                  # full similarity matrix returned by assign_gene_clusters_to_consensus\n",
    "    meta: pd.DataFrame,             # meta passed to assign (aligned to rows of G)\n",
    "    P,                              # bulk prototypes (DataFrame for cosine, array for pearson)\n",
    "    gene_id: str,\n",
    "    sort_rows: bool = True,         # sort by assigned prototype then by max similarity\n",
    "    cmap: str = \"viridis\",\n",
    "    vmin: float = 0.0,\n",
    "    vmax: float = 1.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot a heatmap of similarities (rows = gene clusters, cols = bulk prototypes)\n",
    "    for a single gene and overlay markers for the Hungarian assignment.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - S must be the full similarity matrix returned by `assign_gene_clusters_to_consensus`.\n",
    "      Rows of S must align with rows of `meta` (i.e., meta.loc[i] corresponds to S[i]).\n",
    "    - P can be a DataFrame (then prot IDs are P.index) or a numpy array (then prot IDs are 0..P.shape[0]-1).\n",
    "    \"\"\"\n",
    "    # Prototype IDs (columns of S)\n",
    "    if isinstance(P, pd.DataFrame):\n",
    "        prot_ids = list(P.index)\n",
    "    else:\n",
    "        prot_ids = list(range(P.shape[0]))\n",
    "\n",
    "    # Row indices for the selected gene (rows of S)\n",
    "    idx_rows = meta.index[meta[\"gene_tss\"] == gene_id].tolist()\n",
    "    if not idx_rows:\n",
    "        raise ValueError(f\"No rows in meta for gene_id={gene_id}.\")\n",
    "\n",
    "    # Subset similarity and labels (rows in the same order as S)\n",
    "    S_sub = S[idx_rows, :]\n",
    "    row_labels = meta.loc[idx_rows, \"cluster_id\"].astype(int).tolist()\n",
    "    col_labels = prot_ids\n",
    "\n",
    "    # Row ordering / sorting\n",
    "    if sort_rows:\n",
    "        ass_sub = assign_df[assign_df[\"gene_tss\"] == gene_id].copy()\n",
    "        # map cluster_id -> assigned consensus_id\n",
    "        ass_map = {int(r[\"cluster_id\"]): r[\"consensus_id\"] for _, r in ass_sub.iterrows()}\n",
    "        sort_keys = []\n",
    "        for i, cid in enumerate(row_labels):\n",
    "            assigned = ass_map.get(int(cid), None)\n",
    "            j_ass = col_labels.index(assigned) if (assigned is not None and assigned in col_labels) else -1\n",
    "            # use -max_similarity so that higher similarity sorts earlier within same assigned group\n",
    "            sort_keys.append((j_ass, -float(np.nanmax(S_sub[i])) if S_sub.size else 0.0))\n",
    "        order = sorted(range(len(sort_keys)), key=lambda ii: sort_keys[ii])\n",
    "        S_plot = S_sub[order, :]\n",
    "        row_labels_plot = [row_labels[i] for i in order]\n",
    "    else:\n",
    "        S_plot = S_sub\n",
    "        row_labels_plot = row_labels\n",
    "\n",
    "    # Create heatmap\n",
    "    fig, ax = plt.subplots(\n",
    "        figsize=(1.0 + 0.4 * len(col_labels), 0.6 + 0.4 * len(row_labels_plot))\n",
    "    )\n",
    "    sns.heatmap(\n",
    "        S_plot,\n",
    "        xticklabels=col_labels,\n",
    "        yticklabels=row_labels_plot,\n",
    "        cmap=cmap,\n",
    "        vmin=vmin,\n",
    "        vmax=vmax,\n",
    "        cbar_kws={\"label\": \"similarity\"},\n",
    "        ax=ax,\n",
    "    )\n",
    "    ax.set_title(f\"Mapping of {gene_id} clusters to bulk prototypes\")\n",
    "    ax.set_xlabel(\"bulk consensus ID\")\n",
    "    ax.set_ylabel(\"gene cluster ID\")\n",
    "\n",
    "    # Overlay assignment markers\n",
    "    ass_sub = assign_df[assign_df[\"gene_tss\"] == gene_id]\n",
    "    # Build CID -> row index in the plotted order\n",
    "    row_index_map = {cid: i for i, cid in enumerate(row_labels_plot)}\n",
    "    for _, r in ass_sub.iterrows():\n",
    "        cid = int(r[\"cluster_id\"])\n",
    "        cons_id = r[\"consensus_id\"]\n",
    "        if cons_id is None or cons_id not in col_labels:\n",
    "            continue\n",
    "        i = row_index_map.get(cid, None)\n",
    "        j = col_labels.index(cons_id)\n",
    "        if i is None:\n",
    "            continue\n",
    "        # place marker at the center of the heatmap cell\n",
    "        ax.scatter(j + 0.5, i + 0.5, s=60, facecolors=\"none\", edgecolors=\"white\", linewidths=1.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "def plot_assignment_summary(assign_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Bar chart: number of gene clusters assigned to each bulk prototype (across all genes).\n",
    "    Unassigned clusters (consensus_id == NaN) are dropped.\n",
    "    \"\"\"\n",
    "    df = assign_df.dropna(subset=[\"consensus_id\"])\n",
    "    counts = df.groupby(\"consensus_id\").size().sort_values(ascending=False)\n",
    "    fig, ax = plt.subplots(figsize=(8, 3))\n",
    "    counts.plot(kind=\"bar\", ax=ax, color=\"#4c72b0\")\n",
    "    ax.set_ylabel(\"# gene clusters assigned\")\n",
    "    ax.set_xlabel(\"bulk consensus ID\")\n",
    "    ax.set_title(\"Assignment summary across genes\")\n",
    "    plt.tight_layout()\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "def recolor_meta_by_bulk(assign_df, meta_df, bulk_color_map, gene_id):\n",
    "    \"\"\"\n",
    "    Return a copy of meta_df for `gene_id` with 'cluster_color' replaced by the assigned\n",
    "    bulk prototype color (falls back to original color if unassigned).\n",
    "    \"\"\"\n",
    "    m = meta_df.copy()\n",
    "    ass_sub = assign_df[assign_df[\"gene_tss\"] == gene_id]\n",
    "    ass_map = {int(r[\"cluster_id\"]): r[\"consensus_id\"] for _, r in ass_sub.iterrows()}\n",
    "\n",
    "    colors = []\n",
    "    for _, r in m.iterrows():\n",
    "        cid = int(r[\"cluster_id\"])\n",
    "        cons = ass_map.get(cid, None)\n",
    "        col = bulk_color_map.get(cons, r.get(\"cluster_color\", None))\n",
    "        colors.append(col)\n",
    "    m = m.assign(cluster_color=colors)\n",
    "    return m\n",
    "\n",
    "\n",
    "def plot_assignment_confidence(assign_df: pd.DataFrame, gene_id: str | None = None):\n",
    "    \"\"\"\n",
    "    Scatter plot of best similarity vs. margin (second_best_similarity - best_similarity).\n",
    "    Small margin => ambiguous assignment.\n",
    "    \"\"\"\n",
    "    df = assign_df.copy()\n",
    "    if gene_id is not None:\n",
    "        df = df[df[\"gene_tss\"] == gene_id]\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    ax.scatter(df[\"similarity\"], df[\"margin\"], s=30, alpha=0.7)\n",
    "    ax.set_xlabel(\"best similarity\")\n",
    "    ax.set_ylabel(\"margin (second-best − best)\")\n",
    "    ax.set_title(\"Assignment confidence\" + (f\" ({gene_id})\" if gene_id else \"\"))\n",
    "    ax.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    return fig, ax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "##### Running code on the actual dataset to preprocess + plot the reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### BINNING THE BULK DATA TO BA ABLE TO DO REASSIGNMENT \n",
    "\n",
    "d= bin_then_matrix(\n",
    "    df_bulk,\n",
    "    indexes=['cluster'],\n",
    "    bin_size=50\n",
    ")\n",
    "\n",
    " # ---- Mean methylation per (cluster, bin) ----\n",
    "meth_agg = (\n",
    "        d.groupby(['cluster', 'bin'], as_index=False)\n",
    "        .agg(meth_mean=('meth', 'mean'))\n",
    "    )\n",
    "\n",
    "# Take unique cov per cluster (assumes cov constant per cluster)\n",
    "cov_cluster = d.groupby('cluster')['cov'].first()\n",
    "cov_rows = []\n",
    "for cid, sub in meth_agg.groupby('cluster'):\n",
    "    cov_val = float(cov_cluster.get(cid, np.nan))\n",
    "    for b in sub['bin'].unique():\n",
    "        cov_rows.append({'cluster': cid, 'bin': b, 'cov_bin': cov_val})\n",
    "cov_agg = pd.DataFrame(cov_rows)\n",
    "\n",
    "# ---- Merge meth and coverage ----\n",
    "agg = pd.merge(meth_agg, cov_agg, on=['cluster', 'bin'], how='left')\n",
    "\n",
    "# ---- Pivot to wide profiles ----\n",
    "profiles_df_bulk_binned = agg.pivot(index='cluster', columns='bin', values='meth_mean')\n",
    "coverage_df_bulk_binned = agg.pivot(index='cluster', columns='bin', values='cov_bin')\n",
    "\n",
    "    # ---- Sort columns and set names ----\n",
    "positions_bulk_binned = np.array(sorted(profiles_df_bulk_binned.columns.astype(int)), dtype=int)\n",
    "profiles_df_bulk_binned = profiles_df_bulk_binned.reindex(columns=positions_bulk_binned)\n",
    "coverage_df_bulk_binned = coverage_df_bulk_binned.reindex(columns=positions_bulk_binned)\n",
    "\n",
    "profiles_df_bulk_binned.index.name = 'cluster'\n",
    "profiles_df_bulk_binned.columns = pd.Index(positions_bulk_binned, name='C_pos')\n",
    "coverage_df_bulk_binned.index.name = 'cluster'\n",
    "coverage_df_bulk_binned.columns = pd.Index(positions_bulk_binned, name='C_pos')\n",
    "\n",
    "\n",
    "n_metric = cov_cluster.astype(float)\n",
    "total_cov= float(cov_cluster.sum()) if cov_cluster.sum() is not None else 0.0\n",
    "proportions = (cov_cluster / (total_cov if total_cov > 0 else 1.0)) * 100.0\n",
    "\n",
    "meta_rows = []\n",
    "for cid in profiles_df_bulk_binned.index:\n",
    "        meta_rows.append({\n",
    "            'cluster_id': cid if isinstance(cid, (int, np.integer)) else str(cid),\n",
    "            'cov_total': float(n_metric.get(cid, np.nan)),\n",
    "            'proportion': float(proportions.get(cid, 0.0))\n",
    "        })\n",
    "meta_df_bulk_binned = pd.DataFrame(meta_rows).sort_values('cluster_id').reset_index(drop=True)\n",
    "\n",
    "profiles_df_bulk_binned, \n",
    "meta_df_bulk_binned\n",
    "coverage_df_bulk_binned, \n",
    "positions_bulk_binned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "##RUNNING THE CODE ON THE ACTUAL DATASET -- applying preprocess_dataframe\n",
    "columns_to_drop=['read_strand', 'N', 'n_meth_motif',\n",
    "       'perc_meth_in_motif','peak_cov', 'peak_meth', 'C_in_motif',\n",
    "       'n', 'read_meth_mean', 'motif_id', 'gene2', 'bound', 'cov', 'peakid', 'meth_C_in_motif','read_meth_C_in_motif']\n",
    "\n",
    "df_proc = preprocess_dataframe(df, columns_to_drop)\n",
    "\n",
    "df_proc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHOLE PREPROCESSED DF THAT GETS BINNED UNDER METHYLATION MATRIX (i need it to flter the genes that we want to work on)\n",
    "\n",
    "df_whole_matrix= process_into_matrix(df_proc,gene=None,bin_size= 50)\n",
    "df_whole_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHOLE PREPROCESSED DF THAT IS UNBINNED UNDER METHYLATION MATRIX (i need it plot the average methylation profile unbinned)\n",
    "\n",
    "df_whole_matrix_unbinned= process_into_matrix(df_proc,gene=None,bin_size= 1)\n",
    "df_whole_matrix_unbinned.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILTERING AND KEEPING ONLY THE GENES THAT ARE SUITABLE (HAVE ENOUGH READS OVERLAPPING AND LONG ENOUGH)\n",
    "\n",
    "df_filtered, filtered_regions = filter_reads_per_gene_middle_bin_name(df_whole_matrix, middle_bin_name=None, min_reads =50, min_bins =40,require_middle_bin=True)\n",
    "print(f'THe shape of the filtered dataframe is of {df_filtered.shape}')\n",
    "print(f'There is {df_filtered.index.get_level_values(\"gene_tss\").nunique()} different genes in this dataframe')\n",
    "\n",
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GETTING THE FILTERED GENES NAMES AND SUB DF\n",
    "gene_names, gene_df = get_genes_list(df_filtered)\n",
    "gene_names_unbinned, gene_df_unbinned = get_genes_list(df_whole_matrix_unbinned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gene_names)\n",
    "print(gene_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SELECTNG A GENE OF THE LIST OF GENES (for i=19, SETD1A_30958295_30958295)\n",
    "gene= gene_names[7]\n",
    "df_gene= gene_df[7]\n",
    "\n",
    "gene_test= gene_names[19]\n",
    "df_gene_test= gene_df[19]\n",
    "\n",
    "df_gene_unbinned = gene_df_unbinned[19]\n",
    "\n",
    "\n",
    "print(gene)\n",
    "print(gene_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "a= df_proc[df_proc['gene_tss']== 'CDC45_19505053_19505053'].head()\n",
    "print(a['C_start'].min())\n",
    "print(a['C_end'].max())\n",
    "# df_gene.head(15)\n",
    "# df_gene.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=df_proc[df_proc['gene_tss']== 'SETD1A_30958295_30958295'].head()\n",
    "print(a['C_start'].min())\n",
    "print(a['C_end'].max())\n",
    "# df_gene.head(15)\n",
    "# df_gene.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLotting for a gene - no color, whole gene, all reads\n",
    "\n",
    "df_to_plot= preprocess_long_for_plot(df_proc)\n",
    "plot_reads_long(df_to_plot,filters={'gene_tss':gene}, facet_by=None, color_by=None, hex_colors=None)\n",
    "\n",
    "df_to_plot.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLotting for a gene - no color, whole gene, all reads\n",
    "\n",
    "plot_reads_long(df_to_plot,filters={'gene_tss':gene_test}, facet_by=None, color_by=None, hex_colors=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting for a gene per group - no. color, all reads, faceting by group\n",
    "plot_reads_long(df_to_plot,filters={'gene_tss':gene}, facet_by= 'group', color_by=None, hex_colors=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "##### Running the clustering on the actual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADAPTATIVE SWEEP (unused)\n",
    "# summary_best, best = sweep_k_for_target_fast(\n",
    "#     df=df_gene,                # reads × bins matrix\n",
    "#     k_list=[5, 10, 15, 20, 25, 30],\n",
    "#     target=6,\n",
    "#     transform='arcsine',       # or 'logit'\n",
    "#     metric='euclidean',\n",
    "#     scaling=False,\n",
    "#     pca_or_not=True,\n",
    "#     n_pcs=None,                # None => 95% variance retained\n",
    "#     kernel_type='laplacian',\n",
    "#     nan_threshold=0.7,\n",
    "#     nan_method='drop',\n",
    "#     res_init=1.0,\n",
    "#     res_min=0.05,\n",
    "#     res_max=2.0,\n",
    "#     up_factor=1.6,\n",
    "#     max_iter=10,\n",
    "#     silhouette_sample=None,    # e.g., 2000 to speed up on large N\n",
    "#     seed=42\n",
    "# )\n",
    "\n",
    "# # --- Use best solution ---\n",
    "# clusters = best['clusters']\n",
    "# chosen_k = best['k']\n",
    "# chosen_res = best['resolution']\n",
    "# X_pca = best['X_pca']\n",
    "# g = best['graph']\n",
    "\n",
    "# print(f\"Chosen k/res: {chosen_k} / {chosen_res:.3f} | n_clusters: {best['n_clusters']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "## testing if there are NaN in the distance matrices for different metrics\n",
    "# for metric in ['cosine', 'correlation', 'euclidean-logit', 'jsd']:\n",
    "#     D = pairwise_distance_profiles(X_new, metric)\n",
    "#     print(metric, np.isnan(D).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## COMPARING THE DIFFERENT DISTANCE METRICS FOR THE LEIDEN CLUSTERING ON A SAME PLOT\n",
    "# metrics_to_compare = [\n",
    "#     'cosine',\n",
    "#     'correlation',\n",
    "#     'euclidean-logit',\n",
    "#     'jsd'\n",
    "#     ]\n",
    "\n",
    "# plot_leiden_umap_comparison(\n",
    "#     X= X_new,\n",
    "#     hex_colors=hex_colors,\n",
    "#     metrics=metrics_to_compare,\n",
    "#     n_neighbors=15,\n",
    "#     resolution=0.8,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## PLOTING MULTIPLE UMAPS FOR ALL THE GENES IN THE DF_FILTERED\n",
    "# plot_multiple_umaps(df_filtered,gene_names,leiden_neighbors= 10, umap_neighbors=15, resolution= 0.8, min_dist=0.1, leiden_metric = 'cosine', umap_metric=\"cosine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #df_clean is the df without the reads that have a missing value\n",
    "# df_clean, X, partition, clusters = clustering(df_gene, n_neighbors=10, knn_metric='euclidean', leiden_resolution=0.8)\n",
    "# df_clean_2, X_2, partition_2, clusters_2 = clustering_improved(df_gene, n_neighbors=15, \n",
    "#                                                             #    n_pcs=5,\n",
    "#                                                                metric='euclidean', transform='logit',leiden_resolution=0.8, seed=42)\n",
    "\n",
    "# plotting_the_clustering(\n",
    "#     X, \n",
    "#     clusters, \n",
    "#     n_neighbors=15, \n",
    "#     min_dist=0.1, \n",
    "#     metric='euclidean')\n",
    "\n",
    "# plotting_the_clustering_2(\n",
    "#     X_2,                 # raw profiles or PCA scores\n",
    "#     clusters_2,          # Leiden cluster labels, shape (n_samples,)\n",
    "#     n_neighbors=15,\n",
    "#     min_dist=0.1,\n",
    "#     metric='euclidean',  # 'euclidean', 'cosine', 'correlation', 'jaccard', etc.\n",
    "#     transform=None,      # None | 'logit' | 'sqrt' (apply if X are raw proportions)\n",
    "#     n_pcs=None,          # e.g., 30 if you want PCA before UMAP; None = use X as-is\n",
    "#     seed=42\n",
    "# )\n",
    "\n",
    "# embedding, fig = plot_umap(X, clusters, n_neighbors=15, min_dist=0.1, metric='euclidean', n_pcs=None, standardize=False, seed=42, palette= hex_colors, title=\"UMAP - clustering with euclidean metric (impoved)\")\n",
    "# embedding_2, fig_2 = plot_umap(X_2, clusters_2, n_neighbors=15, min_dist=0.1, metric='euclidean', n_pcs=None, standardize=False, seed=42, title=\"UMAP - clustering with euclidean metric (impoved)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_PCA_euclidean, X_PCA_euclidean, partition_PCA_euclidean, clusters_PCA_euclidean,  = alternate_clustering(\n",
    "#     df_gene_test,\n",
    "#     apply_filter= True,\n",
    "#     min_bin_non_nan_frac = 0.5,\n",
    "#     min_read_non_nan_frac = 0.5,\n",
    "#     transform = 'logit',  # None | 'arcsine' | 'logit'\n",
    "#     min_overlap = 40,\n",
    "#     k = 10,\n",
    "#     positive_only = True,\n",
    "#     shrink_c = None,\n",
    "#     mutual=False,\n",
    "#     leiden_resolution= 1,\n",
    "#     seed= 42\n",
    "# )\n",
    "\n",
    "# n_clusters = len(set(clusters_PCA_euclidean)) \n",
    "# print(\"clusters:\", n_clusters)\n",
    "\n",
    "# # embedding_PCA_euclidean, fig_PCA_euclidean = plot_umap(\n",
    "# #     X_PCA_euclidean,\n",
    "# #     clusters_PCA_euclidean,\n",
    "# #     n_neighbors=25,\n",
    "# #     min_dist=0.1,\n",
    "# #     metric='euclidean',      # for Pipeline 2, use 'euclidean'\n",
    "# #     transform=None,          # None | 'logit' | 'arcsine' (use only if X are raw proportions)\n",
    "# #     n_pcs=None,              # set if X are raw features; None if X are already PCs\n",
    "# #     standardize=False,       # True if using raw features without PCA\n",
    "# #     seed=42,\n",
    "# #     palette=hex_colors,            # optional list/array or matplotlib colormap name\n",
    "# #     title=\"PCA + Euclidean (improved)\"\n",
    "# # )\n",
    "\n",
    "# read_dict_PCA_euclidean = dict_id_cluster_color(df_PCA_euclidean, clusters_PCA_euclidean, hex_colors)\n",
    "\n",
    "# df_dict_PCA_euclidean = dict_to_df(read_dict_PCA_euclidean)\n",
    "# df_test_PCA_euclidean= merge(df_proc, df_dict_PCA_euclidean)\n",
    "\n",
    "# # Long format for plotting\n",
    "# df_test_plot_PCA_euclidean = preprocess_long_for_plot(df_test_PCA_euclidean, include_locus_cluster=True)\n",
    "\n",
    "# # Plot\n",
    "# plot_reads_long(\n",
    "#     df_test_plot_PCA_euclidean,\n",
    "#     filters={\n",
    "#         # \"gene_tss\": gene, \n",
    "#         'group':['BT474_mV_high_Untreated',\n",
    "#         'BT474_mV_low_Untreated',\n",
    "#         'BT474_mV_Untreated_Unsorted']},\n",
    "#     # facet_by='group',\n",
    "#     color_by=\"locus_cluster\",\n",
    "#     hex_colors=hex_colors\n",
    "# )\n",
    "\n",
    "# df_only_interesting_groups_PCA_euclidean = df_PCA_euclidean[df_PCA_euclidean.index.get_level_values('group').isin([\n",
    "#     'BT474_mV_high_Untreated',\n",
    "#     'BT474_mV_low_Untreated',\n",
    "#     'BT474_mV_Untreated_Unsorted'\n",
    "# ])]\n",
    "\n",
    "# df_only_interesting_groups_PCA_euclidean = merge(df_only_interesting_groups_PCA_euclidean, df_dict_PCA_euclidean)\n",
    "# clusters_of_interest_PCA_euclidean = df_only_interesting_groups_PCA_euclidean['locus_cluster'].tolist() \n",
    "# summary(df_only_interesting_groups_PCA_euclidean, clusters_of_interest_PCA_euclidean)\n",
    "\n",
    "\n",
    "# start_PCA_euclidean, end_PCA_euclidean, center_coord_PCA_euclidean= start_end_center(df_test_plot_PCA_euclidean)\n",
    "\n",
    "# plot_avg_methylation_profile(\n",
    "#     df= df_PCA_euclidean,\n",
    "#     df_dict=df_dict_PCA_euclidean,\n",
    "#     start=start_PCA_euclidean,\n",
    "#     end=end_PCA_euclidean,\n",
    "#     center_coord=center_coord_PCA_euclidean,\n",
    "#     read_dict=read_dict_PCA_euclidean\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_PCA_euclidean, X_PCA_euclidean, partition_PCA_euclidean, clusters_PCA_euclidean, metrics_PCA_euclidean = clustering_final(\n",
    "#     df_gene,\n",
    "#     n_neighbors=15,\n",
    "#     nan_threshold=0.7,\n",
    "#     nan_method='drop',\n",
    "#     scaling=False,\n",
    "#     pca_or_not=True,\n",
    "#     n_pcs=30,           # None = no PCA; int for #PCs; float in (0,1) for variance ratio\n",
    "#     metric='euclidean',          # 'euclidean' or 'cosine' \n",
    "#     transform='logit',            # 'none', 'logit', or 'arcsine'\n",
    "#     kernel_type='laplacian',\n",
    "#     leiden_resolution=0.8,\n",
    "#     seed=42\n",
    "# )\n",
    "\n",
    "# embedding_PCA_euclidean, fig_PCA_euclidean = plot_umap(\n",
    "#     X_PCA_euclidean,\n",
    "#     clusters_PCA_euclidean,\n",
    "#     n_neighbors=25,\n",
    "#     min_dist=0.1,\n",
    "#     metric='euclidean',      # for Pipeline 2, use 'euclidean'\n",
    "#     transform=None,          # None | 'logit' | 'arcsine' (use only if X are raw proportions)\n",
    "#     n_pcs=None,              # set if X are raw features; None if X are already PCs\n",
    "#     standardize=False,       # True if using raw features without PCA\n",
    "#     seed=42,\n",
    "#     palette=hex_colors,            # optional list/array or matplotlib colormap name\n",
    "#     title=\"PCA + Euclidean (improved)\"\n",
    "# )\n",
    "\n",
    "# print(metrics_PCA_euclidean)\n",
    "\n",
    "# multi_sil_PCA_euclidean= silhouettes_multi(X_PCA_euclidean, clusters_PCA_euclidean, metric_main='euclidean', extra_metrics=['cosine', 'correlation', 'jaccard'])\n",
    "\n",
    "# print(f\"Silhouette (PCA + Euclidean): {metrics_PCA_euclidean['silhouette']:.3f}\")\n",
    "# print(f\"Silhouette (PCA + Euclidean) - Cosine: {multi_sil_PCA_euclidean['silhouette_cosine']:.3f}\")\n",
    "# print(f\"Silhouette (PCA + Euclidean) - Correlation: {multi_sil_PCA_euclidean['silhouette_correlation']:.3f}\")\n",
    "# print(f\"Silhouette (PCA + Euclidean) - Jaccard: {multi_sil_PCA_euclidean['silhouette_jaccard']:.3f}\")\n",
    "\n",
    "\n",
    "# read_dict_PCA_euclidean = dict_id_cluster_color(df_PCA_euclidean, clusters_PCA_euclidean, hex_colors)\n",
    "\n",
    "# df_dict_PCA_euclidean = dict_to_df(read_dict_PCA_euclidean)\n",
    "# df_test_PCA_euclidean= merge(df_proc, df_dict_PCA_euclidean)\n",
    "\n",
    "# # Long format for plotting\n",
    "# df_test_plot_PCA_euclidean = preprocess_long_for_plot(df_test_PCA_euclidean, include_locus_cluster=True)\n",
    "\n",
    "# # Plot\n",
    "# plot_reads_long(\n",
    "#     df_test_plot_PCA_euclidean,\n",
    "#     filters={\n",
    "#         # \"gene_tss\": gene, \n",
    "#         'group':['BT474_mV_high_Untreated',\n",
    "#         'BT474_mV_low_Untreated',\n",
    "#         'BT474_mV_Untreated_Unsorted']},\n",
    "#     # facet_by='group',\n",
    "#     color_by=\"locus_cluster\",\n",
    "#     hex_colors=hex_colors\n",
    "# )\n",
    "\n",
    "# df_only_interesting_groups_PCA_euclidean = df_PCA_euclidean[df_PCA_euclidean.index.get_level_values('group').isin([\n",
    "#     'BT474_mV_high_Untreated',\n",
    "#     'BT474_mV_low_Untreated',\n",
    "#     'BT474_mV_Untreated_Unsorted'\n",
    "# ])]\n",
    "\n",
    "# df_only_interesting_groups_PCA_euclidean = merge(df_only_interesting_groups_PCA_euclidean, df_dict_PCA_euclidean)\n",
    "# clusters_of_interest_PCA_euclidean = df_only_interesting_groups_PCA_euclidean['locus_cluster'].tolist() \n",
    "# summary(df_only_interesting_groups_PCA_euclidean, clusters_of_interest_PCA_euclidean)\n",
    "\n",
    "\n",
    "# start_PCA_euclidean, end_PCA_euclidean, center_coord_PCA_euclidean= start_end_center(df_test_plot_PCA_euclidean)\n",
    "\n",
    "# plot_avg_methylation_profile(\n",
    "#     df= df_PCA_euclidean,\n",
    "#     df_dict=df_dict_PCA_euclidean,\n",
    "#     start=start_PCA_euclidean,\n",
    "#     end=end_PCA_euclidean,\n",
    "#     center_coord=center_coord_PCA_euclidean,\n",
    "#     read_dict=read_dict_PCA_euclidean\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_PCA_euclidean, X_PCA_euclidean, partition_PCA_euclidean, clusters_PCA_euclidean, metrics_PCA_euclidean = clustering_final(\n",
    "        df_gene,\n",
    "        n_neighbors=15,\n",
    "        nan_threshold=0.7,\n",
    "        nan_method='drop',\n",
    "        scaling=False,\n",
    "        pca_or_not=True,\n",
    "        n_pcs=None,           # None = no PCA; int for #PCs; float in (0,1) for variance ratio\n",
    "        metric='euclidean',          # 'euclidean' or 'cosine'\n",
    "        transform='arcsine',            # 'none', 'logit', or 'arcsine'\n",
    "        kernel_type='laplacian',\n",
    "        leiden_resolution=0.8,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "print(clusters_PCA_euclidean)\n",
    "\n",
    "embedding_PCA_euclidean, fig_PCA_euclidean = plot_umap(\n",
    "        X_PCA_euclidean,\n",
    "        clusters_PCA_euclidean,\n",
    "        n_neighbors=25,\n",
    "        min_dist=0.1,\n",
    "        metric='euclidean',      # for Pipeline 2, use 'euclidean'\n",
    "        transform=None,          # None | 'logit' | 'arcsine' (use only if X are raw proportions)\n",
    "        n_pcs=None,              # set if X are raw features; None if X are already PCs\n",
    "        standardize=False,       # True if using raw features without PCA\n",
    "        seed=42,\n",
    "        palette=hex_colors,            # optional list/array or matplotlib colormap name\n",
    "        title=\"PCA + euclidean + laplacian\"\n",
    "    )\n",
    "\n",
    "\n",
    "df_PCA_euclidean.head()\n",
    "\n",
    "read_dict_PCA_euclidean = dict_id_cluster_color(df_PCA_euclidean, clusters_PCA_euclidean, hex_colors)\n",
    "\n",
    "df_dict_PCA_euclidean = dict_to_df(read_dict_PCA_euclidean)\n",
    "df_test_PCA_euclidean= merge(df_proc, df_dict_PCA_euclidean)\n",
    "\n",
    "    # Long format for plotting\n",
    "df_test_plot_PCA_euclidean = preprocess_long_for_plot(df_test_PCA_euclidean,\n",
    "                                                          include_locus_cluster=True,\n",
    "                                                          filter_outliers= True,\n",
    "                                                          max_span_bp=None,     #i'm letting the filtering be based on the span_quantile\n",
    "                                                          span_quantile=0.99,\n",
    "                                                          require_center_inside=True,\n",
    "                                                          min_cpg=1,\n",
    "                                                          cpg_window_bp=5000\n",
    "                                                          )\n",
    "\n",
    "    # Plot\n",
    "plot_reads_long(\n",
    "        df_test_plot_PCA_euclidean,\n",
    "        filters={\n",
    "            # \"gene_tss\": gene, \n",
    "            'group':['BT474_mV_high_Untreated',\n",
    "            'BT474_mV_low_Untreated',\n",
    "            'BT474_mV_Untreated_Unsorted']},\n",
    "        # facet_by='group',\n",
    "        color_by=\"locus_cluster\",\n",
    "        gene=gene,\n",
    "        hex_colors=hex_colors\n",
    "    )\n",
    "\n",
    "df_only_interesting_groups_PCA_euclidean = df_PCA_euclidean[df_PCA_euclidean.index.get_level_values('group').isin([\n",
    "        'BT474_mV_high_Untreated',\n",
    "        'BT474_mV_low_Untreated',\n",
    "        'BT474_mV_Untreated_Unsorted'\n",
    "    ])]\n",
    "\n",
    "df_only_interesting_groups_PCA_euclidean = merge(df_only_interesting_groups_PCA_euclidean, df_dict_PCA_euclidean)\n",
    "clusters_of_interest_PCA_euclidean = df_only_interesting_groups_PCA_euclidean['locus_cluster'].tolist() \n",
    "summary(df_only_interesting_groups_PCA_euclidean, clusters_of_interest_PCA_euclidean, gene=gene)\n",
    "\n",
    "\n",
    "# start_PCA_euclidean, end_PCA_euclidean, center_coord_PCA_euclidean= start_end_center(df_test_plot_PCA_euclidean)\n",
    "\n",
    "profiles_df_PCA_euclidean, coverage_df_PCA_euclidean, meta_df_PCA_euclidean, positions_PCA_euclidean= compute_gene_centroids(\n",
    "                                                                    df_PCA_euclidean,\n",
    "                                                                    df_dict_PCA_euclidean,\n",
    "                                                                    gene_tss=gene,\n",
    "                                                                   )\n",
    "\n",
    "plot_centroids_with_shading(\n",
    "    profiles_df_PCA_euclidean,\n",
    "    positions_PCA_euclidean,\n",
    "    meta_df_PCA_euclidean,\n",
    "    coverage_df_PCA_euclidean,\n",
    "    hex_colors,\n",
    "    smooth_sigma = 0,\n",
    "    title=gene + 'Average DNA Methylation Profiles per Cluster',\n",
    "    missingness_threshold=0.3,\n",
    ")\n",
    "\n",
    "# position_cols, meth_arrays = plot_avg_methylation_profile(\n",
    "#         df= df_PCA_euclidean,\n",
    "#         df_dict=df_dict_PCA_euclidean,\n",
    "#         start=start_PCA_euclidean,\n",
    "#         end=end_PCA_euclidean,\n",
    "#         center_coord=center_coord_PCA_euclidean,\n",
    "#         read_dict=read_dict_PCA_euclidean,\n",
    "#         gene= gene\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_PCA_euclidean, X_PCA_euclidean, partition_PCA_euclidean, clusters_PCA_euclidean, metrics_PCA_euclidean = clustering_final(\n",
    "        df_gene,\n",
    "        n_neighbors=15,\n",
    "        nan_threshold=0.7,\n",
    "        nan_method='drop',\n",
    "        scaling=False,\n",
    "        pca_or_not=True,\n",
    "        n_pcs=None,           # None = no PCA; int for #PCs; float in (0,1) for variance ratio\n",
    "        metric='euclidean',          # 'euclidean' or 'cosine'\n",
    "        transform='arcsine',            # 'none', 'logit', or 'arcsine'\n",
    "        kernel_type='laplacian',\n",
    "        leiden_resolution=0.8,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "print(clusters_PCA_euclidean)\n",
    "\n",
    "embedding_PCA_euclidean, fig_PCA_euclidean = plot_umap(\n",
    "        X_PCA_euclidean,\n",
    "        clusters_PCA_euclidean,\n",
    "        n_neighbors=25,\n",
    "        min_dist=0.1,\n",
    "        metric='euclidean',      # for Pipeline 2, use 'euclidean'\n",
    "        transform=None,          # None | 'logit' | 'arcsine' (use only if X are raw proportions)\n",
    "        n_pcs=None,              # set if X are raw features; None if X are already PCs\n",
    "        standardize=False,       # True if using raw features without PCA\n",
    "        seed=42,\n",
    "        palette=hex_colors,            # optional list/array or matplotlib colormap name\n",
    "        title=\"PCA + euclidean + laplacian\"\n",
    "    )\n",
    "\n",
    "\n",
    "df_PCA_euclidean.head()\n",
    "\n",
    "read_dict_PCA_euclidean = dict_id_cluster_color(df_PCA_euclidean, clusters_PCA_euclidean, hex_colors)\n",
    "\n",
    "df_dict_PCA_euclidean = dict_to_df(read_dict_PCA_euclidean)\n",
    "df_test_PCA_euclidean= merge(df_proc, df_dict_PCA_euclidean)\n",
    "\n",
    "    # Long format for plotting\n",
    "df_test_plot_PCA_euclidean = preprocess_long_for_plot(df_test_PCA_euclidean,\n",
    "                                                          include_locus_cluster=True,\n",
    "                                                          filter_outliers= True,\n",
    "                                                          max_span_bp=None,     #i'm letting the filtering be based on the span_quantile\n",
    "                                                          span_quantile=0.99,\n",
    "                                                          require_center_inside=True,\n",
    "                                                          min_cpg=1,\n",
    "                                                          cpg_window_bp=5000\n",
    "                                                          )\n",
    "\n",
    "    # Plot\n",
    "plot_reads_long(\n",
    "        df_test_plot_PCA_euclidean,\n",
    "        filters={\n",
    "            # \"gene_tss\": gene, \n",
    "            'group':['BT474_mV_high_Untreated',\n",
    "            'BT474_mV_low_Untreated',\n",
    "            'BT474_mV_Untreated_Unsorted']},\n",
    "        # facet_by='group',\n",
    "        color_by=\"locus_cluster\",\n",
    "        gene=gene,\n",
    "        hex_colors=hex_colors\n",
    "    )\n",
    "\n",
    "df_only_interesting_groups_PCA_euclidean = df_PCA_euclidean[df_PCA_euclidean.index.get_level_values('group').isin([\n",
    "        'BT474_mV_high_Untreated',\n",
    "        'BT474_mV_low_Untreated',\n",
    "        'BT474_mV_Untreated_Unsorted'\n",
    "    ])]\n",
    "\n",
    "df_only_interesting_groups_PCA_euclidean = merge(df_only_interesting_groups_PCA_euclidean, df_dict_PCA_euclidean)\n",
    "clusters_of_interest_PCA_euclidean = df_only_interesting_groups_PCA_euclidean['locus_cluster'].tolist() \n",
    "summary(df_only_interesting_groups_PCA_euclidean, clusters_of_interest_PCA_euclidean, gene=gene)\n",
    "\n",
    "\n",
    "start_PCA_euclidean, end_PCA_euclidean, center_coord_PCA_euclidean= start_end_center(df_test_plot_PCA_euclidean)\n",
    "\n",
    "# profiles_df_PCA_euclidean, coverage_df_PCA_euclidean, meta_df_PCA_euclidean, positions_PCA_euclidean= compute_gene_centroids(\n",
    "#                                                                     df_PCA_euclidean,\n",
    "#                                                                     df_dict_PCA_euclidean,\n",
    "#                                                                     gene_tss=gene,\n",
    "#                                                                    )\n",
    "\n",
    "# plot_centroids_with_shading(\n",
    "#     profiles_df_PCA_euclidean,\n",
    "#     positions_PCA_euclidean,\n",
    "#     meta_df_PCA_euclidean,\n",
    "#     coverage_df_PCA_euclidean,\n",
    "#     hex_colors,\n",
    "#     smooth_sigma = 0,\n",
    "#     title=gene + 'Average DNA Methylation Profiles per Cluster',\n",
    "#     missingness_threshold=0.3,\n",
    "# )\n",
    "\n",
    "position_cols, meth_arrays = plot_avg_methylation_profile(\n",
    "        df= df_PCA_euclidean,\n",
    "        df_dict=df_dict_PCA_euclidean,\n",
    "        start=start_PCA_euclidean,\n",
    "        end=end_PCA_euclidean,\n",
    "        center_coord=center_coord_PCA_euclidean,\n",
    "        read_dict=read_dict_PCA_euclidean,\n",
    "        gene= gene\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TESTING OUT THE CLUSTERING IMPROVED OLD ON PCA AND EUCLIDEAN \n",
    "\n",
    "# df_PCA_euclidean, X_PCA_euclidean, partition_PCA_euclidean, clusters_PCA_euclidean, = clustering_improved_old(\n",
    "#     df_gene,\n",
    "#     n_neighbors=15,\n",
    "#     nan_threshold=0.7,\n",
    "#     nan_method='drop',\n",
    "#     scaling=False,\n",
    "#     n_pcs=30,           # None = no PCA; int for #PCs; float in (0,1) for variance ratio\n",
    "#     metric='euclidean',          # 'euclidean' or 'cosine' \n",
    "#     transform='logit',            # 'none', 'logit', or 'arcsine'\n",
    "#     leiden_resolution=0.8,\n",
    "#     seed=42\n",
    "# )\n",
    "\n",
    "# embedding_PCA_euclidean, fig_PCA_euclidean = plot_umap(\n",
    "#     X_PCA_euclidean,\n",
    "#     clusters_PCA_euclidean,\n",
    "#     n_neighbors=25,\n",
    "#     min_dist=0.1,\n",
    "#     metric='euclidean',      # for Pipeline 2, use 'euclidean'\n",
    "#     transform=None,          # None | 'logit' | 'arcsine' (use only if X are raw proportions)\n",
    "#     n_pcs=None,              # set if X are raw features; None if X are already PCs\n",
    "#     standardize=False,       # True if using raw features without PCA\n",
    "#     seed=42,\n",
    "#     palette=hex_colors,            # optional list/array or matplotlib colormap name\n",
    "#     title=\"PCA + Euclidean (improved)\"\n",
    "# )\n",
    "\n",
    "# multi_sil_PCA_euclidean= silhouettes_multi(X_PCA_euclidean, clusters_PCA_euclidean, metric_main='euclidean', extra_metrics=['cosine', 'correlation', 'jaccard'])\n",
    "\n",
    "# print(f\"Silhouette (PCA + Euclidean): {metrics_PCA_euclidean['silhouette']:.3f}\")\n",
    "# print(f\"Silhouette (PCA + Euclidean) - Cosine: {multi_sil_PCA_euclidean['silhouette_cosine']:.3f}\")\n",
    "# print(f\"Silhouette (PCA + Euclidean) - Correlation: {multi_sil_PCA_euclidean['silhouette_correlation']:.3f}\")\n",
    "# print(f\"Silhouette (PCA + Euclidean) - Jaccard: {multi_sil_PCA_euclidean['silhouette_jaccard']:.3f}\")\n",
    "\n",
    "\n",
    "# read_dict_PCA_euclidean = dict_id_cluster_color(df_PCA_euclidean, clusters_PCA_euclidean, hex_colors)\n",
    "\n",
    "# df_dict_PCA_euclidean = dict_to_df(read_dict_PCA_euclidean)\n",
    "# df_test_PCA_euclidean= merge(df_proc, df_dict_PCA_euclidean)\n",
    "\n",
    "# # Long format for plotting\n",
    "# df_test_plot_PCA_euclidean = preprocess_long_for_plot(df_test_PCA_euclidean, include_locus_cluster=True)\n",
    "\n",
    "# # Plot\n",
    "# plot_reads_long(\n",
    "#     df_test_plot_PCA_euclidean,\n",
    "#     filters={\n",
    "#         # \"gene_tss\": gene, \n",
    "#         'group':['BT474_mV_high_Untreated',\n",
    "#         'BT474_mV_low_Untreated',\n",
    "#         'BT474_mV_Untreated_Unsorted']},\n",
    "#     # facet_by='group',\n",
    "#     color_by=\"locus_cluster\",\n",
    "#     hex_colors=hex_colors\n",
    "# )\n",
    "\n",
    "# df_only_interesting_groups_PCA_euclidean = df_PCA_euclidean[df_PCA_euclidean.index.get_level_values('group').isin([\n",
    "#     'BT474_mV_high_Untreated',\n",
    "#     'BT474_mV_low_Untreated',\n",
    "#     'BT474_mV_Untreated_Unsorted'\n",
    "# ])]\n",
    "\n",
    "# df_only_interesting_groups_PCA_euclidean = merge(df_only_interesting_groups_PCA_euclidean, df_dict_PCA_euclidean)\n",
    "# clusters_of_interest_PCA_euclidean = df_only_interesting_groups_PCA_euclidean['locus_cluster'].tolist() \n",
    "# summary(df_only_interesting_groups_PCA_euclidean, clusters_of_interest_PCA_euclidean)\n",
    "\n",
    "\n",
    "# start_PCA_euclidean, end_PCA_euclidean, center_coord_PCA_euclidean= start_end_center(df_test_plot_PCA_euclidean)\n",
    "\n",
    "# plot_avg_methylation_profile(\n",
    "#     df= df_PCA_euclidean,\n",
    "#     df_dict=df_dict_PCA_euclidean,\n",
    "#     start=start_PCA_euclidean,\n",
    "#     end=end_PCA_euclidean,\n",
    "#     center_coord=center_coord_PCA_euclidean,\n",
    "#     read_dict=read_dict_PCA_euclidean\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_PCA_euclidean, X_PCA_euclidean_unbinned, partition_PCA_euclidean_unbinned, clusters_PCA_euclidean_unbinned, metrics_PCA_euclidean_unbinned = clustering_final(\n",
    "        df_gene,\n",
    "        n_neighbors=9,\n",
    "        nan_threshold=0.1,\n",
    "        nan_method='drop',\n",
    "        scaling=False,\n",
    "        pca_or_not=True,\n",
    "        n_pcs=None,           # None = no PCA; int for #PCs; float in (0,1) for variance ratio\n",
    "        metric='euclidean',          # 'euclidean' or 'cosine'\n",
    "        transform='arcsine',            # 'none', 'logit', or 'arcsine'\n",
    "        kernel_type='laplacian',\n",
    "        leiden_resolution=0.8,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "read_dict_PCA_euclidean_unbinned = dict_id_cluster_color(df_gene_unbinned, clusters_PCA_euclidean_unbinned, hex_colors)\n",
    "\n",
    "df_dict_PCA_euclidean_unbinned = dict_to_df(read_dict_PCA_euclidean_unbinned)\n",
    "df_test_PCA_euclidean_unbinned= merge(df_proc, df_dict_PCA_euclidean_unbinned)\n",
    "\n",
    "df_test_plot_PCA_euclidean_unbinned = preprocess_long_for_plot(df_test_PCA_euclidean_unbinned,\n",
    "                                                          include_locus_cluster=True,\n",
    "                                                          filter_outliers= True,\n",
    "                                                          max_span_bp=None,     #i'm letting the filtering be based on the span_quantile\n",
    "                                                          span_quantile=0.99,\n",
    "                                                          require_center_inside=True,\n",
    "                                                          min_cpg=1,\n",
    "                                                          cpg_window_bp=5000\n",
    "                                                          )\n",
    "\n",
    "profiles_df_unbinned, coverage_df_unbinned_euclidean, meta_df_unbinned_euclidean, positions_unbinned_euclidean= compute_gene_centroids(\n",
    "                                                                    df_gene_unbinned,\n",
    "                                                                    df_dict_PCA_euclidean_unbinned,\n",
    "                                                                    gene_tss='SETD1A_30958295_30958295',\n",
    "                                                                   )\n",
    "\n",
    "plot_centroids_with_shading(\n",
    "    profiles_df_unbinned,\n",
    "    positions_unbinned_euclidean,\n",
    "    meta_df_unbinned_euclidean,\n",
    "    coverage_df_unbinned_euclidean,\n",
    "    hex_colors,\n",
    "    smooth_sigma = 0,\n",
    "    title=gene +'Average DNA Methylation Profiles per Cluster',\n",
    "    missingness_threshold=0.5,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_corr, g_corr, df_corr = alternate_clustering(\n",
    "    df_gene,\n",
    "    apply_filter = False,\n",
    "    min_bin_non_nan_frac = 0.7,\n",
    "    min_read_non_nan_frac = 1,\n",
    "    transform = None,  # None | 'arcsine' | 'logit'\n",
    "    min_overlap = 40,\n",
    "    k = 40,\n",
    "    positive_only = True,\n",
    "    shrink_c = 0.0,\n",
    "    mutual = True,\n",
    "    leiden_resolution= 0.6 ,\n",
    "    seed = 42\n",
    ")\n",
    "\n",
    "# embedding_corr, fig_corr = plot_umap(\n",
    "#     X_cosine,\n",
    "#     clusters_cosine,\n",
    "#     n_neighbors=25,\n",
    "#     min_dist=0.1,\n",
    "#     metric='cosine',      # for Pipeline 2, use 'euclidean'\n",
    "#     transform=None,          # None | 'logit' | 'arcsine' (use only if X are raw proportions)\n",
    "#     n_pcs=None,              # set if X are raw features; None if X are already PCs\n",
    "#     standardize=False,       # True if using raw features without PCA\n",
    "#     seed=42,\n",
    "#     palette=hex_colors,            # optional list/array or matplotlib colormap name\n",
    "#     title=\"Cosine (improved)\"\n",
    "# )\n",
    "\n",
    "# multi_sil_cosine= silhouettes_multi(X_cosine, clusters_cosine, metric_main='cosine', extra_metrics=['euclidean', 'correlation', 'jaccard'])\n",
    "\n",
    "# print(f\"Silhouette (Cosine): {metrics_cosine['silhouette']:.3f}\")\n",
    "# print(f\"Silhouette (Cosine) - Euclidean: {multi_sil_cosine['silhouette_cosine']:.3f}\")\n",
    "# print(f\"Silhouette (Cosine) - Correlation: {multi_sil_cosine['silhouette_correlation']:.3f}\")\n",
    "# print(f\"Silhouette (Cosine) - Jaccard: {multi_sil_cosine['silhouette_jaccard']:.3f}\")\n",
    "\n",
    "\n",
    "read_dict_corr = dict_id_cluster_color(df_corr, clusters_corr, hex_colors)\n",
    "\n",
    "df_dict_corr = dict_to_df(read_dict_corr)\n",
    "df_test_corr= merge(df_proc, df_dict_corr)\n",
    "\n",
    "# Long format for plotting\n",
    "df_test_plot_corr = preprocess_long_for_plot(df_test_corr, include_locus_cluster=True)\n",
    "\n",
    "# Plot\n",
    "plot_reads_long(\n",
    "    df_test_plot_corr,\n",
    "    filters={\n",
    "        # \"gene_tss\": gene, \n",
    "        'group':['BT474_mV_high_Untreated',\n",
    "        'BT474_mV_low_Untreated',\n",
    "        'BT474_mV_Untreated_Unsorted']},\n",
    "    facet_by='group',\n",
    "    color_by=\"locus_cluster\",\n",
    "    hex_colors=hex_colors\n",
    ")\n",
    "\n",
    "df_only_interesting_groups_corr = df_corr[df_corr.index.get_level_values('group').isin([\n",
    "    'BT474_mV_high_Untreated',\n",
    "    'BT474_mV_low_Untreated',\n",
    "    'BT474_mV_Untreated_Unsorted'\n",
    "])]\n",
    "\n",
    "df_only_interesting_groups_corr = merge(df_only_interesting_groups_corr, df_dict_corr)\n",
    "clusters_of_interest_corr = df_only_interesting_groups_corr['locus_cluster'].tolist() \n",
    "summary(df_only_interesting_groups_corr, clusters_of_interest_corr)\n",
    "\n",
    "\n",
    "start_corr, end_corr, center_coord_corr= start_end_center(df_test_plot_corr)\n",
    "\n",
    "plot_avg_methylation_profile(\n",
    "    df= df_corr,\n",
    "    df_dict=df_dict_corr,\n",
    "    start=start_corr,\n",
    "    end=end_corr,\n",
    "    center_coord=center_coord_corr,\n",
    "    read_dict=read_dict_corr\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(gene_df)):\n",
    "    gene = gene_names[i]\n",
    "    df_gene = gene_df[i]\n",
    "\n",
    "    print(gene)\n",
    "\n",
    "    df_PCA_euclidean, X_PCA_euclidean, partition_PCA_euclidean, clusters_PCA_euclidean, metrics_PCA_euclidean = clustering_final(\n",
    "        df_gene,\n",
    "        n_neighbors=15,\n",
    "        nan_threshold=0.7,\n",
    "        nan_method='drop',\n",
    "        scaling=False,\n",
    "        pca_or_not=True,\n",
    "        n_pcs=None,           # None = no PCA; int for #PCs; float in (0,1) for variance ratio\n",
    "        metric='euclidean',          # 'euclidean' or 'cosine' \n",
    "        transform='arcsine',            # 'none', 'logit', or 'arcsine'\n",
    "        kernel_type='laplacian',\n",
    "        leiden_resolution=0.8,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    embedding_PCA_euclidean, fig_PCA_euclidean = plot_umap(\n",
    "        X_PCA_euclidean,\n",
    "        clusters_PCA_euclidean,\n",
    "        n_neighbors=25,\n",
    "        min_dist=0.1,\n",
    "        metric='euclidean',      \n",
    "        transform=None,          # None | 'logit' | 'arcsine' (use only if X are raw proportions)\n",
    "        n_pcs=None,              # set if X are raw features; None if X are already PCs\n",
    "        standardize=False,       # True if using raw features without PCA\n",
    "        seed=42,\n",
    "        palette=hex_colors,            # optional list/array or matplotlib colormap name\n",
    "        title=\"PCA + Euclidean + logit + Laplacian\",\n",
    "        gene=gene\n",
    "    )\n",
    "\n",
    "    print(metrics_PCA_euclidean)\n",
    "\n",
    "    multi_sil_PCA_euclidean= silhouettes_multi(X_PCA_euclidean, clusters_PCA_euclidean, metric_main='euclidean', extra_metrics=['cosine', 'correlation', 'jaccard'])\n",
    "\n",
    "    print(f\"Silhouette (PCA + Euclidean): {metrics_PCA_euclidean['silhouette']:.3f}\")\n",
    "    print(f\"Silhouette (PCA + Euclidean) - Cosine: {multi_sil_PCA_euclidean['silhouette_cosine']:.3f}\")\n",
    "    print(f\"Silhouette (PCA + Euclidean) - Correlation: {multi_sil_PCA_euclidean['silhouette_correlation']:.3f}\")\n",
    "    print(f\"Silhouette (PCA + Euclidean) - Jaccard: {multi_sil_PCA_euclidean['silhouette_jaccard']:.3f}\")\n",
    "\n",
    "\n",
    "    read_dict_PCA_euclidean = dict_id_cluster_color(df_PCA_euclidean, clusters_PCA_euclidean, hex_colors)\n",
    "\n",
    "    df_dict_PCA_euclidean = dict_to_df(read_dict_PCA_euclidean)\n",
    "    df_test_PCA_euclidean= merge(df_proc, df_dict_PCA_euclidean)\n",
    "\n",
    "    # Long format for plotting\n",
    "    df_test_plot_PCA_euclidean = preprocess_long_for_plot(df_test_PCA_euclidean,\n",
    "                                                          include_locus_cluster=True,\n",
    "                                                          filter_outliers= True,\n",
    "                                                          max_span_bp=None,     #i'm letting the filtering be based on the span_quantile\n",
    "                                                          span_quantile=0.99,\n",
    "                                                          require_center_inside=True,\n",
    "                                                          min_cpg=1,\n",
    "                                                          cpg_window_bp=5000\n",
    "                                                          )\n",
    "\n",
    "    # Plot\n",
    "    plot_reads_long(\n",
    "        df_test_plot_PCA_euclidean,\n",
    "        filters={\n",
    "            # \"gene_tss\": gene, \n",
    "            'group':['BT474_mV_high_Untreated',\n",
    "            'BT474_mV_low_Untreated',\n",
    "            'BT474_mV_Untreated_Unsorted']},\n",
    "        # facet_by='group',\n",
    "        color_by=\"locus_cluster\",\n",
    "        gene=gene,\n",
    "        hex_colors=hex_colors\n",
    "    )\n",
    "\n",
    "    df_only_interesting_groups_PCA_euclidean = df_PCA_euclidean[df_PCA_euclidean.index.get_level_values('group').isin([\n",
    "        'BT474_mV_high_Untreated',\n",
    "        'BT474_mV_low_Untreated',\n",
    "        'BT474_mV_Untreated_Unsorted'\n",
    "    ])]\n",
    "\n",
    "    df_only_interesting_groups_PCA_euclidean = merge(df_only_interesting_groups_PCA_euclidean, df_dict_PCA_euclidean)\n",
    "    clusters_of_interest_PCA_euclidean = df_only_interesting_groups_PCA_euclidean['locus_cluster'].tolist() \n",
    "    summary(df_only_interesting_groups_PCA_euclidean, clusters_of_interest_PCA_euclidean, gene=gene)\n",
    "\n",
    "    profiles_df_PCA_euclidean, coverage_df_PCA_euclidean, meta_df_PCA_euclidean, positions_PCA_euclidean= compute_gene_centroids(\n",
    "                                                                        df_PCA_euclidean,\n",
    "                                                                        df_dict_PCA_euclidean,\n",
    "                                                                        gene_tss=gene,\n",
    "                                                                    )\n",
    "\n",
    "    plot_centroids_with_shading(\n",
    "        profiles_df_PCA_euclidean,\n",
    "        positions_PCA_euclidean,\n",
    "        meta_df_PCA_euclidean,\n",
    "        coverage_df_PCA_euclidean,\n",
    "        hex_colors,\n",
    "        smooth_sigma = 0,\n",
    "        title= gene +':Average DNA Methylation Profiles per Cluster - EUCLIDEAN',\n",
    "        missingness_threshold=0.5,\n",
    "    )\n",
    "\n",
    "\n",
    "    # start_PCA_euclidean, end_PCA_euclidean, center_coord_PCA_euclidean= start_end_center(df_test_plot_PCA_euclidean)\n",
    "\n",
    "    # plot_avg_methylation_profile(\n",
    "    #     df= df_PCA_euclidean,\n",
    "    #     df_dict=df_dict_PCA_euclidean,\n",
    "    #     start=start_PCA_euclidean,\n",
    "    #     end=end_PCA_euclidean,\n",
    "    #     center_coord=center_coord_PCA_euclidean,\n",
    "    #     read_dict=read_dict_PCA_euclidean,\n",
    "    #     gene= gene\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(gene_df)):\n",
    "    gene = gene_names[i]\n",
    "    df_gene = gene_df[i]\n",
    "\n",
    "    print(gene)\n",
    "    df_cosine, X_cosine, partition_cosine, clusters_cosine, metrics_cosine = clustering_final(\n",
    "        df_gene,\n",
    "        n_neighbors=15,\n",
    "        nan_threshold=0.7,\n",
    "        nan_method='drop',\n",
    "        scaling=False,\n",
    "        pca_or_not=False,\n",
    "        n_pcs=None,           # None = no PCA; int for #PCs; float in (0,1) for variance ratio\n",
    "        metric='cosine',          # 'euclidean' or 'cosine'\n",
    "        transform='arcsine',            # 'none', 'logit', or 'arcsine'\n",
    "        kernel_type='laplacian',\n",
    "        leiden_resolution=0.8,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    embedding_cosine, fig_cosine = plot_umap(\n",
    "        X_cosine,\n",
    "        clusters_cosine,\n",
    "        n_neighbors=25,\n",
    "        min_dist=0.1,\n",
    "        metric='cosine',      # for Pipeline 2, use 'euclidean'\n",
    "        transform=None,          # None | 'logit' | 'arcsine' (use only if X are raw proportions)\n",
    "        n_pcs=None,              # set if X are raw features; None if X are already PCs\n",
    "        standardize=False,       # True if using raw features without PCA\n",
    "        seed=42,\n",
    "        palette=hex_colors,            # optional list/array or matplotlib colormap name\n",
    "        title=\"NO PCA + cosine + laplacian\",\n",
    "        gene=gene\n",
    "    )\n",
    "\n",
    "    print(metrics_cosine)\n",
    "\n",
    "    multi_sil_cosine= silhouettes_multi(X_cosine, clusters_cosine, metric_main='cosine', extra_metrics=['euclidean', 'correlation', 'jaccard'])\n",
    "\n",
    "    print(f\"Silhouette (Cosine): {metrics_cosine['silhouette']:.3f}\")\n",
    "    print(f\"Silhouette (Cosine) - Euclidean: {multi_sil_cosine['silhouette_euclidean']:.3f}\")\n",
    "    print(f\"Silhouette (Cosine) - Correlation: {multi_sil_cosine['silhouette_correlation']:.3f}\")\n",
    "    print(f\"Silhouette (Cosine) - Jaccard: {multi_sil_cosine['silhouette_jaccard']:.3f}\")\n",
    "\n",
    "\n",
    "    read_dict_cosine = dict_id_cluster_color(df_cosine, clusters_cosine, hex_colors)\n",
    "\n",
    "    df_dict_cosine = dict_to_df(read_dict_cosine)\n",
    "    df_test_cosine= merge(df_proc, df_dict_cosine)\n",
    "\n",
    "    # Long format for plotting\n",
    "    df_test_plot_cosine = preprocess_long_for_plot(df_test_cosine,\n",
    "                                                    include_locus_cluster=True,\n",
    "                                                    filter_outliers= True,\n",
    "                                                    max_span_bp=None,     #i'm letting the filtering be based on the span_quantile\n",
    "                                                    span_quantile=0.99,\n",
    "                                                    require_center_inside=True,\n",
    "                                                    min_cpg=1,\n",
    "                                                    cpg_window_bp=5000\n",
    "                                                    )\n",
    "\n",
    "    # Plot\n",
    "    plot_reads_long(\n",
    "        df_test_plot_cosine,\n",
    "        filters={\n",
    "            # \"gene_tss\": gene, \n",
    "            'group':['BT474_mV_high_Untreated',\n",
    "            'BT474_mV_low_Untreated',\n",
    "            'BT474_mV_Untreated_Unsorted']},\n",
    "        # facet_by='group',\n",
    "        color_by=\"locus_cluster\",\n",
    "        gene= gene,\n",
    "        hex_colors=hex_colors\n",
    "    )\n",
    "\n",
    "    df_only_interesting_groups_cosine = df_cosine[df_cosine.index.get_level_values('group').isin([\n",
    "        'BT474_mV_high_Untreated',\n",
    "        'BT474_mV_low_Untreated',\n",
    "        'BT474_mV_Untreated_Unsorted'\n",
    "    ])]\n",
    "\n",
    "    df_only_interesting_groups_cosine = merge(df_only_interesting_groups_cosine, df_dict_cosine)\n",
    "    clusters_of_interest_cosine = df_only_interesting_groups_cosine['locus_cluster'].tolist() \n",
    "    summary(df_only_interesting_groups_cosine, clusters_of_interest_cosine)\n",
    "\n",
    "    profiles_df_cosine, coverage_df_cosine, meta_df_cosine, positions_cosine = compute_gene_centroids(\n",
    "                                                                        df_cosine,\n",
    "                                                                        df_dict_cosine,\n",
    "                                                                        gene_tss=gene,\n",
    "                                                                    )\n",
    "\n",
    "    plot_centroids_with_shading(\n",
    "        profiles_df_cosine,\n",
    "        positions_cosine,\n",
    "        meta_df_cosine,\n",
    "        coverage_df_cosine,\n",
    "        hex_colors,\n",
    "        smooth_sigma = 0,\n",
    "        title= gene + ' : Average DNA Methylation Profiles per Cluster - COSINE ',\n",
    "        missingness_threshold=0.5,\n",
    "    )\n",
    "\n",
    "#     start_cosine, end_cosine, center_coord_cosine= start_end_center(df_test_plot_cosine)\n",
    "\n",
    "#     plot_avg_methylation_profile(\n",
    "#     df= df_cosine,\n",
    "#     df_dict=df_dict_cosine,\n",
    "#     start=start_cosine,\n",
    "#     end=end_cosine,\n",
    "#     center_coord=center_coord_cosine,\n",
    "#     read_dict=read_dict_cosine,\n",
    "#     gene=gene\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cosine, X_cosine, partition_cosine, clusters_cosine, metrics_cosine = clustering_final(\n",
    "        df_gene,\n",
    "        n_neighbors=15,\n",
    "        nan_threshold=0.7,\n",
    "        nan_method='drop',\n",
    "        scaling=False,\n",
    "        pca_or_not=False,\n",
    "        n_pcs=None,           # None = no PCA; int for #PCs; float in (0,1) for variance ratio\n",
    "        metric='cosine',          # 'euclidean' or 'cosine'\n",
    "        transform='arcsine',            # 'none', 'logit', or 'arcsine'\n",
    "        kernel_type='laplacian',\n",
    "        leiden_resolution=0.8,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "embedding_cosine, fig_cosine = plot_umap(\n",
    "        X_cosine,\n",
    "        clusters_cosine,\n",
    "        n_neighbors=25,\n",
    "        min_dist=0.1,\n",
    "        metric='cosine',      # for Pipeline 2, use 'euclidean'\n",
    "        transform=None,          # None | 'logit' | 'arcsine' (use only if X are raw proportions)\n",
    "        n_pcs=None,              # set if X are raw features; None if X are already PCs\n",
    "        standardize=False,       # True if using raw features without PCA\n",
    "        seed=42,\n",
    "        palette=hex_colors,            # optional list/array or matplotlib colormap name\n",
    "        title=\"NO PCA + cosine + laplacian\",\n",
    "        gene=gene\n",
    "    )\n",
    "\n",
    "print(metrics_cosine)\n",
    "\n",
    "multi_sil_cosine= silhouettes_multi(X_cosine, clusters_cosine, metric_main='cosine', extra_metrics=['euclidean', 'correlation', 'jaccard'])\n",
    "\n",
    "print(f\"Silhouette (Cosine): {metrics_cosine['silhouette']:.3f}\")\n",
    "print(f\"Silhouette (Cosine) - Euclidean: {multi_sil_cosine['silhouette_euclidean']:.3f}\")\n",
    "print(f\"Silhouette (Cosine) - Correlation: {multi_sil_cosine['silhouette_correlation']:.3f}\")\n",
    "print(f\"Silhouette (Cosine) - Jaccard: {multi_sil_cosine['silhouette_jaccard']:.3f}\")\n",
    "\n",
    "\n",
    "read_dict_cosine = dict_id_cluster_color(df_cosine, clusters_cosine, hex_colors)\n",
    "\n",
    "df_dict_cosine = dict_to_df(read_dict_cosine)\n",
    "df_test_cosine= merge(df_proc, df_dict_cosine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gene.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cosine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict_cosine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles_df_u, coverage_df_u, meta_df_u, positions_u = compute_gene_centroids(\n",
    "    df_reads=df_gene_unbinned,        # reads × bins methylation matrix\n",
    "    df_map=df_dict_cosine,        # mapping of readid → cluster + color\n",
    "    gene_tss= 'SETD1A_30958295_30958295',\n",
    "    extra_meta_cols=['gene_tss', 'group', 'cluster']  # add other non-bin columns here if present\n",
    ")\n",
    "\n",
    "\n",
    "plot_centroids_with_shading(\n",
    "    profiles_df_u,\n",
    "    positions_u,\n",
    "    meta_df_u,\n",
    "    coverage_df_u,\n",
    "    hex_colors,\n",
    "    smooth_sigma = 0,\n",
    "    title='Average DNA Methylation Profiles per Cluster',\n",
    "    missingness_threshold=0.5,\n",
    ")\n",
    "meta_df_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles_df, coverage_df, meta_df, positions = compute_gene_centroids(\n",
    "    df_reads=df_cosine,        # reads × bins methylation matrix\n",
    "    df_map=df_dict_cosine,        # mapping of readid → cluster + color\n",
    "    gene_tss= 'SETD1A_30958295_30958295',\n",
    "    extra_meta_cols=['gene_tss', 'group', 'cluster']  # add other non-bin columns here if present\n",
    ")\n",
    "\n",
    "\n",
    "plot_centroids_with_shading(\n",
    "    profiles_df,\n",
    "    positions,\n",
    "    meta_df,\n",
    "    coverage_df,\n",
    "    hex_colors,\n",
    "    smooth_sigma = 0,\n",
    "    title='Average DNA Methylation Profiles per Cluster',\n",
    "    missingness_threshold=0.5,\n",
    ")\n",
    "meta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_bulk has columns: ['cluster','C_pos','meth','cov'] for the whole dataset\n",
    "\n",
    "P_df, coverage_bulk_df, meta_bulk_df, positions_bulk = compute_bulk_centroids(df_bulk, use_cov_for_proportion=True)\n",
    "\n",
    "fig, axes = plot_centroids_with_shading(\n",
    "    P_df=P_df,\n",
    "    positions=positions_bulk,\n",
    "    meta_df=meta_bulk_df,\n",
    "    hex_colors=hex_colors,\n",
    "    title=\"Average DNA Methylation Profiles per Cluster (Bulk)\"\n",
    ")\n",
    "\n",
    "P_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_bulk_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_bulk has columns: ['cluster','C_pos','meth','cov'] for the whole dataset\n",
    "\n",
    "fig, axes = plot_centroids_with_shading(\n",
    "    P_df=profiles_df_bulk_binned,\n",
    "    positions=positions_bulk_binned,\n",
    "    meta_df=meta_df_bulk_binned, #attention ici, j'utilise pas le meta donné par la fonction compute_gene_centroid\n",
    "    hex_colors=hex_colors,\n",
    "    title=\"Average DNA Methylation Profiles per Cluster (Bulk) (Binned)\"\n",
    ")\n",
    "\n",
    "profiles_df_bulk_binned.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "P_array = profiles_df_bulk_binned.to_numpy(float)\n",
    "\n",
    "# Optional per-bin weights for cosine\n",
    "weights = coverage_df.sum(axis=0).to_numpy(float) / (coverage_df.sum(axis=0).mean() + 1e-9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "assign_df, S, D = assign_gene_clusters_to_consensus(\n",
    "    G=profiles_df,               # DataFrame (rows = gene clusters, cols = common positions)\n",
    "    meta=meta_df,         # aligned to G rows\n",
    "    P=profiles_df_bulk_binned,               # DataFrame (rows = bulk clusters, cols = common positions)\n",
    "    method='cosine',\n",
    "    center_rows=True,\n",
    "    # weights=weights,  # optional pd.Series indexed by common positions\n",
    "    min_overlap=40,\n",
    "    min_similarity=0.6,\n",
    "    allow_unassigned=True,\n",
    "    capacity_per_type=1,\n",
    ")\n",
    "\n",
    "assign_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build G and meta for assignment\n",
    "# G = profiles_df_unbinned\n",
    "\n",
    "meta_for_assign = meta_df[['gene_tss', 'cluster_id', 'n_reads']].rename(columns={'n_reads': 'size'})\n",
    "\n",
    "# # Optional cosine weights from coverage\n",
    "# coverage_per_bin = coverage_df.sum(axis=0).to_numpy(float)\n",
    "# weights = coverage_per_bin / (coverage_per_bin.mean() + 1e-9)\n",
    "\n",
    "# print(G.shape) # (n_gene_clusters, n_bins_G)\n",
    "# print(P_array.shape) # (n_prototypes, n_bins_P)\n",
    "# print(weights.shape) # (n_bins_weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assign to consensus prototypes\n",
    "\n",
    "# assign_df, S, D = assign_gene_clusters_to_consensus(\n",
    "#     G=G,                   # rows = clusters from some gene, columns must match positions\n",
    "#     meta=meta_for_assign,    # DataFrame with ['gene_id','cluster_id','size']\n",
    "#     P=P_df,\n",
    "#     method='cosine',       # or 'pearson'\n",
    "#     center_rows=False,\n",
    "#     weights=weights,\n",
    "#     min_similarity=0.4,\n",
    "#     allow_unassigned=True,\n",
    "#     capacity_per_type=1\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2) Heatmap for one gene\n",
    "fig, ax = plot_gene_mapping_heatmap(\n",
    "    assign_df=assign_df,\n",
    "    S=S,\n",
    "    meta=meta_for_assign,\n",
    "    P=profiles_df_bulk_binned,\n",
    "    gene_id='SETD1A_30958295_30958295',\n",
    "    sort_rows=True\n",
    ")\n",
    "\n",
    "# 3) Assignment summary across genes\n",
    "fig2, ax2 = plot_assignment_summary(assign_df)\n",
    "\n",
    "# 4) Recolor meta for plotting centroids\n",
    "bulk_color_map = {cons_id: '#hexcolor' for cons_id in P_df.index}  # build from bulk meta\n",
    "meta_colored = recolor_meta_by_bulk(assign_df, meta_df_u, bulk_color_map, gene_id=gene)\n",
    "\n",
    "# 5) Assignment confidence\n",
    "# fig3, ax3 = plot_assignment_confidence(assign_df, gene_id=gene)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pipeline configurations\n",
    "configs = [\n",
    "    # --- PCA + Euclidean (no scaling) ---\n",
    "    dict(\n",
    "        name='pca_euclidean',\n",
    "        n_neighbors=15,\n",
    "        nan_threshold=0.7,\n",
    "        nan_method='drop',\n",
    "        scaling=False,\n",
    "        pca_or_not=True,\n",
    "        n_pcs=None,  # keep 95% variance\n",
    "        metric='euclidean',\n",
    "        transform='logit',  # can also use 'arcsine' or 'none'\n",
    "        kernel_type='laplacian',\n",
    "        leiden_resolution=1.0,\n",
    "        seed=42\n",
    "    ),\n",
    "\n",
    "    # --- No PCA + Cosine (shape-based) ---\n",
    "    dict(\n",
    "        name='cosine_no_pca',\n",
    "        n_neighbors=15,\n",
    "        nan_threshold=0.7,\n",
    "        nan_method='drop',\n",
    "        scaling=False,  # keep raw 0..1; cosine similarity handles scale\n",
    "        pca_or_not=False,\n",
    "        n_pcs=None,\n",
    "        metric='cosine',\n",
    "        transform='none',  # same transform for fairness\n",
    "        kernel_type='laplacian',\n",
    "        leiden_resolution=1.0,\n",
    "        seed=42\n",
    "    )\n",
    "]\n",
    "\n",
    "# Example input dictionary of gene-level dataframes\n",
    "# e.g. genes_dict = {'GENE1': df_gene1, 'GENE2': df_gene2, ...}\n",
    "\n",
    "# Run the clustering pipelines\n",
    "results_df, outputs = run_pipelines_on_genes(gene_names, gene_df, configs)\n",
    "\n",
    "print(results_df['pipeline'].value_counts())\n",
    "\n",
    "for m in ['silhouette','calinski_harabasz','davies_bouldin','leiden_quality','modularity','n_clusters']: \n",
    "    if m in results_df.columns:\n",
    "        print(m, results_df.pivot_table(index='gene', columns='pipeline', values=m, aggfunc='first').isna().all())\n",
    "# Plot violin + scatter comparison of clustering metrics\n",
    "# figs = plot_violin_scatter(results_df)\n",
    "\n",
    "results_df.head(20)\n",
    "\n",
    "fig, axes = plot_compare_pipelines_grid(results_df,\n",
    "                                        pipelines_order=('pca_euclidean', 'cosine_no_pca'),\n",
    "                                        metrics=('silhouette','calinski_harabasz','davies_bouldin','leiden_quality','modularity','n_clusters'),\n",
    "                                        kind='violin', # or 'box' \n",
    "                                        show_points=True,\n",
    "                                        connect_pairs=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered, X_filtered, partition_filtered, clusters_filtered, metrics_filtered = clustering_final(\n",
    "        df_filtered,\n",
    "        n_neighbors=15,\n",
    "        nan_threshold=0.7,\n",
    "        nan_method='drop',\n",
    "        scaling=False,\n",
    "        pca_or_not=True,\n",
    "        n_pcs=None,           # None = no PCA; int for #PCs; float in (0,1) for variance ratio\n",
    "        metric='euclidean',          # 'euclidean' or 'cosine' \n",
    "        transform='arcsine',            # 'none', 'logit', or 'arcsine'\n",
    "        kernel_type='laplacian',\n",
    "        leiden_resolution=1.1,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "embedding_filtered, fig_filtered = plot_umap(\n",
    "        X_filtered,\n",
    "        clusters_filtered,\n",
    "        n_neighbors=25,\n",
    "        min_dist=0.1,\n",
    "        metric='euclidean',      \n",
    "        transform=None,          # None | 'logit' | 'arcsine' (use only if X are raw proportions)\n",
    "        n_pcs=None,              # set if X are raw features; None if X are already PCs\n",
    "        standardize=False,       # True if using raw features without PCA\n",
    "        seed=42,\n",
    "        palette=hex_colors,            # optional list/array or matplotlib colormap name\n",
    "        title=\"ALL FILTERED GENES PCA + Euclidean + logit + Laplacian\",\n",
    "        gene='ALL GENES'\n",
    "    )\n",
    "\n",
    "print(metrics_filtered)\n",
    "\n",
    "multi_sil_filtered= silhouettes_multi(X_filtered, clusters_filtered, metric_main='euclidean', extra_metrics=['cosine', 'correlation', 'jaccard'])\n",
    "\n",
    "print(f\"Silhouette (PCA + Euclidean): {metrics_filtered['silhouette']:.3f}\")\n",
    "print(f\"Silhouette (PCA + Euclidean) - Cosine: {multi_sil_filtered['silhouette_cosine']:.3f}\")\n",
    "print(f\"Silhouette (PCA + Euclidean) - Correlation: {multi_sil_filtered['silhouette_correlation']:.3f}\")\n",
    "print(f\"Silhouette (PCA + Euclidean) - Jaccard: {multi_sil_filtered['silhouette_jaccard']:.3f}\")\n",
    "\n",
    "\n",
    "read_dict_filtered = dict_id_cluster_color(df_filtered, clusters_filtered, hex_colors)\n",
    "\n",
    "df_dict_filtered = dict_to_df(read_dict_filtered)\n",
    "df_test_filtered= merge(df_proc, df_dict_filtered)\n",
    "\n",
    "# Long format for plotting\n",
    "df_test_plot_filtered = preprocess_long_for_plot(df_test_filtered,\n",
    "                                                          include_locus_cluster=True,\n",
    "                                                          filter_outliers= True,\n",
    "                                                          max_span_bp=None,     #i'm letting the filtering be based on the span_quantile\n",
    "                                                          span_quantile=0.99,\n",
    "                                                          require_center_inside=True,\n",
    "                                                          min_cpg=1,\n",
    "                                                          cpg_window_bp=5000\n",
    "                                                          )\n",
    "\n",
    "    # Plot\n",
    "plot_reads_long(\n",
    "        df_test_plot_filtered,\n",
    "        filters={\n",
    "            # \"gene_tss\": gene, \n",
    "            'group':['BT474_mV_high_Untreated',\n",
    "            'BT474_mV_low_Untreated',\n",
    "            'BT474_mV_Untreated_Unsorted']},\n",
    "        # facet_by='group',\n",
    "        color_by=\"locus_cluster\",\n",
    "        gene='all genes',\n",
    "        hex_colors=hex_colors\n",
    "    )\n",
    "\n",
    "df_only_interesting_groups_filtered = df_filtered[df_filtered.index.get_level_values('group').isin([\n",
    "        'BT474_mV_high_Untreated',\n",
    "        'BT474_mV_low_Untreated',\n",
    "        'BT474_mV_Untreated_Unsorted'\n",
    "    ])]\n",
    "\n",
    "df_only_interesting_groups_filtered = merge(df_only_interesting_groups_filtered, df_dict_filtered)\n",
    "clusters_of_interest_filtered = df_only_interesting_groups_filtered['locus_cluster'].tolist() \n",
    "summary(df_only_interesting_groups_filtered, clusters_of_interest_filtered, gene='all genes')\n",
    "\n",
    "\n",
    "profiles_df_filtered, coverage_df_filtered, meta_df_filtered, positions_filtered= compute_gene_centroids(\n",
    "                                                                    df_PCA_euclidean,\n",
    "                                                                    df_dict_PCA_euclidean,\n",
    "                                                                    gene_tss=gene,\n",
    "                                                                   )\n",
    "\n",
    "plot_centroids_with_shading(\n",
    "    profiles_df_PCA_euclidean,\n",
    "    positions_PCA_euclidean,\n",
    "    meta_df_PCA_euclidean,\n",
    "    coverage_df_PCA_euclidean,\n",
    "    hex_colors,\n",
    "    smooth_sigma = 0,\n",
    "    title='Average DNA Methylation Profiles per Cluster',\n",
    "    missingness_threshold=0.5,\n",
    ")\n",
    "\n",
    "# start_filtered, end_filtered, center_coord_filtered= start_end_center(df_test_plot_filtered)\n",
    "\n",
    "# plot_avg_methylation_profile(\n",
    "#         df= df_filtered,\n",
    "#         df_dict=df_dict_filtered,\n",
    "#         start=start_filtered,\n",
    "#         end=end_filtered,\n",
    "#         center_coord=center_coord_filtered,\n",
    "#         read_dict=read_dict_filtered,\n",
    "#         gene= 'all genes'\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_c, X_filtered_c, partition_filtered_c, clusters_filtered_c, metrics_filtered_c = clustering_final(\n",
    "        df_filtered,\n",
    "        n_neighbors=15,\n",
    "        nan_threshold=0.7,\n",
    "        nan_method='drop',\n",
    "        scaling=False,\n",
    "        pca_or_not=True,\n",
    "        n_pcs=None,           # None = no PCA; int for #PCs; float in (0,1) for variance ratio\n",
    "        metric='cosine',          # 'euclidean' or 'cosine' \n",
    "        transform='none',            # 'none', 'logit', or 'arcsine'\n",
    "        kernel_type='laplacian',\n",
    "        leiden_resolution=1.1,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "embedding_filtered_c, fig_filtered_c = plot_umap(\n",
    "        X_filtered_c,\n",
    "        clusters_filtered_c,\n",
    "        n_neighbors=25,\n",
    "        min_dist=0.1,\n",
    "        metric='cosine',      \n",
    "        transform=None,          # None | 'logit' | 'arcsine' (use only if X are raw proportions)\n",
    "        n_pcs=None,              # set if X are raw features; None if X are already PCs\n",
    "        standardize=False,       # True if using raw features without PCA\n",
    "        seed=42,\n",
    "        palette=hex_colors,            # optional list/array or matplotlib colormap name\n",
    "        title=\"ALL FILTERED GENES Cosine\",\n",
    "        gene='ALL GENES'\n",
    "    )\n",
    "\n",
    "print(metrics_filtered_c)\n",
    "\n",
    "multi_sil_filtered_c= silhouettes_multi(X_filtered_c, clusters_filtered_c, metric_main='cosine', extra_metrics=['euclidean', 'correlation', 'jaccard'])\n",
    "\n",
    "print(f\"Silhouette (Cosine): {metrics_filtered_c['silhouette']:.3f}\")\n",
    "print(f\"Silhouette (Cosine) - Euclidean: {multi_sil_filtered_c['silhouette_euclidean']:.3f}\")\n",
    "print(f\"Silhouette (Cosine) - Correlation: {multi_sil_filtered_c['silhouette_correlation']:.3f}\")\n",
    "print(f\"Silhouette (Cosine) - Jaccard: {multi_sil_filtered_c['silhouette_jaccard']:.3f}\")\n",
    "\n",
    "\n",
    "read_dict_filtered_c = dict_id_cluster_color(df_filtered_c, clusters_filtered_c, hex_colors)\n",
    "\n",
    "df_dict_filtered_c = dict_to_df(read_dict_filtered_c)\n",
    "df_test_filtered_c= merge(df_proc, df_dict_filtered_c)\n",
    "\n",
    "# Long format for plotting\n",
    "df_test_plot_filtered_c = preprocess_long_for_plot(df_test_filtered_c,\n",
    "                                                          include_locus_cluster=True,\n",
    "                                                          filter_outliers= True,\n",
    "                                                          max_span_bp=None,     #i'm letting the filtering be based on the span_quantile\n",
    "                                                          span_quantile=0.99,\n",
    "                                                          require_center_inside=True,\n",
    "                                                          min_cpg=1,\n",
    "                                                          cpg_window_bp=5000\n",
    "                                                          )\n",
    "\n",
    "    # Plot\n",
    "plot_reads_long(\n",
    "        df_test_plot_filtered_c,\n",
    "        filters={\n",
    "            # \"gene_tss\": gene, \n",
    "            'group':['BT474_mV_high_Untreated',\n",
    "            'BT474_mV_low_Untreated',\n",
    "            'BT474_mV_Untreated_Unsorted']},\n",
    "        # facet_by='group',\n",
    "        color_by=\"locus_cluster\",\n",
    "        gene='all genes - cosine',\n",
    "        hex_colors=hex_colors\n",
    "    )\n",
    "\n",
    "df_only_interesting_groups_filtered_c = df_filtered_c[df_filtered_c.index.get_level_values('group').isin([\n",
    "        'BT474_mV_high_Untreated',\n",
    "        'BT474_mV_low_Untreated',\n",
    "        'BT474_mV_Untreated_Unsorted'\n",
    "    ])]\n",
    "\n",
    "df_only_interesting_groups_filtered_c = merge(df_only_interesting_groups_filtered_c, df_dict_filtered_c)\n",
    "clusters_of_interest_filtered_c = df_only_interesting_groups_filtered_c['locus_cluster'].tolist() \n",
    "summary(df_only_interesting_groups_filtered_c, clusters_of_interest_filtered_c, gene='all genes - cosine')\n",
    "\n",
    "\n",
    "start_filtered_c, end_filtered_c, center_coord_filtered_c= start_end_center(df_test_plot_filtered_c)\n",
    "\n",
    "plot_avg_methylation_profile(\n",
    "        df= df_filtered_c,\n",
    "        df_dict=df_dict_filtered_c,\n",
    "        start=start_filtered_c,\n",
    "        end=end_filtered_c,\n",
    "        center_coord=center_coord_filtered_c,\n",
    "        read_dict=read_dict_filtered_c,\n",
    "        gene= 'all genes - cosine'\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
